architecture - 
flowchart TD
    User[User Interface] --> QueryProcessor[Query Processor]
    
    subgraph SourceSelection[Source Selection Layer]
        QueryProcessor --> SourceController[Source Controller]
        SourceController -->|Confluence Only| ConfluenceFlow
        SourceController -->|Remedy Only| RemedyFlow
        SourceController -->|Both Sources| HybridFlow[Hybrid Flow]
    end
    
    subgraph DataIngestion[Data Ingestion Layer]
        ConfluenceFlow --> ConfluenceConnector[Confluence Connector]
        RemedyFlow --> RemedyConnector[Remedy Connector]
        HybridFlow --> ConfluenceConnector & RemedyConnector
        
        ConfluenceConnector --> ConfluenceParser[Content Parser]
        RemedyConnector --> RemedyParser[Ticket Parser]
    end
    
    subgraph ProcessingLayer[Advanced Processing Layer]
        ConfluenceParser --> DocProcessor[Document Processor]
        RemedyParser --> TicketProcessor[Ticket Processor]
        
        DocProcessor --> ContentRouter{Content Type Router}
        ContentRouter -->|Text| TextProcessor[Text Processor]
        ContentRouter -->|Tables| TableProcessor[Table Processor]
        ContentRouter -->|Images| ImageProcessor[Image Processor]
        ContentRouter -->|Code| CodeProcessor[Code Processor]
        ContentRouter -->|Formulas| FormulaProcessor[Formula Processor]
        
        ImageProcessor --> OCREngine[Advanced OCR Engine]
        OCREngine --> TextProcessor
        
        TextProcessor & TableProcessor & CodeProcessor & FormulaProcessor --> ChunkingEngine[Semantic Chunking Engine]
    end
    
    subgraph IndexingLayer[Indexing Layer]
        ChunkingEngine --> VectorDB[(Vector Database)]
        ChunkingEngine --> BM25Index[(BM25 Index)]
        ChunkingEngine --> KnowledgeGraph[(Knowledge Graph)]
        
        VectorDB & BM25Index & KnowledgeGraph --> HybridRetriever[Hybrid Retriever]
    end
    
    subgraph AnswerGeneration[Answer Generation Layer]
        HybridRetriever --> EvidenceRanker[Evidence Ranker]
        EvidenceRanker --> AnswerSynthesizer[Answer Synthesizer]
        AnswerSynthesizer --> ResponseGenerator[Response Generator]
        ResponseGenerator --> ResponseFormatter[Response Formatter]
    end
    
    ResponseFormatter --> User







Base Connector with Advanced Error Handling and Rate Limiting

"""
Base connector class with advanced error handling, retry mechanisms, 
rate limiting, and caching capabilities.
"""
import time
import logging
import requests
import hashlib
import json
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, Tuple, Union
from urllib.parse import urljoin
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from datetime import datetime, timedelta
from pathlib import Path
from diskcache import Cache

logger = logging.getLogger(__name__)

class APIRateLimiter:
    """
    Implements adaptive rate limiting for API calls, automatically
    adjusting to stay under rate limits.
    """
    
    def __init__(self, max_calls_per_minute: int = 60, 
                 max_calls_per_hour: int = 1000,
                 initial_delay: float = 0.1):
        self.max_calls_per_minute = max_calls_per_minute
        self.max_calls_per_hour = max_calls_per_hour
        self.delay = initial_delay
        self.call_history = []
        self.backoff_factor = 1.5
        self.min_delay = 0.05
        self.max_delay = 5.0
        
    def wait(self) -> None:
        """Wait before making an API call based on rate limits."""
        now = datetime.now()
        
        # Clean up old call history
        self.call_history = [t for t in self.call_history 
                             if now - t < timedelta(hours=1)]
        
        # Calculate current rates
        minute_ago = now - timedelta(minutes=1)
        hour_ago = now - timedelta(hours=1)
        
        calls_last_minute = sum(1 for t in self.call_history if t > minute_ago)
        calls_last_hour = len(self.call_history)
        
        # Adjust delay based on current usage
        if calls_last_minute >= self.max_calls_per_minute:
            # We're at minute limit, increase delay
            self.delay = min(self.delay * self.backoff_factor, self.max_delay)
        elif calls_last_hour >= self.max_calls_per_hour:
            # We're at hour limit, increase delay drastically
            self.delay = min(self.delay * self.backoff_factor * 2, self.max_delay)
        else:
            # We're under limits, we can potentially decrease delay
            self.delay = max(self.delay / self.backoff_factor, self.min_delay)
        
        time.sleep(self.delay)
        
    def record_call(self) -> None:
        """Record that an API call was made."""
        self.call_history.append(datetime.now())


class ConnectorCache:
    """
    Implements multi-level caching for API responses with TTL.
    """
    
    def __init__(self, cache_dir: str, default_ttl: int = 3600):
        """
        Initialize the cache system.
        
        Args:
            cache_dir: Directory to store cache files
            default_ttl: Default time-to-live in seconds
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.default_ttl = default_ttl
        
        # In-memory cache for frequently accessed items
        self.memory_cache = {}
        self.memory_cache_ttl = {}
        
        # Disk cache for persistence
        self.disk_cache = Cache(directory=str(self.cache_dir))
    
    def _get_cache_key(self, url: str, params: Optional[Dict] = None) -> str:
        """Generate a unique cache key for the request."""
        key_parts = [url]
        if params:
            key_parts.append(json.dumps(params, sort_keys=True))
        
        return hashlib.md5("".join(key_parts).encode()).hexdigest()
    
    def get(self, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """Get a cached response if available and not expired."""
        key = self._get_cache_key(url, params)
        now = time.time()
        
        # Check memory cache first
        if key in self.memory_cache and now < self.memory_cache_ttl.get(key, 0):
            logger.debug(f"Memory cache hit for {url}")
            return self.memory_cache[key]
        
        # Check disk cache
        if key in self.disk_cache:
            cached_data = self.disk_cache[key]
            if now < cached_data.get('expires_at', 0):
                # Add to memory cache for faster access next time
                self.memory_cache[key] = cached_data['data']
                self.memory_cache_ttl[key] = cached_data['expires_at']
                logger.debug(f"Disk cache hit for {url}")
                return cached_data['data']
            else:
                # Expired, remove from disk cache
                del self.disk_cache[key]
        
        return None
    
    def set(self, url: str, params: Optional[Dict], data: Dict, ttl: Optional[int] = None) -> None:
        """Cache a response with expiration time."""
        key = self._get_cache_key(url, params)
        expires_at = time.time() + (ttl if ttl is not None else self.default_ttl)
        
        # Store in memory cache
        self.memory_cache[key] = data
        self.memory_cache_ttl[key] = expires_at
        
        # Store in disk cache
        self.disk_cache[key] = {
            'data': data,
            'expires_at': expires_at,
            'created_at': time.time()
        }
    
    def invalidate(self, url: str, params: Optional[Dict] = None) -> None:
        """Invalidate a cached item."""
        key = self._get_cache_key(url, params)
        
        # Remove from memory cache
        if key in self.memory_cache:
            del self.memory_cache[key]
            del self.memory_cache_ttl[key]
        
        # Remove from disk cache
        if key in self.disk_cache:
            del self.disk_cache[key]
    
    def clear(self) -> None:
        """Clear all cached data."""
        self.memory_cache.clear()
        self.memory_cache_ttl.clear()
        self.disk_cache.clear()


class BaseConnector(ABC):
    """
    Base connector class with advanced features common to all API connectors.
    """
    
    def __init__(self, base_url: str, username: Optional[str] = None, 
                 password: Optional[str] = None, api_token: Optional[str] = None,
                 max_retries: int = 5, timeout: int = 30, 
                 cache_dir: str = "cache", cache_ttl: int = 3600,
                 max_calls_per_minute: int = 60, max_calls_per_hour: int = 1000):
        """
        Initialize the base connector.
        
        Args:
            base_url: Base URL for API
            username: Username for authentication (if applicable)
            password: Password for authentication (if applicable)
            api_token: API token for authentication (if applicable)
            max_retries: Maximum number of retries for failed requests
            timeout: Request timeout in seconds
            cache_dir: Directory to store cache files
            cache_ttl: Default cache TTL in seconds
            max_calls_per_minute: Maximum API calls per minute
            max_calls_per_hour: Maximum API calls per hour
        """
        self.base_url = base_url
        self.username = username
        self.password = password
        self.api_token = api_token
        self.timeout = timeout
        
        # Setup session with retry strategy
        self.session = self._setup_session(max_retries)
        
        # Setup rate limiter
        self.rate_limiter = APIRateLimiter(
            max_calls_per_minute=max_calls_per_minute,
            max_calls_per_hour=max_calls_per_hour
        )
        
        # Setup cache
        self.cache = ConnectorCache(
            cache_dir=os.path.join(cache_dir, self._get_connector_name()),
            default_ttl=cache_ttl
        )
        
        # Track failed calls for circuit breaking
        self.recent_failures = []
        self.circuit_open = False
        self.circuit_reset_time = None
        self.max_failures = 5
        self.circuit_timeout = 300  # 5 minutes
        
        # Authentication state
        self.authenticated = False
        self.auth_token = None
        self.token_expiry = None
    
    def _get_connector_name(self) -> str:
        """Get the name of the connector for logging and caching."""
        return self.__class__.__name__.lower()
    
    def _setup_session(self, max_retries: int) -> requests.Session:
        """Setup a requests session with retry capabilities."""
        session = requests.Session()
        
        # Configure retry strategy
        retries = Retry(
            total=max_retries,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        
        # Mount the adapter to the session
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session
    
    def _circuit_breaker_check(self) -> bool:
        """Check if circuit breaker is open."""
        # If circuit is open, check if we should reset
        if self.circuit_open:
            if time.time() > self.circuit_reset_time:
                logger.info(f"Resetting circuit breaker for {self._get_connector_name()}")
                self.circuit_open = False
                self.recent_failures = []
            else:
                logger.warning(f"Circuit breaker open for {self._get_connector_name()}, skipping request")
                return False
        
        # Clean up old failures
        now = time.time()
        self.recent_failures = [t for t in self.recent_failures if now - t < 60]
        
        # Check failure count
        if len(self.recent_failures) >= self.max_failures:
            logger.warning(f"Circuit breaker tripped for {self._get_connector_name()}")
            self.circuit_open = True
            self.circuit_reset_time = time.time() + self.circuit_timeout
            return False
        
        return True
    
    def _record_failure(self) -> None:
        """Record a failed API call."""
        self.recent_failures.append(time.time())
    
    def _record_success(self) -> None:
        """Record a successful API call."""
        # Sometimes we want to reduce the failure count on success
        if self.recent_failures:
            self.recent_failures.pop(0)
    
    @abstractmethod
    def authenticate(self) -> bool:
        """Authenticate with the API and get token if applicable."""
        pass
    
    def _check_authentication(self) -> bool:
        """Check if authentication is valid and re-authenticate if needed."""
        # If no authentication needed
        if not any([self.username, self.password, self.api_token]):
            return True
        
        # Check if token expired
        if self.auth_token and self.token_expiry and datetime.now() < self.token_expiry:
            return True
        
        # Re-authenticate
        return self.authenticate()
    
    def _make_url(self, endpoint: str) -> str:
        """Create a full URL from an endpoint."""
        return urljoin(self.base_url, endpoint)
    
    def _prepare_headers(self, additional_headers: Optional[Dict] = None) -> Dict:
        """Prepare headers for the request, including authentication."""
        headers = {}
        
        # Add authorization header if token exists
        if self.auth_token:
            headers['Authorization'] = f"Bearer {self.auth_token}"
        
        # Add additional headers
        if additional_headers:
            headers.update(additional_headers)
        
        return headers
    
    def request(self, method: str, endpoint: str, params: Optional[Dict] = None, 
                data: Optional[Dict] = None, headers: Optional[Dict] = None, 
                use_cache: bool = True, cache_ttl: Optional[int] = None, 
                raw_response: bool = False) -> Union[Dict, bytes, None]:
        """
        Make an API request with caching, rate limiting, and circuit breaking.
        
        Args:
            method: HTTP method (GET, POST, etc.)
            endpoint: API endpoint
            params: Query parameters
            data: Request data
            headers: Additional headers
            use_cache: Whether to use cache
            cache_ttl: Override default cache TTL
            raw_response: Return raw response instead of parsing JSON
            
        Returns:
            API response as dictionary, bytes (for raw response), or None on error
        """
        url = self._make_url(endpoint)
        
        # Check circuit breaker
        if not self._circuit_breaker_check():
            raise Exception(f"Circuit breaker open for {self._get_connector_name()}")
        
        # Check and refresh authentication if needed
        if not self._check_authentication():
            raise Exception(f"Authentication failed for {self._get_connector_name()}")
        
        # Check cache for GET requests
        if method.upper() == "GET" and use_cache:
            cached_response = self.cache.get(url, params)
            if cached_response is not None:
                return cached_response
        
        # Prepare headers
        request_headers = self._prepare_headers(headers)
        
        # Rate limiting
        self.rate_limiter.wait()
        
        try:
            # Make the request
            logger.debug(f"Making {method} request to {url}")
            response = self.session.request(
                method=method,
                url=url,
                params=params,
                json=data,
                headers=request_headers,
                timeout=self.timeout
            )
            
            # Record the API call
            self.rate_limiter.record_call()
            
            # Check response status
            response.raise_for_status()
            
            # Process response
            if raw_response:
                result = response.content
            else:
                result = response.json()
            
            # Cache successful GET responses
            if method.upper() == "GET" and use_cache:
                self.cache.set(url, params, result, cache_ttl)
            
            # Record success for circuit breaker
            self._record_success()
            
            return result
            
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else 0
            
            # Handle specific status codes
            if status_code == 401:
                # Authentication error, refresh token and retry
                logger.warning(f"Authentication error, refreshing token: {e}")
                self.authenticated = False
                self.auth_token = None
                
                # Try one more time
                if self._check_authentication():
                    return self.request(method, endpoint, params, data, headers, use_cache, cache_ttl, raw_response)
            
            # Record failure for circuit breaker
            self._record_failure()
            logger.error(f"HTTP error in {self._get_connector_name()}: {e}")
            raise
            
        except (requests.exceptions.ConnectionError, 
                requests.exceptions.Timeout,
                requests.exceptions.RequestException) as e:
            # Record failure for circuit breaker
            self._record_failure()
            logger.error(f"Request error in {self._get_connector_name()}: {e}")
            raise
    
    def get(self, endpoint: str, params: Optional[Dict] = None, 
            headers: Optional[Dict] = None, use_cache: bool = True,
            cache_ttl: Optional[int] = None) -> Dict:
        """Make a GET request."""
        return self.request("GET", endpoint, params, None, headers, use_cache, cache_ttl)
    
    def post(self, endpoint: str, data: Dict, params: Optional[Dict] = None,
             headers: Optional[Dict] = None) -> Dict:
        """Make a POST request."""
        return self.request("POST", endpoint, params, data, headers, use_cache=False)
    
    def put(self, endpoint: str, data: Dict, params: Optional[Dict] = None,
            headers: Optional[Dict] = None) -> Dict:
        """Make a PUT request."""
        return self.request("PUT", endpoint, params, data, headers, use_cache=False)
    
    def delete(self, endpoint: str, params: Optional[Dict] = None,
               headers: Optional[Dict] = None) -> Dict:
        """Make a DELETE request."""
        return self.request("DELETE", endpoint, params, None, headers, use_cache=False)
    
    def invalidate_cache(self, endpoint: str, params: Optional[Dict] = None) -> None:
        """Invalidate cache for a specific endpoint."""
        url = self._make_url(endpoint)
        self.cache.invalidate(url, params)
    
    def clear_cache(self) -> None:
        """Clear all cached data."""
        self.cache.clear()
    
    @abstractmethod
    def check_connection(self) -> bool:
        """Check if connection to the API is working."""
        pass















Confluence Connector with Rich Content Extraction

"""
Confluence connector for retrieving and parsing content from Confluence.
Handles authentication, pagination, and extracting various content types.
"""
"""
Confluence connector for retrieving and parsing content from Confluence.
Handles authentication, pagination, and extracting various content types.
"""
import logging
import re
import json
import base64
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union, Generator
from urllib.parse import quote
from bs4 import BeautifulSoup
import requests
import io
from PIL import Image

from connectors.connector_base import BaseConnector
from utils.helpers import html_to_text, extract_table_from_html

logger = logging.getLogger(__name__)

class ConfluenceConnector(BaseConnector):
    """
    Connector for Atlassian Confluence with support for rich content extraction.
    """
    
    def __init__(self, base_url: str, username: Optional[str] = None, 
                 password: Optional[str] = None, api_token: Optional[str] = None,
                 space_key: Optional[str] = None, **kwargs):
        """
        Initialize the Confluence connector.
        
        Args:
            base_url: Confluence base URL
            username: Confluence username
            password: Confluence password
            api_token: Confluence API token
            space_key: Confluence space key
        """
        super().__init__(base_url, username, password, api_token, **kwargs)
        self.space_key = space_key
        self.api_path = "/rest/api"
        self.content_api = f"{self.api_path}/content"
        self.search_api = f"{self.api_path}/search"
        self.user_api = f"{self.api_path}/user"
        
        # Maximum results per page
        self.max_results = 100
        
        # Content expansion parameters
        self.content_expand = [
            "body.view", "body.storage", 
            "history", "space", 
            "ancestors", "children.page", 
            "descendants.attachment", 
            "metadata.labels", "version"
        ]
        
        # Track visited pages to avoid duplicate processing
        self.visited_pages = set()
        
        # Cache for content IDs by title
        self.content_id_cache = {}
        
        # Cache for metadata
        self.metadata_cache = {}
    
    def authenticate(self) -> bool:
        """
        Authenticate with Confluence and get session token.
        Supports both basic auth and API token auth.
        """
        try:
            # If using API token
            if self.api_token:
                auth_header = base64.b64encode(
                    f"{self.username}:{self.api_token}".encode()
                ).decode()
                
                self.session.headers.update({
                    "Authorization": f"Basic {auth_header}",
                    "Content-Type": "application/json"
                })
                
                # Test connection
                response = self.get(f"{self.user_api}/current")
                
                if response and "username" in response:
                    self.authenticated = True
                    logger.info(f"Successfully authenticated with Confluence as {response.get('username')}")
                    return True
                    
            # If using username/password
            elif self.username and self.password:
                auth_data = {
                    "username": self.username,
                    "password": self.password
                }
                
                # Atlassian Cloud has a different auth endpoint
                if "atlassian.net" in self.base_url:
                    auth_header = base64.b64encode(
                        f"{self.username}:{self.password}".encode()
                    ).decode()
                    
                    self.session.headers.update({
                        "Authorization": f"Basic {auth_header}",
                        "Content-Type": "application/json"
                    })
                else:
                    # For server installations
                    response = self.post("/rest/auth/1/session", auth_data)
                    
                    if response and "session" in response:
                        self.session.cookies.update(response.get("session", {}))
                
                # Test connection
                test_response = self.get(f"{self.user_api}/current")
                
                if test_response and "username" in test_response:
                    self.authenticated = True
                    logger.info(f"Successfully authenticated with Confluence as {test_response.get('username')}")
                    return True
            
            logger.error("Failed to authenticate with Confluence")
            return False
            
        except Exception as e:
            logger.error(f"Authentication error with Confluence: {str(e)}")
            return False
    
    def check_connection(self) -> bool:
        """Check if connection to Confluence API is working."""
        try:
            response = self.get(f"{self.user_api}/current")
            return response and "username" in response
        except Exception as e:
            logger.error(f"Confluence connection check failed: {str(e)}")
            return False
    
    def get_spaces(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get list of available Confluence spaces.
        
        Args:
            limit: Maximum number of spaces to return
            
        Returns:
            List of space dictionaries
        """
        spaces = []
        start = 0
        
        while True:
            response = self.get(
                f"{self.api_path}/space",
                params={
                    "limit": min(limit - len(spaces), self.max_results),
                    "start": start,
                    "expand": "description.view,homepage"
                }
            )
            
            if not response or "results" not in response:
                break
                
            current_results = response.get("results", [])
            spaces.extend(current_results)
            
            if len(current_results) < self.max_results or len(spaces) >= limit:
                break
                
            start += len(current_results)
            
        return spaces[:limit]
    
    def get_content_by_id(self, content_id: str, expand: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """
        Get content by ID with expanded fields.
        
        Args:
            content_id: Content ID
            expand: List of fields to expand
            
        Returns:
            Content dictionary or None if not found
        """
        if not expand:
            expand = self.content_expand
            
        try:
            return self.get(
                f"{self.content_api}/{content_id}",
                params={"expand": ",".join(expand)}
            )
        except Exception as e:
            logger.error(f"Error getting content by ID {content_id}: {str(e)}")
            return None
    
    def get_content_by_title(self, title: str, space_key: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        Get content by title with expanded fields.
        
        Args:
            title: Content title
            space_key: Space key (defaults to connector's space_key)
            
        Returns:
            Content dictionary or None if not found
        """
        # Use cache if available
        cache_key = f"{space_key or self.space_key}_{title}"
        if cache_key in self.content_id_cache:
            return self.get_content_by_id(self.content_id_cache[cache_key])
            
        try:
            params = {
                "title": title,
                "expand": ",".join(self.content_expand)
            }
            
            if space_key or self.space_key:
                params["spaceKey"] = space_key or self.space_key
                
            response = self.get(self.content_api, params=params)
            
            if response and "results" in response and response["results"]:
                content = response["results"][0]
                # Cache the ID for future use
                self.content_id_cache[cache_key] = content["id"]
                return content
                
            return None
            
        except Exception as e:
            logger.error(f"Error getting content by title '{title}': {str(e)}")
            return None
    
    def search_content(self, query: str, limit: int = 50, 
                       content_type: str = "page", 
                       space_key: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Search for content in Confluence.
        
        Args:
            query: Search query
            limit: Maximum number of results
            content_type: Type of content to search for (page, blogpost, etc.)
            space_key: Space key to limit search to
            
        Returns:
            List of matching content dictionaries
        """
        results = []
        start = 0
        
        # Build CQL query
        cql = f'type="{content_type}" AND text ~ "{query}"'
        if space_key or self.space_key:
            cql += f' AND space="{space_key or self.space_key}"'
        
        while True:
            try:
                response = self.get(
                    self.search_api,
                    params={
                        "cql": cql,
                        "limit": min(limit - len(results), self.max_results),
                        "start": start,
                        "expand": ",".join(self.content_expand)
                    }
                )
                
                if not response or "results" not in response:
                    break
                    
                current_results = response.get("results", [])
                results.extend(current_results)
                
                if len(current_results) < self.max_results or len(results) >= limit:
                    break
                    
                start += len(current_results)
                
            except Exception as e:
                logger.error(f"Error searching Confluence: {str(e)}")
                break
                
        return results[:limit]
    
    def get_children_pages(self, parent_id: str, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get children pages of a parent page.
        
        Args:
            parent_id: Parent page ID
            limit: Maximum number of results
            
        Returns:
            List of child page dictionaries
        """
        results = []
        start = 0
        
        while True:
            try:
                response = self.get(
                    f"{self.content_api}/{parent_id}/child/page",
                    params={
                        "limit": min(limit - len(results), self.max_results),
                        "start": start,
                        "expand": ",".join(self.content_expand)
                    }
                )
                
                if not response or "results" not in response:
                    break
                    
                current_results = response.get("results", [])
                results.extend(current_results)
                
                if len(current_results) < self.max_results or len(results) >= limit:
                    break
                    
                start += len(current_results)
                
            except Exception as e:
                logger.error(f"Error getting children pages for {parent_id}: {str(e)}")
                break
                
        return results[:limit]
    
    def get_attachments(self, content_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        """
        Get attachments for a content page.
        
        Args:
            content_id: Content ID
            limit: Maximum number of results
            
        Returns:
            List of attachment dictionaries
        """
        results = []
        start = 0
        
        while True:
            try:
                response = self.get(
                    f"{self.content_api}/{content_id}/child/attachment",
                    params={
                        "limit": min(limit - len(results), self.max_results),
                        "start": start,
                        "expand": "version"
                    }
                )
                
                if not response or "results" not in response:
                    break
                    
                current_results = response.get("results", [])
                results.extend(current_results)
                
                if len(current_results) < self.max_results or len(results) >= limit:
                    break
                    
                start += len(current_results)
                
            except Exception as e:
                logger.error(f"Error getting attachments for {content_id}: {str(e)}")
                break
                
        return results[:limit]
    
    def download_attachment(self, attachment: Dict[str, Any]) -> Optional[bytes]:
        """
        Download an attachment's content.
        
        Args:
            attachment: Attachment dictionary
            
        Returns:
            Attachment content as bytes or None if failed
        """
        try:
            # Get download URL
            if "links" in attachment and "_self" in attachment.get("_links", {}):
                download_url = attachment["_links"]["download"]
                
                # For relative URLs, prefix with base URL
                if not download_url.startswith("http"):
                    download_url = f"{self.base_url}{download_url}"
                
                # Download the file
                response = self.request(
                    "GET", 
                    download_url, 
                    raw_response=True,
                    use_cache=True
                )
                
                return response
            
            return None
            
        except Exception as e:
            logger.error(f"Error downloading attachment: {str(e)}")
            return None
    
    def get_labels(self, content_id: str) -> List[str]:
        """
        Get labels for content.
        
        Args:
            content_id: Content ID
            
        Returns:
            List of label names
        """
        try:
            response = self.get(f"{self.content_api}/{content_id}/label")
            
            if response and "results" in response:
                return [label.get("name") for label in response.get("results", [])]
                
            return []
            
        except Exception as e:
            logger.error(f"Error getting labels for {content_id}: {str(e)}")
            return []
    
    def extract_content_metadata(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract metadata from content dictionary.
        
        Args:
            content: Content dictionary
            
        Returns:
            Metadata dictionary
        """
        metadata = {
            "id": content.get("id"),
            "title": content.get("title"),
            "type": content.get("type"),
            "created_at": content.get("history", {}).get("createdDate"),
            "updated_at": content.get("history", {}).get("lastUpdated", {}).get("when"),
            "created_by": content.get("history", {}).get("createdBy", {}).get("displayName"),
            "updated_by": content.get("history", {}).get("lastUpdated", {}).get("by", {}).get("displayName"),
            "version": content.get("version", {}).get("number"),
            "space_key": content.get("space", {}).get("key"),
            "space_name": content.get("space", {}).get("name"),
            "url": self.base_url + content.get("_links", {}).get("webui", ""),
            "ancestors": [ancestor.get("title") for ancestor in content.get("ancestors", [])],
            "labels": []
        }
        
        # Extract labels if they're expanded
        if "metadata" in content and "labels" in content["metadata"]:
            metadata["labels"] = [label.get("name") for label in 
                                 content["metadata"]["labels"].get("results", [])]
        
        return metadata
    
    def _clean_html(self, html_content: str) -> str:
        """
        Clean HTML content - remove scripts, styles, etc.
        
        Args:
            html_content: HTML content
            
        Returns:
            Cleaned HTML content
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove scripts, styles, and comments
        for element in soup(["script", "style"]):
            element.decompose()
            
        # Remove comments
        for comment in soup.find_all(string=lambda text: isinstance(text, str) and '<!--' in text):
            comment.extract()
            
        return str(soup)
    
    def _extract_tables(self, html_content: str) -> List[Dict[str, Any]]:
        """
        Extract tables from HTML content.
        
        Args:
            html_content: HTML content
            
        Returns:
            List of extracted tables with metadata
        """
        tables = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for i, table_tag in enumerate(soup.find_all('table')):
            # Extract table caption or nearest heading as title
            title = None
            caption = table_tag.find('caption')
            if caption:
                title = caption.get_text().strip()
            
            if not title:
                # Look for preceding heading
                prev_elem = table_tag.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if prev_elem:
                    title = prev_elem.get_text().strip()
            
            if not title:
                title = f"Table {i+1}"
                
            # Extract table HTML
            table_html = str(table_tag)
            
            # Try to convert to data format
            headers, rows = extract_table_from_html(table_html)
            
            tables.append({
                "title": title,
                "headers": headers,
                "rows": rows,
                "html": table_html,
                "order": i
            })
            
        return tables
    
    def _extract_images(self, html_content: str, page_url: str) -> List[Dict[str, Any]]:
        """
        Extract images from HTML content.
        
        Args:
            html_content: HTML content
            page_url: URL of the page (for resolving relative URLs)
            
        Returns:
            List of extracted images with metadata
        """
        images = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for i, img_tag in enumerate(soup.find_all('img')):
            # Get image URL
            src = img_tag.get('src', '')
            
            # Skip empty sources or data URIs
            if not src or src.startswith('data:'):
                continue
                
            # Make absolute URL if needed
            if not src.startswith(('http://', 'https://')):
                if src.startswith('/'):
                    base_url = '/'.join(self.base_url.split('/')[:3])  # http(s)://domain.com
                    src = f"{base_url}{src}"
                else:
                    src = f"{page_url}/{src}"
            
            # Get alt text and surrounding context
            alt_text = img_tag.get('alt', '')
            
            # Look for figure caption
            caption = None
            figure_parent = img_tag.find_parent('figure')
            if figure_parent:
                figcaption = figure_parent.find('figcaption')
                if figcaption:
                    caption = figcaption.get_text().strip()
            
            # Or look for nearest heading
            if not caption:
                prev_heading = img_tag.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if prev_heading:
                    caption = prev_heading.get_text().strip()
            
            images.append({
                "src": src,
                "alt_text": alt_text,
                "caption": caption,
                "order": i,
                "context": self._get_element_context(img_tag)
            })
            
        return images
    
    def _extract_code_blocks(self, html_content: str) -> List[Dict[str, Any]]:
        """
        Extract code blocks from HTML content.
        
        Args:
            html_content: HTML content
            
        Returns:
            List of extracted code blocks with metadata
        """
        code_blocks = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Confluence uses various methods to display code
        # Check for <pre> elements
        for i, pre_tag in enumerate(soup.find_all('pre')):
            code_tag = pre_tag.find('code')
            language = None
            
            # Try to determine language from class
            if code_tag and code_tag.get('class'):
                for cls in code_tag.get('class'):
                    if cls.startswith('language-'):
                        language = cls.replace('language-', '')
                        break
            
            # Get code content
            if code_tag:
                code_content = code_tag.get_text()
            else:
                code_content = pre_tag.get_text()
                
            # Get title from context
            title = None
            prev_heading = pre_tag.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if prev_heading:
                title = prev_heading.get_text().strip()
                
            if not title:
                title = f"Code Block {i+1}"
                
            code_blocks.append({
                "content": code_content,
                "language": language,
                "title": title,
                "order": i,
                "context": self._get_element_context(pre_tag)
            })
            
        # Also look for Confluence's code macro
        for i, div in enumerate(soup.find_all('div', {'class': 'code-block'})):
            # Language might be in a nested element
            language_elem = div.find(class_='code-lang')
            language = language_elem.get_text().strip() if language_elem else None
            
            # Code might be in a nested element
            code_content = div.get_text()
            if div.find('pre'):
                code_content = div.find('pre').get_text()
                
            # Get title from context or code block title
            title = None
            title_elem = div.find(class_='code-title')
            if title_elem:
                title = title_elem.get_text().strip()
                
            if not title:
                prev_heading = div.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if prev_heading:
                    title = prev_heading.get_text().strip()
                    
            if not title:
                title = f"Code Block {len(code_blocks) + i + 1}"
                
            code_blocks.append({
                "content": code_content,
                "language": language,
                "title": title,
                "order": len(code_blocks) + i,
                "context": self._get_element_context(div)
            })
            
        return code_blocks
    
    def _extract_lists(self, html_content: str) -> List[Dict[str, Any]]:
        """
        Extract lists from HTML content.
        
        Args:
            html_content: HTML content
            
        Returns:
            List of extracted lists with metadata
        """
        lists = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for i, list_tag in enumerate(soup.find_all(['ul', 'ol'])):
            # Skip nested lists
            if list_tag.find_parent(['ul', 'ol']):
                continue
                
            # Get list type
            list_type = 'unordered' if list_tag.name == 'ul' else 'ordered'
            
            # Get list items
            items = [li.get_text().strip() for li in list_tag.find_all('li', recursive=False)]
            
            # Get title from context
            title = None
            prev_heading = list_tag.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if prev_heading:
                title = prev_heading.get_text().strip()
                
            if not title:
                title = f"{list_type.capitalize()} List {i+1}"
                
            lists.append({
                "items": items,
                "type": list_type,
                "title": title,
                "order": i,
                "context": self._get_element_context(list_tag)
            })
            
        return lists
    
    def _get_element_context(self, element, context_size: int = 100) -> str:
        """
        Get surrounding context for an element.
        
        Args:
            element: BeautifulSoup element
            context_size: Number of characters of context
            
        Returns:
            Context text
        """
        # Try to get the parent paragraph or div
        parent = element.find_parent(['p', 'div', 'section'])
        if parent:
            # Get text before element
            before = ""
            for sibling in parent.find_all_previous(string=True, limit=5):
                before = sibling.strip() + " " + before
                if len(before) > context_size:
                    break
                    
            # Get text after element
            after = ""
            for sibling in parent.find_all_next(string=True, limit=5):
                after += " " + sibling.strip()
                if len(after) > context_size:
                    break
                    
            return (before[-context_size:] if len(before) > context_size else before) + " " + (after[:context_size] if len(after) > context_size else after)
            
        return ""
    
    def parse_content(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parse Confluence content to extract text, tables, images, etc.
        
        Args:
            content: Content dictionary from Confluence API
            
        Returns:
            Dictionary with parsed content
        """
        result = {
            "metadata": self.extract_content_metadata(content),
            "text": "",
            "html": "",
            "tables": [],
            "images": [],
            "code_blocks": [],
            "lists": [],
            "attachments": []
        }
        
        # Get content body
        if "body" in content:
            if "view" in content["body"]:
                html_content = content["body"]["view"]["value"]
                result["html"] = self._clean_html(html_content)
                result["text"] = html_to_text(result["html"])
                
                # Extract tables, images, and code blocks
                result["tables"] = self._extract_tables(result["html"])
                result["images"] = self._extract_images(result["html"], result["metadata"]["url"])
                result["code_blocks"] = self._extract_code_blocks(result["html"])
                result["lists"] = self._extract_lists(result["html"])
                
            elif "storage" in content["body"]:
                html_content = content["body"]["storage"]["value"]
                result["html"] = self._clean_html(html_content)
                result["text"] = html_to_text(result["html"])
                
                # Extract tables, images, and code blocks
                result["tables"] = self._extract_tables(result["html"])
                result["images"] = self._extract_images(result["html"], result["metadata"]["url"])
                result["code_blocks"] = self._extract_code_blocks(result["html"])
                result["lists"] = self._extract_lists(result["html"])
        
        # Get attachments
        if "descendants" in content and "attachment" in content["descendants"]:
            for attachment in content["descendants"]["attachment"].get("results", []):
                # Extract basic info
                attachment_info = {
                    "id": attachment.get("id"),
                    "title": attachment.get("title"),
                    "filename": attachment.get("title"),
                    "media_type": attachment.get("metadata", {}).get("mediaType"),
                    "file_size": attachment.get("extensions", {}).get("fileSize"),
                    "url": self.base_url + attachment.get("_links", {}).get("download", ""),
                    "created_at": attachment.get("version", {}).get("when"),
                    "created_by": attachment.get("version", {}).get("by", {}).get("displayName")
                }
                
                result["attachments"].append(attachment_info)
        
        return result
    
    def _extract_text_from_image(self, image_data: bytes) -> str:
        """
        Extract text from image using OCR.
        
        Args:
            image_data: Image data as bytes
            
        Returns:
            Extracted text
        """
        try:
            # Import here to avoid dependency issues
            import pytesseract
            from PIL import Image
            
            # Open image
            img = Image.open(io.BytesIO(image_data))
            
            # Perform OCR
            text = pytesseract.image_to_string(img)
            
            return text.strip()
            
        except ImportError:
            logger.warning("pytesseract not installed. OCR not available.")
            return ""
        except Exception as e:
            logger.error(f"Error extracting text from image: {str(e)}")
            return ""
    
    def get_page_with_content(self, page_id: str, include_attachments: bool = True) -> Dict[str, Any]:
        """
        Get a page with all its parsed content.
        
        Args:
            page_id: Page ID
            include_attachments: Whether to process attachments
            
        Returns:
            Dictionary with parsed content
        """
        content = self.get_content_by_id(page_id)
        if not content:
            return {}
            
        result = self.parse_content(content)
        
        # Process attachments if requested
        if include_attachments:
            for i, attachment in enumerate(result["attachments"]):
                # Skip non-image attachments for now
                if not attachment.get("media_type", "").startswith("image/"):
                    continue
                    
                # Download attachment
                attachment_data = self.download_attachment(attachment)
                if attachment_data:
                    # Extract text using OCR
                    attachment["extracted_text"] = self._extract_text_from_image(attachment_data)
        
        return result
    
    def crawl_space(self, space_key: Optional[str] = None, 
                    max_pages: int = 100, 
                    include_attachments: bool = True,
                    callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """
        Crawl a Confluence space and collect all pages.
        
        Args:
            space_key: Space key (defaults to connector's space_key)
            max_pages: Maximum number of pages to crawl
            include_attachments: Whether to process attachments
            callback: Optional callback function for each page
            
        Returns:
            List of pages with content
        """
        space_key = space_key or self.space_key
        if not space_key:
            logger.error("No space key provided for crawling")
            return []
            
        results = []
        self.visited_pages = set()
        
        # Get the space's homepage
        try:
            space = self.get(
                f"{self.api_path}/space/{space_key}",
                params={"expand": "homepage"}
            )
            
            if not space or "homepage" not in space:
                logger.error(f"Could not get homepage for space {space_key}")
                return []
                
            # Start crawling from homepage
            homepage_id = space["homepage"]["id"]
            queue = [(homepage_id, 0)]  # (page_id, depth)
            
            while queue and len(results) < max_pages:
                page_id, depth = queue.pop(0)
                
                # Skip if already visited
                if page_id in self.visited_pages:
                    continue
                    
                self.visited_pages.add(page_id)
                
                # Get page content
                page = self.get_page_with_content(page_id, include_attachments)
                if page:
                    # Add to results
                    results.append(page)
                    
                    # Call callback if provided
                    if callback:
                        callback(page)
                    
                    # Add children to queue
                    children = self.get_children_pages(page_id)
                    for child in children:
                        if child["id"] not in self.visited_pages:
                            queue.append((child["id"], depth + 1))
            
            return results
            
        except Exception as e:
            logger.error(f"Error crawling space {space_key}: {str(e)}")
            return results
    
    def get_page_tree(self, space_key: Optional[str] = None) -> Dict[str, Any]:
        """
        Get a tree structure of pages in a space.
        
        Args:
            space_key: Space key (defaults to connector's space_key)
            
        Returns:
            Dictionary with page tree structure
        """
        space_key = space_key or self.space_key
        if not space_key:
            logger.error("No space key provided for page tree")
            return {}
            
        try:
            # Get space homepage
            space = self.get(
                f"{self.api_path}/space/{space_key}",
                params={"expand": "homepage"}
            )
            
            if not space or "homepage" not in space:
                logger.error(f"Could not get homepage for space {space_key}")
                return {}
                
            # Start building tree from homepage
            homepage_id = space["homepage"]["id"]
            
            def build_tree(page_id):
                page = self.get_content_by_id(page_id, expand=["version", "title"])
                if not page:
                    return None
                    
                node = {
                    "id": page["id"],
                    "title": page["title"],
                    "children": []
                }
                
                # Get children
                children = self.get_children_pages(page_id, limit=1000)
                for child in children:
                    child_node = build_tree(child["id"])
                    if child_node:
                        node["children"].append(child_node)
                
                return node
                
            return build_tree(homepage_id) or {}
            
        except Exception as e:
            logger.error(f"Error getting page tree for space {space_key}: {str(e)}")
            return {}








Remedy Connector with Advanced Ticket Processing
"""
Remedy connector for retrieving and parsing tickets from BMC Remedy.
Handles authentication, advanced filtering, and structured data extraction.
"""
import logging
import json
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union, Generator
import xml.etree.ElementTree as ET

from connectors.connector_base import BaseConnector

logger = logging.getLogger(__name__)

class RemedyConnector(BaseConnector):
    """
    Connector for BMC Remedy AR System with support for advanced query capabilities.
    """
    
    def __init__(self, base_url: str, username: Optional[str] = None, 
                 password: Optional[str] = None, api_token: Optional[str] = None,
                 server_name: Optional[str] = None, **kwargs):
        """
        Initialize the Remedy connector.
        
        Args:
            base_url: Remedy API base URL
            username: Remedy username
            password: Remedy password
            api_token: Remedy API token
            server_name: Remedy server name
        """
        super().__init__(base_url, username, password, api_token, **kwargs)
        self.server_name = server_name
        self.api_path = "/api/jwt"
        self.endpoint_auth = f"{self.api_path}/login"
        self.endpoint_logout = f"{self.api_path}/logout"
        self.endpoint_search = f"{self.api_path}/search"
        self.endpoint_entry = f"{self.api_path}/entry"
        
        # Remedy specific configuration
        self.token_type = "AR-JWT"
        self.content_type = "application/x-www-form-urlencoded"
        
        # Form mapping - update with actual form names for your Remedy instance
        self.form_mapping = {
            "incident": "HPD:Help Desk",
            "change": "CHG:Change",
            "problem": "PBM:Problem",
            "service_request": "SR:ServiceRequest",
            "work_order": "WOI:WorkOrder"
        }
        
        # Field mapping - update with actual field names for your forms
        self.field_mapping = {
            "incident": {
                "id": "Incident Number",
                "title": "Summary",
                "description": "Description",
                "status": "Status",
                "priority": "Priority",
                "created_by": "Submitter",
                "assigned_to": "Assignee",
                "created_at": "Submit Date",
                "updated_at": "Last Modified Date",
                "category": "Category",
                "impact": "Impact",
                "urgency": "Urgency",
                "resolution": "Resolution",
                "closed_at": "Closed Date",
                "owner_group": "Owner Group",
                "support_group": "Support Group Name",
                "customer": "Customer",
                "company": "Company"
            },
            "change": {
                "id": "Change ID",
                "title": "Summary",
                "description": "Description",
                "status": "Status",
                "priority": "Priority",
                "created_by": "Requester",
                "assigned_to": "Assignee",
                "created_at": "Create Date",
                "updated_at": "Last Modified Date",
                "category": "Category",
                "impact": "Impact",
                "risk": "Risk",
                "scheduled_start": "Scheduled Start Date",
                "scheduled_end": "Scheduled End Date",
                "owner_group": "Owner Group",
                "change_type": "Change Type",
                "approval_status": "Approval Status"
            }
            # Add other form mappings as needed
        }
        
        # Cache for frequently used queries
        self.query_cache = {}
    
    def authenticate(self) -> bool:
        """
        Authenticate with Remedy and get JWT token.
        """
        try:
            # Prepare login data
            login_data = {
                "username": self.username,
                "password": self.password
            }
            
            headers = {
                "Content-Type": "application/x-www-form-urlencoded"
            }
            
            # Make authentication request
            login_url = f"{self.base_url}{self.endpoint_auth}"
            
            # Convert dict to form-urlencoded format
            form_data = "&".join(f"{key}={value}" for key, value in login_data.items())
            
            # Using _session directly to avoid calling request() which would check auth
            response = self.session.post(
                login_url,
                data=form_data,
                headers=headers,
                timeout=self.timeout
            )
            
            # Check response
            if response.status_code == 200:
                # Parse response
                auth_response = response.json()
                
                if "token" in auth_response:
                    self.auth_token = auth_response["token"]
                    
                    # Set token expiry (default: 1 hour)
                    if "expiration" in auth_response:
                        # Parse Unix timestamp
                        self.token_expiry = datetime.fromtimestamp(auth_response["expiration"] / 1000)
                    else:
                        # Default expiry: 1 hour
                        self.token_expiry = datetime.now() + timedelta(hours=1)
                    
                    self.authenticated = True
                    logger.info(f"Successfully authenticated with Remedy as {self.username}")
                    return True
            
            logger.error(f"Failed to authenticate with Remedy: {response.text}")
            return False
            
        except Exception as e:
            logger.error(f"Authentication error with Remedy: {str(e)}")
            return False
    
    def logout(self) -> bool:
        """
        Logout from Remedy to invalidate the JWT token.
        """
        if not self.auth_token:
            return True
            
        try:
            # Make logout request
            self.post(self.endpoint_logout, {})
            
            # Reset authentication state
            self.auth_token = None
            self.token_expiry = None
            self.authenticated = False
            
            logger.info("Successfully logged out from Remedy")
            return True
            
        except Exception as e:
            logger.error(f"Error logging out from Remedy: {str(e)}")
            return False
    
    def check_connection(self) -> bool:
        """Check if connection to Remedy API is working."""
        try:
            # Try to get server info
            auth_ok = self.authenticate()
            if auth_ok:
                # Basic query to check connection
                test_query = self.search_tickets(
                    form_name=self.form_mapping.get("incident"),
                    query="'Status' != \"\"",
                    limit=1
                )
                
                return isinstance(test_query, list)
            
            return False
            
        except Exception as e:
            logger.error(f"Remedy connection check failed: {str(e)}")
            return False
        finally:
            # Always logout to clean up
            self.logout()
    
    def get_form_name(self, form_type: str) -> str:
        """
        Get actual form name from form type.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            
        Returns:
            Actual form name in Remedy
        """
        return self.form_mapping.get(form_type.lower(), form_type)
    
    def get_field_map(self, form_type: str) -> Dict[str, str]:
        """
        Get field mapping for a form type.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            
        Returns:
            Dictionary mapping friendly field names to Remedy field names
        """
        return self.field_mapping.get(form_type.lower(), {})
    
    def search_tickets(self, form_name: str, query: str = "", 
                       fields: Optional[List[str]] = None,
                       sort_by: Optional[str] = None,
                       sort_order: str = "ASC",
                       limit: int = 50,
                       offset: int = 0) -> List[Dict[str, Any]]:
        """
        Search for tickets in Remedy using AR query syntax.
        
        Args:
            form_name: Name of the form to search
            query: AR query syntax string
            fields: List of fields to retrieve (None for all)
            sort_by: Field to sort by
            sort_order: Sort order (ASC or DESC)
            limit: Maximum number of results
            offset: Result offset
            
        Returns:
            List of ticket dictionaries
        """
        try:
            # Build the query parameters
            params = {
                "q": query,
                "limit": limit,
                "offset": offset
            }
            
            # Add fields parameter if specified
            if fields:
                params["fields"] = ",".join(fields)
                
            # Add sorting if specified
            if sort_by:
                params["sort"] = f"{sort_by} {sort_order}"
            
            # Make the request
            endpoint = f"{self.endpoint_search}/{form_name}"
            
            # Generate cache key
            cache_key = f"{form_name}_{query}_{str(fields)}_{offset}_{limit}"
            
            # Check cache
            if cache_key in self.query_cache:
                return self.query_cache[cache_key]
                
            response = self.get(endpoint, params=params)
            
            # Process response
            if response and "entries" in response:
                tickets = response["entries"]
                
                # Cache the result for future use
                self.query_cache[cache_key] = tickets
                
                return tickets
                
            return []
            
        except Exception as e:
            logger.error(f"Error searching Remedy tickets: {str(e)}")
            return []
    
    def get_ticket(self, form_name: str, ticket_id: str, 
                   fields: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """
        Get a single ticket by ID.
        
        Args:
            form_name: Name of the form
            ticket_id: Ticket ID
            fields: List of fields to retrieve (None for all)
            
        Returns:
            Ticket dictionary or None if not found
        """
        try:
            # Prepare parameters
            params = {}
            if fields:
                params["fields"] = ",".join(fields)
                
            # Make the request
            endpoint = f"{self.endpoint_entry}/{form_name}/{ticket_id}"
            
            response = self.get(endpoint, params=params)
            
            # Check if ticket exists
            if response and "values" in response:
                return response
                
            return None
            
        except Exception as e:
            logger.error(f"Error getting Remedy ticket {ticket_id}: {str(e)}")
            return None
    
    def get_ticket_history(self, form_name: str, ticket_id: str) -> List[Dict[str, Any]]:
        """
        Get history of a ticket.
        
        Args:
            form_name: Name of the form
            ticket_id: Ticket ID
            
        Returns:
            List of history entries
        """
        try:
            endpoint = f"{self.endpoint_entry}/{form_name}/{ticket_id}/history"
            
            response = self.get(endpoint)
            
            if response and "entries" in response:
                return response["entries"]
                
            return []
            
        except Exception as e:
            logger.error(f"Error getting history for ticket {ticket_id}: {str(e)}")
            return []
    
    def get_ticket_attachments(self, form_name: str, ticket_id: str) -> List[Dict[str, Any]]:
        """
        Get attachments for a ticket.
        
        Args:
            form_name: Name of the form
            ticket_id: Ticket ID
            
        Returns:
            List of attachment metadata
        """
        try:
            endpoint = f"{self.endpoint_entry}/{form_name}/{ticket_id}/attachments"
            
            response = self.get(endpoint)
            
            if response and "entries" in response:
                return response["entries"]
                
            return []
            
        except Exception as e:
            logger.error(f"Error getting attachments for ticket {ticket_id}: {str(e)}")
            return []
    
    def download_attachment(self, form_name: str, ticket_id: str, 
                            attachment_id: str) -> Optional[bytes]:
        """
        Download an attachment.
        
        Args:
            form_name: Name of the form
            ticket_id: Ticket ID
            attachment_id: Attachment ID
            
        Returns:
            Attachment content as bytes or None if failed
        """
        try:
            endpoint = f"{self.endpoint_entry}/{form_name}/{ticket_id}/attachments/{attachment_id}/content"
            
            response = self.request(
                "GET",
                endpoint,
                raw_response=True,
                use_cache=True
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error downloading attachment {attachment_id}: {str(e)}")
            return None
    
    def search_tickets_advanced(self, form_type: str, 
                              status: Optional[List[str]] = None,
                              priority: Optional[List[str]] = None,
                              assignee: Optional[str] = None,
                              group: Optional[str] = None,
                              customer: Optional[str] = None,
                              created_after: Optional[datetime] = None,
                              created_before: Optional[datetime] = None,
                              keywords: Optional[List[str]] = None,
                              fields: Optional[List[str]] = None,
                              limit: int = 50) -> List[Dict[str, Any]]:
        """
        Search for tickets with multiple filters.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            status: List of statuses to filter by
            priority: List of priorities to filter by
            assignee: Assignee to filter by
            group: Owner group to filter by
            customer: Customer to filter by
            created_after: Filter tickets created after this datetime
            created_before: Filter tickets created before this datetime
            keywords: List of keywords to search for
            fields: List of fields to retrieve
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        # Get form name and field map
        form_name = self.get_form_name(form_type)
        field_map = self.get_field_map(form_type)
        
        # Build query conditions
        conditions = []
        
        # Add status filter
        if status and field_map.get("status"):
            status_conditions = [f"'{field_map['status']}' = \"{s}\"" for s in status]
            if status_conditions:
                conditions.append(f"({' OR '.join(status_conditions)})")
        
        # Add priority filter
        if priority and field_map.get("priority"):
            priority_conditions = [f"'{field_map['priority']}' = \"{p}\"" for p in priority]
            if priority_conditions:
                conditions.append(f"({' OR '.join(priority_conditions)})")
        
        # Add assignee filter
        if assignee and field_map.get("assigned_to"):
            conditions.append(f"'{field_map['assigned_to']}' = \"{assignee}\"")
        
        # Add owner group filter
        if group and field_map.get("owner_group"):
            conditions.append(f"'{field_map['owner_group']}' = \"{group}\"")
        
        # Add customer filter
        if customer and field_map.get("customer"):
            conditions.append(f"'{field_map['customer']}' = \"{customer}\"")
        
        # Add date filters
        if created_after and field_map.get("created_at"):
            date_str = created_after.strftime("%Y-%m-%d %H:%M:%S")
            conditions.append(f"'{field_map['created_at']}' >= \"{date_str}\"")
        
        if created_before and field_map.get("created_at"):
            date_str = created_before.strftime("%Y-%m-%d %H:%M:%S")
            conditions.append(f"'{field_map['created_at']}' <= \"{date_str}\"")
        
        # Add keyword search (search in title and description)
        if keywords:
            keyword_conditions = []
            if field_map.get("title"):
                for keyword in keywords:
                    keyword_conditions.append(f"'{field_map['title']}' LIKE \"%{keyword}%\"")
            
            if field_map.get("description"):
                for keyword in keywords:
                    keyword_conditions.append(f"'{field_map['description']}' LIKE \"%{keyword}%\"")
            
            if keyword_conditions:
                conditions.append(f"({' OR '.join(keyword_conditions)})")
        
        # Combine all conditions
        query = " AND ".join(conditions) if conditions else ""
        
        # Map field names if needed
        mapped_fields = None
        if fields:
            mapped_fields = [field_map.get(field, field) for field in fields]
        
        # Perform the search
        return self.search_tickets(
            form_name=form_name,
            query=query,
            fields=mapped_fields,
            sort_by=field_map.get("updated_at", "Last Modified Date"),
            sort_order="DESC",
            limit=limit
        )
    
    def get_tickets_by_status(self, form_type: str, status: List[str], 
                             limit: int = 50) -> List[Dict[str, Any]]:
        """
        Get tickets by status.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            status: List of statuses to filter by
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        return self.search_tickets_advanced(
            form_type=form_type,
            status=status,
            limit=limit
        )
    
    def get_recent_tickets(self, form_type: str, days: int = 7, 
                          limit: int = 50) -> List[Dict[str, Any]]:
        """
        Get recent tickets.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            days: Number of days to look back
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        created_after = datetime.now() - timedelta(days=days)
        
        return self.search_tickets_advanced(
            form_type=form_type,
            created_after=created_after,
            limit=limit
        )
    
    def get_tickets_for_user(self, form_type: str, username: str, 
                            limit: int = 50) -> List[Dict[str, Any]]:
        """
        Get tickets assigned to a user.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            username: Username to filter by
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        return self.search_tickets_advanced(
            form_type=form_type,
            assignee=username,
            limit=limit
        )
    
    def get_tickets_for_group(self, form_type: str, group: str, 
                             limit: int = 50) -> List[Dict[str, Any]]:
        """
        Get tickets assigned to a group.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            group: Group name to filter by
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        return self.search_tickets_advanced(
            form_type=form_type,
            group=group,
            limit=limit
        )
    
    def search_tickets_by_keyword(self, form_type: str, keywords: List[str], 
                                limit: int = 50) -> List[Dict[str, Any]]:
        """
        Search tickets by keywords.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            keywords: List of keywords to search for
            limit: Maximum number of results
            
        Returns:
            List of ticket dictionaries
        """
        return self.search_tickets_advanced(
            form_type=form_type,
            keywords=keywords,
            limit=limit
        )
    
    def _normalize_ticket_fields(self, ticket: Dict[str, Any], 
                                form_type: str) -> Dict[str, Any]:
        """
        Normalize ticket fields from Remedy format to friendly format.
        
        Args:
            ticket: Ticket dictionary from Remedy
            form_type: Type of form (incident, change, etc.)
            
        Returns:
            Normalized ticket dictionary
        """
        # Get field map (reversed)
        field_map = self.get_field_map(form_type)
        reversed_map = {v: k for k, v in field_map.items()}
        
        normalized = {}
        
        # Add raw values for reference
        normalized["raw"] = ticket.get("values", {})
        
        # Extract core metadata
        if "id" in ticket:
            normalized["id"] = ticket["id"]
        
        # Map fields based on field map
        for remedy_field, remedy_value in ticket.get("values", {}).items():
            friendly_field = reversed_map.get(remedy_field, remedy_field)
            normalized[friendly_field] = remedy_value
        
        # Ensure core fields exist
        core_fields = ["id", "title", "status", "priority", "created_at", "updated_at", 
                       "assigned_to", "owner_group", "description"]
                       
        for field in core_fields:
            if field not in normalized:
                normalized[field] = None
        
        return normalized
    
    def get_ticket_with_details(self, form_type: str, ticket_id: str,
                               include_history: bool = True,
                               include_attachments: bool = True) -> Dict[str, Any]:
        """
        Get a ticket with full details including history and attachments.
        
        Args:
            form_type: Type of form (incident, change, etc.)
            ticket_id: Ticket ID
            include_history: Whether to include history
            include_attachments: Whether to include attachments
            
        Returns:
            Detailed ticket dictionary
        """
        form_name = self.get_form_name(form_type)
        
        # Get base ticket
        ticket = self.get_ticket(form_name, ticket_id)
        if not ticket:
            return {}
            
        # Normalize fields
        result = self._normalize_ticket_fields(ticket, form_type)
        
        # Add history if requested
        if include_history:
            history = self.get_ticket_history(form_name, ticket_id)
            result["history"] = [self._normalize_ticket_fields(h, form_type) for h in history]
        
        # Add attachments if requested
        if include_attachments:
            attachments = self.get_ticket_attachments(form_name, ticket_id)
            result["attachments"] = attachments
        
        return result
    
    def extract_content_from_tickets(self, tickets: List[Dict[str, Any]], 
                                    form_type: str) -> Dict[str, Any]:
        """
        Extract structured content from a list of tickets.
        
        Args:
            tickets: List of ticket dictionaries
            form_type: Type of form (incident, change, etc.)
            
        Returns:
            Dictionary with extracted content
        """
        # Normalize all tickets
        normalized_tickets = [self._normalize_ticket_fields(t, form_type) for t in tickets]
        
        # Group tickets by various criteria
        by_status = {}
        by_priority = {}
        by_group = {}
        by_assignee = {}
        
        for ticket in normalized_tickets:
            # Group by status
            status = ticket.get("status")
            if status:
                if status not in by_status:
                    by_status[status] = []
                by_status[status].append(ticket)
            
            # Group by priority
            priority = ticket.get("priority")
            if priority:
                if priority not in by_priority:
                    by_priority[priority] = []
                by_priority[priority].append(ticket)
            
            # Group by owner group
            group = ticket.get("owner_group")
            if group:
                if group not in by_group:
                    by_group[group] = []
                by_group[group].append(ticket)
            
            # Group by assignee
            assignee = ticket.get("assigned_to")
            if assignee:
                if assignee not in by_assignee:
                    by_assignee[assignee] = []
                by_assignee[assignee].append(ticket)
        
        # Extract common themes and topics
        topics = self._extract_common_topics(normalized_tickets)
        
        # Compile result
        result = {
            "tickets": normalized_tickets,
            "by_status": by_status,
            "by_priority": by_priority,
            "by_group": by_group,
            "by_assignee": by_assignee,
            "topics": topics,
            "summary": self._generate_summary(normalized_tickets, topics),
            "ticket_count": len(normalized_tickets)
        }
        
        return result
    
    def _extract_common_topics(self, tickets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract common topics from tickets.
        
        Args:
            tickets: List of normalized ticket dictionaries
            
        Returns:
            List of topic dictionaries
        """
        # Extract all descriptions and titles
        texts = []
        for ticket in tickets:
            if ticket.get("description"):
                texts.append(ticket.get("description"))
            if ticket.get("title"):
                texts.append(ticket.get("title"))
        
        # Simple word frequency analysis
        word_freq = {}
        for text in texts:
            if not text:
                continue
                
            # Tokenize and count
            words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
            for word in words:
                if word not in word_freq:
                    word_freq[word] = 0
                word_freq[word] += 1
        
        # Filter out common words
        stopwords = {"the", "and", "for", "not", "with", "this", "that", "has", "have", 
                    "had", "are", "were", "was", "will", "been", "being", "they", "them", 
                    "their", "from", "what", "which", "when", "where", "who", "why", "how"}
                    
        for word in stopwords:
            if word in word_freq:
                del word_freq[word]
        
        # Find top words
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # Group tickets by these topics
        topics = []
        for word, count in top_words:
            # Find tickets that contain this word
            matching_tickets = []
            for ticket in tickets:
                desc = (ticket.get("description") or "").lower()
                title = (ticket.get("title") or "").lower()
                
                if word in desc or word in title:
                    matching_tickets.append({
                        "id": ticket.get("id"),
                        "title": ticket.get("title"),
                        "status": ticket.get("status"),
                        "priority": ticket.get("priority")
                    })
            
            topics.append({
                "topic": word,
                "count": count,
                "tickets": matching_tickets
            })
        
        return topics
    
    def _generate_summary(self, tickets: List[Dict[str, Any]], 
                         topics: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate a summary of tickets.
        
        Args:
            tickets: List of normalized ticket dictionaries
            topics: List of extracted topics
            
        Returns:
            Summary dictionary
        """
        # Count tickets by status
        status_counts = {}
        for ticket in tickets:
            status = ticket.get("status")
            if status:
                if status not in status_counts:
                    status_counts[status] = 0
                status_counts[status] += 1
        
        # Count tickets by priority
        priority_counts = {}
        for ticket in tickets:
            priority = ticket.get("priority")
            if priority:
                if priority not in priority_counts:
                    priority_counts[priority] = 0
                priority_counts[priority] += 1
        
        # Find unassigned tickets
        unassigned = [t for t in tickets if not t.get("assigned_to")]
        
        # Find oldest and newest tickets
        tickets_with_dates = [t for t in tickets if t.get("created_at")]
        oldest = min(tickets_with_dates, key=lambda x: x["created_at"]) if tickets_with_dates else None
        newest = max(tickets_with_dates, key=lambda x: x["created_at"]) if tickets_with_dates else None
        
        # Compile summary
        summary = {
            "total_tickets": len(tickets),
            "status_distribution": status_counts,
            "priority_distribution": priority_counts,
            "top_topics": [t["topic"] for t in topics[:5]] if topics else [],
            "unassigned_count": len(unassigned),
            "oldest_ticket": {
                "id": oldest.get("id"),
                "title": oldest.get("title"),
                "created_at": oldest.get("created_at")
            } if oldest else None,
            "newest_ticket": {
                "id": newest.get("id"),
                "title": newest.get("title"),
                "created_at": newest.get("created_at")
            } if newest else None
        }
        
        return summary














Advanced Document Processor with Multi-Modal Support
"""
Advanced document processor with support for multi-modal content including
text, tables, images, code blocks, and formulas.
"""
import logging
import re
import os
import tempfile
from typing import Dict, Any, List, Optional, Tuple, Union, Generator
from pathlib import Path
import json
import base64
from datetime import datetime
import hashlib
import io

import numpy as np
from bs4 import BeautifulSoup
import pandas as pd
from PIL import Image
try:
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("pytesseract not installed. OCR functionality will be limited.")

try:
    import pdf2image
    PDF_IMAGES_AVAILABLE = True
except ImportError:
    PDF_IMAGES_AVAILABLE = False
    logging.warning("pdf2image not installed. PDF image extraction will be limited.")

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    logging.warning("python-docx not installed. DOCX processing will be limited.")

try:
    import PyPDF2
    PYPDF_AVAILABLE = True
except ImportError:
    PYPDF_AVAILABLE = False
    logging.warning("PyPDF2 not installed. PDF text extraction will be limited.")

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF not installed. Advanced PDF processing will be limited.")

try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False
    logging.warning("tabula-py not installed. PDF table extraction will be limited.")

try:
    import camelot
    CAMELOT_AVAILABLE = True
except ImportError:
    CAMELOT_AVAILABLE = False
    logging.warning("camelot-py not installed. Advanced PDF table extraction will be limited.")

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logging.warning("pdfplumber not installed. PDF table extraction will be limited.")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logging.warning("OpenCV (cv2) not installed. Advanced image processing will be limited.")

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    logging.warning("easyocr not installed. Advanced OCR will be limited.")

logger = logging.getLogger(__name__)

class ContentType:
    """Content type constants."""
    TEXT = "text"
    TABLE = "table"
    IMAGE = "image"
    CODE = "code"
    FORMULA = "formula"
    LIST = "list"
    HEADING = "heading"
    ATTACHMENT = "attachment"
    UNKNOWN = "unknown"

class DocumentContent:
    """
    Class representing content extracted from a document,
    with metadata and content-specific attributes.
    """
    
    def __init__(self, content_type: str, content: Any, 
                 metadata: Optional[Dict[str, Any]] = None,
                 source_document: Optional[str] = None,
                 source_page: Optional[int] = None,
                 source_position: Optional[Dict[str, Any]] = None):
        """
        Initialize document content.
        
        Args:
            content_type: Type of content (text, table, image, etc.)
            content: The content itself (text, DataFrame, image data, etc.)
            metadata: Additional metadata
            source_document: Source document identifier
            source_page: Page number in source document
            source_position: Position in source document (e.g., bounding box)
        """
        self.content_type = content_type
        self.content = content
        self.metadata = metadata or {}
        self.source_document = source_document
        self.source_page = source_page
        self.source_position = source_position or {}
        self.extracted_text = None
        self.embedding = None
        self.id = self._generate_id()
    
    def _generate_id(self) -> str:
        """Generate a unique ID for this content."""
        # Create a hash from content and metadata
        hasher = hashlib.md5()
        
        # Add content type
        hasher.update(self.content_type.encode('utf-8'))
        
        # Add source info
        if self.source_document:
            hasher.update(self.source_document.encode('utf-8'))
        if self.source_page is not None:
            hasher.update(str(self.source_page).encode('utf-8'))
            
        # Add content (if hashable)
        if isinstance(self.content, (str, bytes)):
            hasher.update(str(self.content).encode('utf-8'))
        elif isinstance(self.content, pd.DataFrame):
            hasher.update(str(self.content.shape).encode('utf-8'))
            
        # Add timestamp for uniqueness
        hasher.update(str(datetime.now().timestamp()).encode('utf-8'))
        
        return hasher.hexdigest()
    
    def get_text(self) -> str:
        """Get text representation of content."""
        if self.extracted_text is not None:
            return self.extracted_text
            
        if self.content_type == ContentType.TEXT:
            return str(self.content)
        elif self.content_type == ContentType.TABLE:
            if isinstance(self.content, pd.DataFrame):
                return self._dataframe_to_text(self.content)
            return str(self.content)
        elif self.content_type == ContentType.CODE:
            return str(self.content)
        elif self.content_type == ContentType.LIST:
            return "\n".join([f"- {item}" for item in self.content])
        elif self.content_type == ContentType.HEADING:
            return str(self.content)
        else:
            return self.metadata.get("description", "")
    
    def _dataframe_to_text(self, df: pd.DataFrame) -> str:
        """Convert DataFrame to text representation."""
        if df.empty:
            return ""
            
        # Get table title if available
        title = self.metadata.get("title", "Table")
        
        # Start with title
        text = f"{title}:\n"
        
        # Add column names
        text += "| " + " | ".join(str(col) for col in df.columns) + " |\n"
        
        # Add separator
        text += "| " + " | ".join(["-" * len(str(col)) for col in df.columns]) + " |\n"
        
        # Add rows
        for _, row in df.head(10).iterrows():  # Limit to 10 rows
            text += "| " + " | ".join(str(val) for val in row) + " |\n"
            
        # Add note if truncated
        if len(df) > 10:
            text += f"(Showing 10 of {len(df)} rows)\n"
            
        return text
    
    def get_metadata_str(self) -> str:
        """Get metadata as a string."""
        # Build a summary of metadata
        meta_str = []
        
        # Add source information
        if self.source_document:
            meta_str.append(f"Source: {self.source_document}")
        if self.source_page is not None:
            meta_str.append(f"Page: {self.source_page}")
            
        # Add content-specific metadata
        if self.content_type == ContentType.TABLE:
            if isinstance(self.content, pd.DataFrame):
                meta_str.append(f"Rows: {len(self.content)}")
                meta_str.append(f"Columns: {len(self.content.columns)}")
        elif self.content_type == ContentType.IMAGE:
            meta_str.append(f"Format: {self.metadata.get('format', 'Unknown')}")
            if 'width' in self.metadata and 'height' in self.metadata:
                meta_str.append(f"Dimensions: {self.metadata['width']}x{self.metadata['height']}")
        elif self.content_type == ContentType.CODE:
            meta_str.append(f"Language: {self.metadata.get('language', 'Unknown')}")
        
        return ", ".join(meta_str)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        result = {
            "id": self.id,
            "content_type": self.content_type,
            "metadata": self.metadata,
            "source_document": self.source_document,
            "source_page": self.source_page,
            "source_position": self.source_position,
            "extracted_text": self.extracted_text
        }
        
        # Handle content based on type
        if self.content_type == ContentType.TEXT:
            result["content"] = self.content
        elif self.content_type == ContentType.TABLE:
            if isinstance(self.content, pd.DataFrame):
                result["content"] = self.content.to_dict()
            else:
                result["content"] = str(self.content)
        elif self.content_type == ContentType.IMAGE:
            # Skip binary content for images
            result["content"] = None
        elif self.content_type == ContentType.CODE:
            result["content"] = self.content
        elif self.content_type == ContentType.LIST:
            result["content"] = self.content
        elif self.content_type == ContentType.HEADING:
            result["content"] = self.content
        else:
            result["content"] = str(self.content)
            
        return result
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DocumentContent':
        """Create from dictionary representation."""
        obj = cls(
            content_type=data["content_type"],
            content=data["content"],
            metadata=data["metadata"],
            source_document=data["source_document"],
            source_page=data["source_page"],
            source_position=data["source_position"]
        )
        obj.id = data["id"]
        obj.extracted_text = data["extracted_text"]
        return obj

class Document:
    """Class representing a document with multiple content elements."""
    
    def __init__(self, document_id: str, title: Optional[str] = None, 
                 source_path: Optional[str] = None,
                 metadata: Optional[Dict[str, Any]] = None):
        """
        Initialize document.
        
        Args:
            document_id: Unique document identifier
            title: Document title
            source_path: Path to source file
            metadata: Additional metadata
        """
        self.document_id = document_id
        self.title = title
        self.source_path = source_path
        self.metadata = metadata or {}
        self.contents = []
        
    def add_content(self, content: DocumentContent) -> None:
        """Add content to document."""
        self.contents.append(content)
        
    def get_text_contents(self) -> List[DocumentContent]:
        """Get all text contents."""
        return [c for c in self.contents if c.content_type == ContentType.TEXT]
        
    def get_table_contents(self) -> List[DocumentContent]:
        """Get all table contents."""
        return [c for c in self.contents if c.content_type == ContentType.TABLE]
        
    def get_image_contents(self) -> List[DocumentContent]:
        """Get all image contents."""
        return [c for c in self.contents if c.content_type == ContentType.IMAGE]
        
    def get_code_contents(self) -> List[DocumentContent]:
        """Get all code contents."""
        return [c for c in self.contents if c.content_type == ContentType.CODE]
        
    def get_formula_contents(self) -> List[DocumentContent]:
        """Get all formula contents."""
        return [c for c in self.contents if c.content_type == ContentType.FORMULA]
        
    def get_list_contents(self) -> List[DocumentContent]:
        """Get all list contents."""
        return [c for c in self.contents if c.content_type == ContentType.LIST]
        
    def get_heading_contents(self) -> List[DocumentContent]:
        """Get all heading contents."""
        return [c for c in self.contents if c.content_type == ContentType.HEADING]
        
    def get_all_text(self) -> str:
        """Get all text content as a single string."""
        texts = []
        
        # Get headings first
        headings = self.get_heading_contents()
        for heading in sorted(headings, key=lambda x: x.source_page or 0):
            texts.append(heading.get_text())
            
        # Get regular text
        text_contents = self.get_text_contents()
        for content in sorted(text_contents, key=lambda x: x.source_page or 0):
            texts.append(content.get_text())
            
        # Add text from lists
        list_contents = self.get_list_contents()
        for content in sorted(list_contents, key=lambda x: x.source_page or 0):
            texts.append(content.get_text())
            
        # Add text from tables
        table_contents = self.get_table_contents()
        for content in sorted(table_contents, key=lambda x: x.source_page or 0):
            texts.append(content.get_text())
            
        # Add text from code blocks
        code_contents = self.get_code_contents()
        for content in sorted(code_contents, key=lambda x: x.source_page or 0):
            texts.append(content.get_text())
            
        return "\n\n".join(texts)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "document_id": self.document_id,
            "title": self.title,
            "source_path": self.source_path,
            "metadata": self.metadata,
            "contents": [c.to_dict() for c in self.contents]
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Document':
        """Create from dictionary representation."""
        doc = cls(
            document_id=data["document_id"],
            title=data["title"],
            source_path=data["source_path"],
            metadata=data["metadata"]
        )
        
        for content_data in data["contents"]:
            doc.add_content(DocumentContent.from_dict(content_data))
            
        return doc

class ImageProcessor:
    """Processor for extracting text and information from images."""
    
    def __init__(self, use_advanced_ocr: bool = False):
        """
        Initialize image processor.
        
        Args:
            use_advanced_ocr: Whether to use advanced OCR if available
        """
        self.use_advanced_ocr = use_advanced_ocr
        
        # Initialize OCR readers
        self.easyocr_reader = None
        if use_advanced_ocr and EASYOCR_AVAILABLE:
            self.easyocr_reader = easyocr.Reader(['en'])
    
    def process_image(self, image_path: str) -> Dict[str, Any]:
        """
        Process an image and extract text and metadata.
        
        Args:
            image_path: Path to image file
            
        Returns:
            Dictionary with extracted text and metadata
        """
        if not os.path.exists(image_path):
            logger.error(f"Image file not found: {image_path}")
            return {"error": "File not found"}
            
        try:
            # Open image and get basic metadata
            with Image.open(image_path) as img:
                metadata = {
                    "format": img.format,
                    "mode": img.mode,
                    "width": img.width,
                    "height": img.height
                }
                
                # Extract EXIF data if available
                if hasattr(img, '_getexif') and img._getexif() is not None:
                    exif = img._getexif()
                    exif_data = {}
                    for tag_id, value in exif.items():
                        # Look up the tag name
                        from PIL.ExifTags import TAGS
                        tag = TAGS.get(tag_id, tag_id)
                        exif_data[tag] = str(value)
                    metadata["exif"] = exif_data
                
                # Keep a copy of the image for OCR
                img_copy = img.copy()
                
            # Extract text from image using OCR
            extracted_text = self.extract_text_from_image(img_copy)
            
            # Try to detect if image contains a table
            has_table = self.detect_table_in_image(image_path) if CV2_AVAILABLE else False
            
            return {
                "metadata": metadata,
                "extracted_text": extracted_text,
                "has_table": has_table
            }
            
        except Exception as e:
            logger.error(f"Error processing image {image_path}: {str(e)}")
            return {"error": str(e)}
    
    def extract_text_from_image(self, image: Image.Image) -> str:
        """
        Extract text from an image using OCR.
        
        Args:
            image: PIL Image object
            
        Returns:
            Extracted text
        """
        # Try to use the best available OCR method
        if self.use_advanced_ocr and self.easyocr_reader is not None:
            try:
                # EasyOCR works directly with PIL images
                results = self.easyocr_reader.readtext(np.array(image))
                # Extract text from results (format is different)
                text = " ".join([result[1] for result in results])
                return text
            except Exception as e:
                logger.warning(f"Error using EasyOCR: {str(e)}")
                # Fall back to basic OCR
                
        # Use basic pytesseract OCR if available
        if OCR_AVAILABLE:
            try:
                text = pytesseract.image_to_string(image)
                return text
            except Exception as e:
                logger.warning(f"Error using pytesseract: {str(e)}")
                
        logger.warning("No OCR method available or failed to extract text")
        return ""
    
    def detect_table_in_image(self, image_path: str) -> bool:
        """
        Detect if an image contains a table.
        
        Args:
            image_path: Path to image file
            
        Returns:
            True if a table is detected, False otherwise
        """
        if not CV2_AVAILABLE:
            return False
            
        try:
            # Read image with OpenCV
            img = cv2.imread(image_path)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Apply adaptive thresholding
            thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                          cv2.THRESH_BINARY_INV, 11, 2)
            
            # Find horizontal and vertical lines
            hor = cv2.getStructuringElement(cv2.MORPH_RECT, (img.shape[1]//30, 1))
            ver = cv2.getStructuringElement(cv2.MORPH_RECT, (1, img.shape[0]//30))
            
            horizontal = cv2.erode(thresh, hor, iterations=3)
            horizontal = cv2.dilate(horizontal, hor, iterations=3)
            
            vertical = cv2.erode(thresh, ver, iterations=3)
            vertical = cv2.dilate(vertical, ver, iterations=3)
            
            # Combine horizontal and vertical lines
            combined = cv2.addWeighted(horizontal, 0.5, vertical, 0.5, 0)
            
            # Find contours
            contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Check if we have enough line intersections to form a table
            intersections = cv2.bitwise_and(horizontal, vertical)
            intersection_points = cv2.findNonZero(intersections)
            
            # If we have multiple intersection points and some contours, it's likely a table
            return (intersection_points is not None and 
                   len(intersection_points) > 10 and 
                   len(contours) > 5)
                   
        except Exception as e:
            logger.warning(f"Error detecting table in image: {str(e)}")
            return False
    
    def extract_table_from_image(self, image_path: str) -> Optional[pd.DataFrame]:
        """
        Extract table from an image.
        
        Args:
            image_path: Path to image file
            
        Returns:
            DataFrame with extracted table or None if failed
        """
        if not self.detect_table_in_image(image_path):
            return None
            
        try:
            # For now, we'll just extract text and note that it comes from a table
            # A full implementation would use more sophisticated table extraction
            return None
        except Exception as e:
            logger.warning(f"Error extracting table from image: {str(e)}")
            return None

class PDFProcessor:
    """Processor for extracting content from PDF documents."""
    
    def __init__(self, use_ocr: bool = True):
        """
        Initialize PDF processor.
        
        Args:
            use_ocr: Whether to use OCR for text extraction
        """
        self.use_ocr = use_ocr
        self.image_processor = ImageProcessor()
    
    def extract_text(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract text from PDF.
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            Dictionary with extracted text by page
        """
        if not os.path.exists(pdf_path):
            logger.error(f"PDF file not found: {pdf_path}")
            return {"error": "File not found"}
            
        # Try different PDF extraction methods in order of quality
        if PYMUPDF_AVAILABLE:
            return self._extract_text_pymupdf(pdf_path)
        elif PYPDF_AVAILABLE:
            return self._extract_text_pypdf(pdf_path)
        else:
            logger.error("No PDF extraction libraries available")
            return {"error": "No PDF extraction libraries available"}
    
    def _extract_text_pymupdf(self, pdf_path: str) -> Dict[str, Any]:
        """Extract text using PyMuPDF (higher quality)."""
        try:
            doc = fitz.open(pdf_path)
            result = {
                "metadata": {
                    "title": doc.metadata.get("title", ""),
                    "author": doc.metadata.get("author", ""),
                    "subject": doc.metadata.get("subject", ""),
                    "keywords": doc.metadata.get("keywords", ""),
                    "creator": doc.metadata.get("creator", ""),
                    "producer": doc.metadata.get("producer", ""),
                    "page_count": len(doc)
                },
                "pages": {}
            }
            
            for page_num, page in enumerate(doc):
                # Extract text
                text = page.get_text()
                
                # Extract headings based on font size
                blocks = page.get_text("dict")["blocks"]
                headings = []
                
                for block in blocks:
                    if "lines" in block:
                        for line in block["lines"]:
                            if "spans" in line:
                                for span in line["spans"]:
                                    # Check if this looks like a heading
                                    # (larger font, bold, not too long)
                                    if (span.get("size", 0) > 11 and 
                                        span.get("flags", 0) & 16 and  # Check bold flag
                                        len(span.get("text", "")) < 100):
                                        headings.append({
                                            "text": span["text"],
                                            "size": span["size"],
                                            "position": {
                                                "x": span["origin"][0],
                                                "y": span["origin"][1]
                                            }
                                        })
                
                # Save page data
                result["pages"][str(page_num + 1)] = {
                    "text": text,
                    "headings": headings
                }
                
                # Extract images if use_ocr is enabled
                if self.use_ocr:
                    images = []
                    for img_index, img in enumerate(page.get_images(full=True)):
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        
                        # Save to temporary file
                        with tempfile.NamedTemporaryFile(suffix=f".{base_image['ext']}", delete=False) as temp_img:
                            temp_img.write(image_bytes)
                            img_path = temp_img.name
                        
                        # Process the image
                        image_data = self.image_processor.process_image(img_path)
                        
                        # Delete temporary file
                        try:
                            os.unlink(img_path)
                        except Exception:
                            pass
                            
                        # Save image data
                        images.append({
                            "index": img_index,
                            "extracted_text": image_data.get("extracted_text", ""),
                            "metadata": image_data.get("metadata", {}),
                            "has_table": image_data.get("has_table", False)
                        })
                    
                    result["pages"][str(page_num + 1)]["images"] = images
            
            return result
            
        except Exception as e:
            logger.error(f"Error extracting text from PDF with PyMuPDF: {str(e)}")
            return {"error": str(e)}
    
    def _extract_text_pypdf(self, pdf_path: str) -> Dict[str, Any]:
        """Extract text using PyPDF2 (basic extraction)."""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                
                result = {
                    "metadata": {
                        "title": reader.metadata.get("/Title", ""),
                        "author": reader.metadata.get("/Author", ""),
                        "subject": reader.metadata.get("/Subject", ""),
                        "keywords": reader.metadata.get("/Keywords", ""),
                        "creator": reader.metadata.get("/Creator", ""),
                        "producer": reader.metadata.get("/Producer", ""),
                        "page_count": len(reader.pages)
                    },
                    "pages": {}
                }
                
                for page_num, page in enumerate(reader.pages):
                    text = page.extract_text()
                    result["pages"][str(page_num + 1)] = {"text": text}
                
                return result
                
        except Exception as e:
            logger.error(f"Error extracting text from PDF with PyPDF2: {str(e)}")
            return {"error": str(e)}
    
    def extract_tables(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract tables from PDF.
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            Dictionary with extracted tables by page
        """
        if not os.path.exists(pdf_path):
            logger.error(f"PDF file not found: {pdf_path}")
            return {"error": "File not found"}
            
        # Try different table extraction methods in order of quality
        if CAMELOT_AVAILABLE:
            return self._extract_tables_camelot(pdf_path)
        elif TABULA_AVAILABLE:
            return self._extract_tables_tabula(pdf_path)
        elif PDFPLUMBER_AVAILABLE:
            return self._extract_tables_pdfplumber(pdf_path)
        else:
            logger.warning("No PDF table extraction libraries available")
            return {"tables": {}}
    
    def _extract_tables_camelot(self, pdf_path: str) -> Dict[str, Any]:
        """Extract tables using Camelot (higher quality)."""
        try:
            tables_by_page = {}
            
            # Extract tables
            tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')
            
            for i, table in enumerate(tables):
                page_num = table.parsing_report['page']
                
                if page_num not in tables_by_page:
                    tables_by_page[page_num] = []
                    
                # Convert to pandas DataFrame
                df = table.df
                
                # Add to results
                tables_by_page[page_num].append({
                    "index": i,
                    "dataframe": df,
                    "accuracy": table.parsing_report['accuracy'],
                    "whitespace": table.parsing_report['whitespace']
                })
                
            return {"tables": tables_by_page}
            
        except Exception as e:
            logger.error(f"Error extracting tables from PDF with Camelot: {str(e)}")
            return {"tables": {}}
    
    def _extract_tables_tabula(self, pdf_path: str) -> Dict[str, Any]:
        """Extract tables using Tabula."""
        try:
            # Extract tables from all pages
            all_tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)
            
            tables_by_page = {}
            
            # Tabula doesn't return page numbers, so we'll have to estimate
            page_num = 1
            tables_per_page = 0
            
            for i, df in enumerate(all_tables):
                if not isinstance(df, pd.DataFrame) or df.empty:
                    continue
                    
                # Increment page number periodically
                tables_per_page += 1
                if tables_per_page > 3:  # Assume at most 3 tables per page
                    page_num += 1
                    tables_per_page = 1
                
                if page_num not in tables_by_page:
                    tables_by_page[page_num] = []
                    
                # Add to results
                tables_by_page[page_num].append({
                    "index": i,
                    "dataframe": df
                })
                
            return {"tables": tables_by_page}
            
        except Exception as e:
            logger.error(f"Error extracting tables from PDF with Tabula: {str(e)}")
            return {"tables": {}}
    
    def _extract_tables_pdfplumber(self, pdf_path: str) -> Dict[str, Any]:
        """Extract tables using PDFPlumber."""
        try:
            tables_by_page = {}
            
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    tables = page.extract_tables()
                    
                    if not tables:
                        continue
                        
                    if page_num + 1 not in tables_by_page:
                        tables_by_page[page_num + 1] = []
                        
                    for i, table_data in enumerate(tables):
                        # Convert to pandas DataFrame
                        df = pd.DataFrame(table_data[1:], columns=table_data[0])
                        
                        # Add to results
                        tables_by_page[page_num + 1].append({
                            "index": i,
                            "dataframe": df
                        })
                
            return {"tables": tables_by_page}
            
        except Exception as e:
            logger.error(f"Error extracting tables from PDF with PDFPlumber: {str(e)}")
            return {"tables": {}}
    
    def extract_images(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract images from PDF.
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            Dictionary with extracted images by page
        """
        if not os.path.exists(pdf_path):
            logger.error(f"PDF file not found: {pdf_path}")
            return {"error": "File not found"}
            
        # Try different image extraction methods in order of quality
        if PYMUPDF_AVAILABLE:
            return self._extract_images_pymupdf(pdf_path)
        elif PDF_IMAGES_AVAILABLE:
            return self._extract_images_pdf2image(pdf_path)
        else:
            logger.warning("No PDF image extraction libraries available")
            return {"images": {}}
    
    def _extract_images_pymupdf(self, pdf_path: str) -> Dict[str, Any]:
        """Extract images using PyMuPDF."""
        try:
            images_by_page = {}
            
            doc = fitz.open(pdf_path)
            
            for page_num, page in enumerate(doc):
                images = []
                
                # Extract images
                for img_index, img in enumerate(page.get_images(full=True)):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # Save to temporary file
                    with tempfile.NamedTemporaryFile(suffix=f".{base_image['ext']}", delete=False) as temp_img:
                        temp_img.write(image_bytes)
                        img_path = temp_img.name
                    
                    # Process the image
                    image_data = self.image_processor.process_image(img_path)
                    
                    # Delete temporary file
                    try:
                        os.unlink(img_path)
                    except Exception:
                        pass
                        
                    # Save image data
                    images.append({
                        "index": img_index,
                        "extracted_text": image_data.get("extracted_text", ""),
                        "metadata": image_data.get("metadata", {}),
                        "has_table": image_data.get("has_table", False)
                    })
                
                if images:
                    images_by_page[page_num + 1] = images
            
            return {"images": images_by_page}
            
        except Exception as e:
            logger.error(f"Error extracting images from PDF with PyMuPDF: {str(e)}")
            return {"images": {}}
    
    def _extract_images_pdf2image(self, pdf_path: str) -> Dict[str, Any]:
        """Extract images by converting PDF pages to images."""
        try:
            images_by_page = {}
            
            # Convert PDF to images
            images = pdf2image.convert_from_path(pdf_path)
            
            for page_num, img in enumerate(images):
                # Save to temporary file
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as temp_img:
                    img.save(temp_img.name, format="PNG")
                    img_path = temp_img.name
                
                # Process the image
                image_data = self.image_processor.process_image(img_path)
                
                # Delete temporary file
                try:
                    os.unlink(img_path)
                except Exception:
                    pass
                    
                images_by_page[page_num + 1] = [{
                    "index": 0,
                    "extracted_text": image_data.get("extracted_text", ""),
                    "metadata": image_data.get("metadata", {}),
                    "has_table": image_data.get("has_table", False)
                }]
            
            return {"images": images_by_page}
            
        except Exception as e:
            logger.error(f"Error extracting images from PDF with pdf2image: {str(e)}")
            return {"images": {}}
    
    def process_pdf(self, pdf_path: str) -> Document:
        """
        Process a PDF file and extract all content.
        
        Args:
            pdf_path: Path to PDF file
            
        Returns:
            Document object with extracted content
        """
        if not os.path.exists(pdf_path):
            logger.error(f"PDF file not found: {pdf_path}")
            return Document(document_id=os.path.basename(pdf_path), 
                           source_path=pdf_path,
                           metadata={"error": "File not found"})
            
        # Extract text
        text_data = self.extract_text(pdf_path)
        
        # Extract tables
        table_data = self.extract_tables(pdf_path)
        
        # Extract images
        image_data = self.extract_images(pdf_path)
        
        # Create document
        doc_id = os.path.basename(pdf_path)
        title = text_data.get("metadata", {}).get("title", doc_id)
        
        document = Document(
            document_id=doc_id,
            title=title,
            source_path=pdf_path,
            metadata=text_data.get("metadata", {})
        )
        
        # Add text content
        for page_num, page_data in text_data.get("pages", {}).items():
            # Add headings
            for heading in page_data.get("headings", []):
                document.add_content(
                    DocumentContent(
                        content_type=ContentType.HEADING,
                        content=heading["text"],
                        metadata={"size": heading["size"]},
                        source_document=doc_id,
                        source_page=int(page_num),
                        source_position=heading.get("position")
                    )
                )
            
            # Add main text
            document.add_content(
                DocumentContent(
                    content_type=ContentType.TEXT,
                    content=page_data["text"],
                    source_document=doc_id,
                    source_page=int(page_num)
                )
            )
        
        # Add table content
        for page_num, tables in table_data.get("tables", {}).items():
            for table in tables:
                document.add_content(
                    DocumentContent(
                        content_type=ContentType.TABLE,
                        content=table["dataframe"],
                        metadata={"index": table["index"]},
                        source_document=doc_id,
                        source_page=int(page_num)
                    )
                )
        
        # Add image content
        for page_num, images in image_data.get("images", {}).items():
            for image in images:
                # If image contains a table, add as TABLE type
                if image.get("has_table", False):
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TABLE,
                            content=image.get("extracted_text", ""),
                            metadata={"from_image": True, **image.get("metadata", {})},
                            source_document=doc_id,
                            source_page=int(page_num)
                        )
                    )
                else:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.IMAGE,
                            content=None,  # Don't store actual image data
                            metadata=image.get("metadata", {}),
                            source_document=doc_id,
                            source_page=int(page_num),
                            extracted_text=image.get("extracted_text", "")
                        )
                    )
        
        return document

class DOCXProcessor:
    """Processor for extracting content from DOCX documents."""
    
    def __init__(self):
        """Initialize DOCX processor."""
        if not DOCX_AVAILABLE:
            logger.warning("python-docx not installed. DOCX processing will be limited.")
            
        self.image_processor = ImageProcessor()
    
    def process_docx(self, docx_path: str) -> Document:
        """
        Process a DOCX file and extract all content.
        
        Args:
            docx_path: Path to DOCX file
            
        Returns:
            Document object with extracted content
        """
        if not DOCX_AVAILABLE:
            logger.error("python-docx not installed. Cannot process DOCX files.")
            return Document(document_id=os.path.basename(docx_path), 
                           source_path=docx_path,
                           metadata={"error": "python-docx not installed"})
            
        if not os.path.exists(docx_path):
            logger.error(f"DOCX file not found: {docx_path}")
            return Document(document_id=os.path.basename(docx_path), 
                           source_path=docx_path,
                           metadata={"error": "File not found"})
            
        try:
            # Open document
            doc = docx.Document(docx_path)
            
            # Extract metadata
            core_properties = doc.core_properties
            metadata = {
                "title": core_properties.title,
                "author": core_properties.author,
                "subject": core_properties.subject,
                "keywords": core_properties.keywords,
                "created": core_properties.created,
                "modified": core_properties.modified,
                "last_modified_by": core_properties.last_modified_by,
                "revision": core_properties.revision,
                "paragraph_count": len(doc.paragraphs),
                "table_count": len(doc.tables)
            }
            
            # Create document
            doc_id = os.path.basename(docx_path)
            title = metadata.get("title", doc_id)
            
            document = Document(
                document_id=doc_id,
                title=title,
                source_path=docx_path,
                metadata=metadata
            )
            
            # Process paragraphs
            for i, para in enumerate(doc.paragraphs):
                # Skip empty paragraphs
                if not para.text.strip():
                    continue
                    
                # Check if it's a heading
                if para.style.name.startswith('Heading'):
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.HEADING,
                            content=para.text,
                            metadata={"style": para.style.name, "level": int(para.style.name[-1])},
                            source_document=doc_id,
                            source_page=None,  # DOCX doesn't have page numbers
                            source_position={"index": i}
                        )
                    )
                else:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=para.text,
                            metadata={"style": para.style.name},
                            source_document=doc_id,
                            source_page=None,
                            source_position={"index": i}
                        )
                    )
            
            # Process tables
            for i, table in enumerate(doc.tables):
                # Extract data
                data = []
                for row in table.rows:
                    row_data = [cell.text for cell in row.cells]
                    data.append(row_data)
                    
                # Convert to DataFrame
                df = pd.DataFrame(data[1:], columns=data[0] if data else None)
                
                document.add_content(
                    DocumentContent(
                        content_type=ContentType.TABLE,
                        content=df,
                        metadata={"index": i},
                        source_document=doc_id,
                        source_page=None
                    )
                )
            
            # Process images (requires additional libraries)
            # Not implemented here due to complexity
            
            return document
            
        except Exception as e:
            logger.error(f"Error processing DOCX file {docx_path}: {str(e)}")
            return Document(document_id=os.path.basename(docx_path), 
                           source_path=docx_path,
                           metadata={"error": str(e)})

class TableProcessor:
    """Processor for working with table data."""
    
    def __init__(self):
        """Initialize table processor."""
        pass
    
    def extract_structured_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Extract structured data from a DataFrame.
        
        Args:
            df: DataFrame to analyze
            
        Returns:
            Dictionary with extracted data
        """
        if df.empty:
            return {"error": "Empty table"}
            
        try:
            # Get basic stats
            stats = {
                "row_count": len(df),
                "column_count": len(df.columns),
                "columns": list(df.columns)
            }
            
            # Analyze column types
            column_types = {}
            numeric_columns = []
            text_columns = []
            date_columns = []
            
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    column_types[col] = "numeric"
                    numeric_columns.append(col)
                elif pd.api.types.is_datetime64_dtype(df[col]):
                    column_types[col] = "datetime"
                    date_columns.append(col)
                else:
                    # Try to infer if it's a date string
                    try:
                        if df[col].str.match(r'\d{1,4}[-/]\d{1,2}[-/]\d{1,4}').any():
                            pd.to_datetime(df[col], errors='coerce')
                            if df[col].notna().any():
                                column_types[col] = "date_string"
                                date_columns.append(col)
                            else:
                                column_types[col] = "text"
                                text_columns.append(col)
                        else:
                            column_types[col] = "text"
                            text_columns.append(col)
                    except:
                        column_types[col] = "text"
                        text_columns.append(col)
            
            stats["column_types"] = column_types
            
            # Get statistics for numeric columns
            numeric_stats = {}
            for col in numeric_columns:
                numeric_stats[col] = {
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "median": float(df[col].median()),
                    "std": float(df[col].std())
                }
            
            # Get unique values count for categorical columns
            categorical_stats = {}
            for col in text_columns:
                value_counts = df[col].value_counts().to_dict()
                
                # Limit to top 10 values
                top_values = dict(sorted(value_counts.items(), 
                                         key=lambda x: x[1], 
                                         reverse=True)[:10])
                
                categorical_stats[col] = {
                    "unique_count": df[col].nunique(),
                    "top_values": top_values
                }
            
            # Get date range for date columns
            date_stats = {}
            for col in date_columns:
                if column_types[col] == "datetime":
                    date_stats[col] = {
                        "min": df[col].min().strftime("%Y-%m-%d") if not pd.isna(df[col].min()) else None,
                        "max": df[col].max().strftime("%Y-%m-%d") if not pd.isna(df[col].max()) else None
                    }
                else:
                    # For string dates, convert to datetime first
                    dates = pd.to_datetime(df[col], errors='coerce')
                    date_stats[col] = {
                        "min": dates.min().strftime("%Y-%m-%d") if not pd.isna(dates.min()) else None,
                        "max": dates.max().strftime("%Y-%m-%d") if not pd.isna(dates.max()) else None
                    }
            
            # Compile final result
            result = {
                "stats": stats,
                "numeric_stats": numeric_stats,
                "categorical_stats": categorical_stats,
                "date_stats": date_stats,
                "sample_data": df.head(5).to_dict(orient="records")
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Error extracting structured data from table: {str(e)}")
            return {"error": str(e)}
    
    def table_to_text(self, df: pd.DataFrame, include_stats: bool = True) -> str:
        """
        Convert table to textual description.
        
        Args:
            df: DataFrame to convert
            include_stats: Whether to include statistics
            
        Returns:
            Textual description of table
        """
        if df.empty:
            return "Empty table"
            
        try:
            # Start with basic description
            lines = [f"Table with {len(df)} rows and {len(df.columns)} columns."]
            lines.append(f"Columns: {', '.join(str(col) for col in df.columns)}")
            
            # Add sample data
            lines.append("\nSample data:")
            for i, row in df.head(5).iterrows():
                lines.append(f"Row {i+1}: {' | '.join(str(val) for val in row)}")
                
            # Add statistics if requested
            if include_stats:
                # Get structured data
                data = self.extract_structured_data(df)
                
                # Add numeric stats
                if data.get("numeric_stats"):
                    lines.append("\nNumeric column statistics:")
                    for col, stats in data["numeric_stats"].items():
                        lines.append(f"{col}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}")
                
                # Add categorical stats
                if data.get("categorical_stats"):
                    lines.append("\nCategorical column statistics:")
                    for col, stats in data["categorical_stats"].items():
                        top_values = ", ".join(f"{k} ({v})" for k, v in list(stats["top_values"].items())[:3])
                        lines.append(f"{col}: {stats['unique_count']} unique values, top: {top_values}")
                
                # Add date stats
                if data.get("date_stats"):
                    lines.append("\nDate column statistics:")
                    for col, stats in data["date_stats"].items():
                        if stats["min"] and stats["max"]:
                            lines.append(f"{col}: range from {stats['min']} to {stats['max']}")
            
            return "\n".join(lines)
            
        except Exception as e:
            logger.error(f"Error converting table to text: {str(e)}")
            return f"Error processing table: {str(e)}"

class CodeProcessor:
    """Processor for extracting and analyzing code."""
    
    def __init__(self):
        """Initialize code processor."""
        # Map of file extensions to languages
        self.language_map = {
            "py": "python",
            "js": "javascript",
            "java": "java",
            "c": "c",
            "cpp": "c++",
            "cs": "c#",
            "php": "php",
            "rb": "ruby",
            "go": "go",
            "rs": "rust",
            "ts": "typescript",
            "html": "html",
            "css": "css",
            "sql": "sql",
            "sh": "bash",
            "yaml": "yaml",
            "yml": "yaml",
            "json": "json",
            "md": "markdown",
            "xml": "xml"
        }
    
    def detect_language(self, code: str, filename: Optional[str] = None) -> str:
        """
        Detect programming language from code snippet or filename.
        
        Args:
            code: Code snippet
            filename: Optional filename
            
        Returns:
            Detected language
        """
        # Try to detect from filename
        if filename:
            ext = filename.split('.')[-1].lower()
            if ext in self.language_map:
                return self.language_map[ext]
                
        # Simple heuristics for common languages
        code = code.strip()
        
        # Python
        if code.startswith('import ') or code.startswith('from ') or code.startswith('def ') or code.startswith('class '):
            return 'python'
            
        # JavaScript/TypeScript
        if 'function ' in code or 'const ' in code or 'let ' in code or 'var ' in code or 'export ' in code:
            if '.tsx' in code or '<' in code:
                return 'typescript'
            return 'javascript'
            
        # Java
        if 'public class ' in code or 'private class ' in code or 'protected class ' in code:
            return 'java'
            
        # C/C++
        if '#include ' in code:
            if 'cout' in code or 'namespace' in code or '::' in code:
                return 'c++'
            return 'c'
            
        # HTML
        if code.startswith('<!DOCTYPE html>') or '<html' in code:
            return 'html'
            
        # SQL
        if 'SELECT ' in code.upper() or 'CREATE TABLE ' in code.upper():
            return 'sql'
            
        # Markdown
        if code.startswith('# ') or '## ' in code:
            return 'markdown'
            
        # XML
        if code.startswith('<?xml ') or '<' in code and '>' in code:
            return 'xml'
            
        # Default to text
        return 'text'
    
    def extract_docstrings(self, code: str, language: str) -> List[str]:
        """
        Extract docstrings or comments from code.
        
        Args:
            code: Code snippet
            language: Programming language
            
        Returns:
            List of extracted docstrings
        """
        docstrings = []
        
        if language == 'python':
            # Python docstrings
            docstring_patterns = [
                r'"""(.*?)"""',  # Multi-line docstrings
                r"'''(.*?)'''",  # Alternative multi-line docstrings
                r'#\s*(.*?)$'    # Single-line comments
            ]
            
            for pattern in docstring_patterns:
                matches = re.findall(pattern, code, re.DOTALL | re.MULTILINE)
                docstrings.extend(matches)
                
        elif language in ['javascript', 'typescript', 'java', 'c++', 'c#']:
            # C-style comments
            docstring_patterns = [
                r'/\*\*(.*?)\*/',  # JavaDoc comments
                r'/\*(.*?)\*/',    # Block comments
                r'//\s*(.*?)$'     # Single-line comments
            ]
            
            for pattern in docstring_patterns:
                matches = re.findall(pattern, code, re.DOTALL | re.MULTILINE)
                docstrings.extend(matches)
                
        elif language == 'html' or language == 'xml':
            # HTML/XML comments
            matches = re.findall(r'<!--(.*?)-->', code, re.DOTALL)
            docstrings.extend(matches)
            
        # Clean up docstrings
        docstrings = [ds.strip() for ds in docstrings if ds.strip()]
        
        return docstrings
    
    def extract_functions(self, code: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract function definitions from code.
        
        Args:
            code: Code snippet
            language: Programming language
            
        Returns:
            List of extracted functions with names and signatures
        """
        functions = []
        
        if language == 'python':
            # Python functions
            func_pattern = r'def\s+([a-zA-Z0-9_]+)\s*\((.*?)\)(?:\s*->\s*([a-zA-Z0-9_\[\],\s]+))?\s*:'
            matches = re.findall(func_pattern, code)
            
            for match in matches:
                name, params, return_type = match
                functions.append({
                    "name": name,
                    "params": params.strip(),
                    "return_type": return_type.strip() if return_type else "None",
                    "language": language
                })
                
        elif language in ['javascript', 'typescript']:
            # JavaScript/TypeScript functions
            func_patterns = [
                r'function\s+([a-zA-Z0-9_]+)\s*\((.*?)\)',  # function name()
                r'const\s+([a-zA-Z0-9_]+)\s*=\s*\((.*?)\)\s*=>', # const name = () =>
                r'let\s+([a-zA-Z0-9_]+)\s*=\s*\((.*?)\)\s*=>', # let name = () =>
                r'var\s+([a-zA-Z0-9_]+)\s*=\s*\((.*?)\)\s*=>', # var name = () =>
                r'([a-zA-Z0-9_]+)\s*=\s*\((.*?)\)\s*=>', # name = () =>
                r'([a-zA-Z0-9_]+)\s*\((.*?)\)\s*{' # name() {
            ]
            
            for pattern in func_patterns:
                matches = re.findall(pattern, code)
                for match in matches:
                    name, params = match
                    functions.append({
                        "name": name,
                        "params": params.strip(),
                        "language": language
                    })
                    
        elif language in ['java', 'c++', 'c#']:
            # Java/C++/C# methods
            func_pattern = r'(?:public|private|protected)?\s+(?:static\s+)?([a-zA-Z0-9_<>]+)\s+([a-zA-Z0-9_]+)\s*\((.*?)\)'
            matches = re.findall(func_pattern, code)
            
            for match in matches:
                return_type, name, params = match
                functions.append({
                    "name": name,
                    "params": params.strip(),
                    "return_type": return_type.strip(),
                    "language": language
                })
                
        return functions
    
    def extract_classes(self, code: str, language: str) -> List[Dict[str, Any]]:
        """
        Extract class definitions from code.
        
        Args:
            code: Code snippet
            language: Programming language
            
        Returns:
            List of extracted classes
        """
        classes = []
        
        if language == 'python':
            # Python classes
            class_pattern = r'class\s+([a-zA-Z0-9_]+)(?:\((.*?)\))?\s*:'
            matches = re.findall(class_pattern, code)
            
            for match in matches:
                name, inherits = match
                classes.append({
                    "name": name,
                    "inherits": inherits.strip(),
                    "language": language
                })
                
        elif language in ['java', 'c++', 'c#']:
            # Java/C++/C# classes
            class_pattern = r'(?:public|private|protected)?\s+class\s+([a-zA-Z0-9_]+)(?:\s+extends\s+([a-zA-Z0-9_]+))?(?:\s+implements\s+([a-zA-Z0-9_, ]+))?'
            matches = re.findall(class_pattern, code)
            
            for match in matches:
                if len(match) == 3:
                    name, extends, implements = match
                    classes.append({
                        "name": name,
                        "extends": extends.strip(),
                        "implements": implements.strip(),
                        "language": language
                    })
                else:
                    name = match[0]
                    classes.append({
                        "name": name,
                        "language": language
                    })
                    
        elif language in ['javascript', 'typescript']:
            # JavaScript/TypeScript classes
            class_pattern = r'class\s+([a-zA-Z0-9_]+)(?:\s+extends\s+([a-zA-Z0-9_]+))?'
            matches = re.findall(class_pattern, code)
            
            for match in matches:
                name, extends = match
                classes.append({
                    "name": name,
                    "extends": extends.strip(),
                    "language": language
                })
                
        return classes
    
    def analyze_code(self, code: str, filename: Optional[str] = None) -> Dict[str, Any]:
        """
        Analyze code snippet and extract useful information.
        
        Args:
            code: Code snippet
            filename: Optional filename
            
        Returns:
            Dictionary with code analysis
        """
        # Detect language
        language = self.detect_language(code, filename)
        
        # Extract docstrings
        docstrings = self.extract_docstrings(code, language)
        
        # Extract functions
        functions = self.extract_functions(code, language)
        
        # Extract classes
        classes = self.extract_classes(code, language)
        
        # Get lines of code
        loc = len(code.split('\n'))
        
        # Analyze imports/dependencies
        imports = []
        if language == 'python':
            import_patterns = [
                r'import\s+([a-zA-Z0-9_\.]+)',
                r'from\s+([a-zA-Z0-9_\.]+)\s+import'
            ]
            for pattern in import_patterns:
                imports.extend(re.findall(pattern, code))
        elif language in ['javascript', 'typescript']:
            import_patterns = [
                r'import\s+.*?from\s+[\'"]([a-zA-Z0-9_\-\.\/]+)[\'"]',
                r'require\([\'"]([a-zA-Z0-9_\-\.\/]+)[\'"]\)'
            ]
            for pattern in import_patterns:
                imports.extend(re.findall(pattern, code))
        
        return {
            "language": language,
            "lines_of_code": loc,
            "functions": functions,
            "classes": classes,
            "docstrings": docstrings,
            "imports": imports
        }
    
    def get_code_summary(self, code: str, filename: Optional[str] = None) -> str:
        """
        Generate a summary of the code.
        
        Args:
            code: Code snippet
            filename: Optional filename
            
        Returns:
            Summary string
        """
        analysis = self.analyze_code(code, filename)
        
        lines = []
        
        # Basic info
        lines.append(f"{analysis['language'].capitalize()} code with {analysis['lines_of_code']} lines.")
        
        # Functions
        if analysis['functions']:
            func_names = [f['name'] for f in analysis['functions']]
            lines.append(f"Contains {len(func_names)} functions: {', '.join(func_names[:5])}" + 
                        (f" and {len(func_names) - 5} more" if len(func_names) > 5 else ""))
                        
        # Classes
        if analysis['classes']:
            class_names = [c['name'] for c in analysis['classes']]
            lines.append(f"Contains {len(class_names)} classes: {', '.join(class_names)}")
            
        # Imports
        if analysis['imports']:
            lines.append(f"Imports {len(analysis['imports'])} modules: " + 
                        ", ".join(analysis['imports'][:5]) + 
                        (f" and {len(analysis['imports']) - 5} more" if len(analysis['imports']) > 5 else ""))
                        
        # Purpose from docstrings
        if analysis['docstrings']:
            # Use the first substantial docstring
            for ds in analysis['docstrings']:
                if len(ds) > 20:  # Minimum size for meaningful docstring
                    lines.append(f"Purpose: {ds.split('.')[0]}")  # First sentence
                    break
        
        return "\n".join(lines)

class FormulaProcessor:
    """Processor for extracting and analyzing mathematical formulas."""
    
    def __init__(self):
        """Initialize formula processor."""
        pass
    
    def detect_formula(self, text: str) -> bool:
        """
        Detect if text contains a mathematical formula.
        
        Args:
            text: Text to analyze
            
        Returns:
            True if text likely contains a formula, False otherwise
        """
        # Look for LaTeX-style formulas
        latex_patterns = [
            r'\$\$.*?\$\$',  # Display math
            r'\$.*?\$',      # Inline math
            r'\\begin\{equation\}.*?\\end\{equation\}',  # Equation environment
            r'\\begin\{align\}.*?\\end\{align\}'         # Align environment
        ]
        
        for pattern in latex_patterns:
            if re.search(pattern, text, re.DOTALL):
                return True
                
        # Look for common mathematical symbols
        math_symbols = [
            r'[=<>]',  # Equality and inequality
            r'[+\-*/%]', # Basic operators
            r'[]', # Calculus symbols
            r'[]', # Set operators
            r'[]',      # Roots
            r'[]',       # Summation, product
            r'[]', # Greek letters
            r'\^[0-9a-z]+', # Exponents
            r'_[0-9a-z]+',  # Subscripts
            r'\([^)]+\)',   # Parentheses with content
            r'\[[^\]]+\]'   # Brackets with content
        ]
        
        # Count matches
        symbol_count = 0
        for pattern in math_symbols:
            matches = re.findall(pattern, text)
            symbol_count += len(matches)
            
        # If we have multiple math symbols, likely a formula
        return symbol_count >= 3
    
    def extract_formulas(self, text: str) -> List[str]:
        """
        Extract mathematical formulas from text.
        
        Args:
            text: Text to analyze
            
        Returns:
            List of extracted formulas
        """
        formulas = []
        
        # Look for LaTeX-style formulas
        latex_patterns = [
            (r'\$\$(.*?)\$\$', "display"),                       # Display math
            (r'\$(.*?)\$', "inline"),                            # Inline math
            (r'\\begin\{equation\}(.*?)\\end\{equation\}', "equation"),  # Equation environment
            (r'\\begin\{align\}(.*?)\\end\{align\}', "align")    # Align environment
        ]
        
        for pattern, formula_type in latex_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                formulas.append({"type": formula_type, "latex": match, "format": "latex"})
                
        # If no LaTeX formulas found, look for plaintext formulas
        if not formulas:
            # Split text by lines and check each line
            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                # Skip short lines or lines with too much text
                if len(line) < 5 or len(line.split()) > 20:
                    continue
                    
                if self.detect_formula(line):
                    formulas.append({"type": "plaintext", "formula": line, "format": "text"})
        
        return formulas
    
    def formula_to_text(self, formula: Dict[str, str]) -> str:
        """
        Convert formula to descriptive text.
        
        Args:
            formula: Formula dictionary
            
        Returns:
            Descriptive text of formula
        """
        if formula["format"] == "text":
            return f"Formula: {formula['formula']}"
            
        # For LaTeX formulas, attempt to describe in plain language
        latex = formula["latex"]
        
        # Remove LaTeX-specific commands
        latex = re.sub(r'\\text\{([^}]*)\}', r'\1', latex)  # Extract text
        latex = re.sub(r'\\[a-zA-Z]+', ' ', latex)  # Remove commands
        latex = re.sub(r'[{}]', '', latex)  # Remove braces
        
        return f"Mathematical formula (in LaTeX): {latex}"

class DocumentProcessor:
    """Main processor for handling various document types."""
    
    def __init__(self, cache_dir: Optional[str] = "cache"):
        """
        Initialize document processor.
        
        Args:
            cache_dir: Directory for cached data
        """
        self.cache_dir = cache_dir
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
            
        # Initialize sub-processors
        self.pdf_processor = PDFProcessor()
        self.docx_processor = DOCXProcessor()
        self.image_processor = ImageProcessor()
        self.table_processor = TableProcessor()
        self.code_processor = CodeProcessor()
        self.formula_processor = FormulaProcessor()
    
    def process_document(self, file_path: str) -> Document:
        """
        Process a document file.
        
        Args:
            file_path: Path to document file
            
        Returns:
            Document object with extracted content
        """
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return Document(document_id=os.path.basename(file_path), 
                           source_path=file_path,
                           metadata={"error": "File not found"})
                           
        # Check cache first
        doc_id = os.path.basename(file_path)
        cache_path = os.path.join(self.cache_dir, f"{doc_id}.json")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r") as f:
                    doc_data = json.load(f)
                    return Document.from_dict(doc_data)
            except Exception as e:
                logger.warning(f"Error loading cached document: {str(e)}")
        
        # Determine file type
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == ".pdf":
            document = self.pdf_processor.process_pdf(file_path)
        elif ext == ".docx":
            document = self.docx_processor.process_docx(file_path)
        elif ext in [".jpg", ".jpeg", ".png", ".gif", ".bmp"]:
            document = self._process_image(file_path)
        elif ext in [".csv", ".xls", ".xlsx"]:
            document = self._process_tabular(file_path)
        elif ext in [".py", ".js", ".java", ".cpp", ".cs", ".php", ".html", ".css"]:
            document = self._process_code(file_path)
        elif ext == ".txt":
            document = self._process_text(file_path)
        elif ext in [".json", ".xml", ".yaml", ".yml"]:
            document = self._process_structured_data(file_path)
        else:
            logger.warning(f"Unsupported file type: {ext}")
            document = self._process_unknown(file_path)
            
        # Cache the processed document
        try:
            with open(cache_path, "w") as f:
                json.dump(document.to_dict(), f)
        except Exception as e:
            logger.warning(f"Error caching document: {str(e)}")
            
        return document
    
    def _process_image(self, file_path: str) -> Document:
        """Process an image file."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        # Extract image metadata and text
        image_data = self.image_processor.process_image(file_path)
        
        if "error" in image_data:
            document.metadata["error"] = image_data["error"]
            return document
            
        # Add image metadata
        document.metadata.update(image_data.get("metadata", {}))
        
        # Add image content
        extracted_text = image_data.get("extracted_text", "")
        
        # Check if image contains a table
        if image_data.get("has_table", False):
            # Create a table content object
            document.add_content(
                DocumentContent(
                    content_type=ContentType.TABLE,
                    content=extracted_text,  # Just use text for now
                    metadata={"from_image": True, **image_data.get("metadata", {})},
                    source_document=doc_id
                )
            )
        
        # Always add the image content
        document.add_content(
            DocumentContent(
                content_type=ContentType.IMAGE,
                content=None,  # Don't store actual image data
                metadata=image_data.get("metadata", {}),
                source_document=doc_id,
                extracted_text=extracted_text
            )
        )
        
        # Check if text contains formulas
        formulas = self.formula_processor.extract_formulas(extracted_text)
        for formula in formulas:
            document.add_content(
                DocumentContent(
                    content_type=ContentType.FORMULA,
                    content=formula,
                    source_document=doc_id
                )
            )
        
        return document
    
    def _process_tabular(self, file_path: str) -> Document:
        """Process a tabular data file (CSV, Excel)."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        try:
            # Read data based on file type
            ext = os.path.splitext(file_path)[1].lower()
            
            if ext == ".csv":
                df = pd.read_csv(file_path)
                document.metadata["format"] = "csv"
            elif ext in [".xls", ".xlsx"]:
                # Read all sheets
                excel_file = pd.ExcelFile(file_path)
                sheets = {}
                
                for sheet_name in excel_file.sheet_names:
                    df_sheet = pd.read_excel(excel_file, sheet_name=sheet_name)
                    
                    # Add as separate content
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TABLE,
                            content=df_sheet,
                            metadata={"sheet_name": sheet_name},
                            source_document=doc_id
                        )
                    )
                    
                    sheets[sheet_name] = df_sheet
                    
                # Use first sheet as main data
                df = sheets[excel_file.sheet_names[0]]
                
                document.metadata["format"] = "excel"
                document.metadata["sheets"] = excel_file.sheet_names
            else:
                raise ValueError(f"Unsupported tabular file type: {ext}")
                
            # Set metadata
            document.metadata["rows"] = len(df)
            document.metadata["columns"] = len(df.columns)
            document.metadata["column_names"] = list(df.columns)
            
            # Extract structured data
            structured_data = self.table_processor.extract_structured_data(df)
            document.metadata["structured_data"] = structured_data
            
            # If not already added (for Excel), add main table content
            if ext == ".csv":
                document.add_content(
                    DocumentContent(
                        content_type=ContentType.TABLE,
                        content=df,
                        source_document=doc_id
                    )
                )
            
            # Add text description of the table
            table_text = self.table_processor.table_to_text(df)
            document.add_content(
                DocumentContent(
                    content_type=ContentType.TEXT,
                    content=table_text,
                    metadata={"description": "Table summary"},
                    source_document=doc_id
                )
            )
            
        except Exception as e:
            logger.error(f"Error processing tabular file {file_path}: {str(e)}")
            document.metadata["error"] = str(e)
            
        return document
    
    def _process_code(self, file_path: str) -> Document:
        """Process a code file."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        try:
            # Read code content
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                code = f.read()
                
            # Detect language and analyze code
            analysis = self.code_processor.analyze_code(code, doc_id)
            
            # Add metadata
            document.metadata["language"] = analysis["language"]
            document.metadata["lines_of_code"] = analysis["lines_of_code"]
            document.metadata["functions"] = len(analysis["functions"])
            document.metadata["classes"] = len(analysis["classes"])
            
            # Add code content
            document.add_content(
                DocumentContent(
                    content_type=ContentType.CODE,
                    content=code,
                    metadata={"language": analysis["language"]},
                    source_document=doc_id
                )
            )
            
            # Add docstrings as text content
            for i, docstring in enumerate(analysis["docstrings"]):
                if len(docstring) > 10:  # Skip very short comments
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=docstring,
                            metadata={"type": "docstring", "index": i},
                            source_document=doc_id
                        )
                    )
                    
            # Add code summary
            summary = self.code_processor.get_code_summary(code, doc_id)
            document.add_content(
                DocumentContent(
                    content_type=ContentType.TEXT,
                    content=summary,
                    metadata={"description": "Code summary"},
                    source_document=doc_id
                )
            )
            
        except Exception as e:
            logger.error(f"Error processing code file {file_path}: {str(e)}")
            document.metadata["error"] = str(e)
            
        return document
    
    def _process_text(self, file_path: str) -> Document:
        """Process a plain text file."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        try:
            # Read text content
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                text = f.read()
                
            # Set metadata
            document.metadata["length"] = len(text)
            document.metadata["lines"] = len(text.split("\n"))
            document.metadata["words"] = len(text.split())
            
            # Detect language
            language = detect_language(text) if 'detect_language' in globals() else "unknown"
            document.metadata["language"] = language
            
            # Split text into sections
            sections = self._split_text_into_sections(text)
            
            for i, section in enumerate(sections):
                # Check if section is a heading
                if i > 0 and len(section.split("\n")) <= 2 and len(section) < 100:
                    # Likely a heading
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.HEADING,
                            content=section,
                            metadata={"level": 2, "index": i},  # Assume level 2 for simplicity
                            source_document=doc_id
                        )
                    )
                else:
                    # Regular text section
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=section,
                            metadata={"index": i},
                            source_document=doc_id
                        )
                    )
                    
            # Check for formulas
            formulas = self.formula_processor.extract_formulas(text)
            for formula in formulas:
                document.add_content(
                    DocumentContent(
                        content_type=ContentType.FORMULA,
                        content=formula,
                        source_document=doc_id
                    )
                )
                
            # Check if it might be code
            if len(text.split("\n")) > 5:
                # Take a sample of the text
                sample = "\n".join(text.split("\n")[:20])
                language = self.code_processor.detect_language(sample)
                
                if language != "text":
                    # It looks like code
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.CODE,
                            content=text,
                            metadata={"language": language, "detected_from_text": True},
                            source_document=doc_id
                        )
                    )
            
        except Exception as e:
            logger.error(f"Error processing text file {file_path}: {str(e)}")
            document.metadata["error"] = str(e)
            
        return document
    
    def _split_text_into_sections(self, text: str) -> List[str]:
        """Split text into logical sections based on line breaks and headings."""
        lines = text.split("\n")
        sections = []
        current_section = []
        
        for line in lines:
            line = line.rstrip()
            
            # Check if this is a section break
            if not line and current_section:
                # Empty line after content indicates section break
                sections.append("\n".join(current_section))
                current_section = []
            elif re.match(r'^[A-Z][\w\s]{0,40}$', line) and not current_section:
                # Likely a heading (capitalizes, not too long, at start of section)
                if sections:  # Don't add the first heading as its own section
                    sections.append(line)
                else:
                    current_section.append(line)
            else:
                current_section.append(line)
        
        # Add the last section
        if current_section:
            sections.append("\n".join(current_section))
            
        return sections
    
    def _process_structured_data(self, file_path: str) -> Document:
        """Process a structured data file (JSON, XML, YAML)."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        try:
            # Read content
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                text = f.read()
                
            # Determine format
            ext = os.path.splitext(file_path)[1].lower()
            
            if ext == ".json":
                # Parse JSON
                data = json.loads(text)
                document.metadata["format"] = "json"
                
                # For large JSON, just add a summary
                if len(text) > 10000:
                    if isinstance(data, list):
                        summary = f"JSON array with {len(data)} items"
                        if data and isinstance(data[0], dict):
                            summary += f", keys: {', '.join(list(data[0].keys())[:5])}"
                    elif isinstance(data, dict):
                        summary += f"JSON object with keys: {', '.join(list(data.keys())[:10])}"
                    
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=summary,
                            metadata={"description": "JSON summary"},
                            source_document=doc_id
                        )
                    )
                
            elif ext in [".xml"]:
                # For XML, we'll just treat it as text
                document.metadata["format"] = "xml"
                
            elif ext in [".yaml", ".yml"]:
                # For YAML, treat as text
                document.metadata["format"] = "yaml"
                
            # Add the raw content as code
            document.add_content(
                DocumentContent(
                    content_type=ContentType.CODE,
                    content=text,
                    metadata={"language": ext[1:]},  # Use extension without dot as language
                    source_document=doc_id
                )
            )
            
            # Try to convert to table if possible
            try:
                if ext == ".json" and isinstance(data, list) and all(isinstance(item, dict) for item in data):
                    df = pd.DataFrame(data)
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TABLE,
                            content=df,
                            metadata={"converted_from": "json"},
                            source_document=doc_id
                        )
                    )
            except Exception as e:
                logger.warning(f"Error converting to table: {str(e)}")
                
        except Exception as e:
            logger.error(f"Error processing structured data file {file_path}: {str(e)}")
            document.metadata["error"] = str(e)
            
        return document
    
    def _process_unknown(self, file_path: str) -> Document:
        """Process a file of unknown type."""
        doc_id = os.path.basename(file_path)
        document = Document(document_id=doc_id, title=doc_id, source_path=file_path)
        
        try:
            # Try to read as text
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                try:
                    text = f.read()
                    
                    # Add as text content
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=text,
                            source_document=doc_id
                        )
                    )
                    
                    document.metadata["read_as"] = "text"
                    
                except UnicodeDecodeError:
                    # Not a text file, treat as binary
                    document.metadata["read_as"] = "binary"
                    document.metadata["error"] = "Cannot process binary file"
            
        except Exception as e:
            logger.error(f"Error processing unknown file {file_path}: {str(e)}")
            document.metadata["error"] = str(e)
            
        return document














Advanced Semantic Chunking Engine


"""
Advanced semantic chunking engine that breaks documents into meaningful chunks
while preserving context and semantic boundaries.
"""
import re
import logging
import nltk
from typing import List, Dict, Any, Optional, Tuple, Union
import hashlib
from dataclasses import dataclass
import numpy as np
from textwrap import dedent

try:
    # Try to download necessary NLTK data if not already present
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

logger = logging.getLogger(__name__)

@dataclass
class Chunk:
    """Class representing a document chunk with metadata."""
    text: str
    source_document: str
    chunk_id: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    start_char_idx: Optional[int] = None
    end_char_idx: Optional[int] = None
    prev_chunk_id: Optional[str] = None
    next_chunk_id: Optional[str] = None
    heading_hierarchy: Optional[List[str]] = None

    def __post_init__(self):
        """Generate chunk_id if not provided."""
        if not self.chunk_id:
            # Generate a hash-based ID if not provided
            hasher = hashlib.md5()
            hasher.update(f"{self.source_document}_{self.start_char_idx}_{self.end_char_idx}".encode())
            hasher.update(self.text[:100].encode())  # Use first 100 chars to make it content-dependent
            self.chunk_id = hasher.hexdigest()

    def get_text_with_context(self) -> str:
        """Get text with heading hierarchy as context."""
        if not self.heading_hierarchy:
            return self.text
            
        # Format headings as breadcrumbs
        headings_context = " > ".join(self.heading_hierarchy)
        return f"{headings_context}\n\n{self.text}"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert chunk to dictionary."""
        return {
            "text": self.text,
            "source_document": self.source_document,
            "chunk_id": self.chunk_id,
            "metadata": self.metadata,
            "start_char_idx": self.start_char_idx,
            "end_char_idx": self.end_char_idx,
            "prev_chunk_id": self.prev_chunk_id,
            "next_chunk_id": self.next_chunk_id,
            "heading_hierarchy": self.heading_hierarchy
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Chunk':
        """Create chunk from dictionary."""
        return cls(
            text=data["text"],
            source_document=data["source_document"],
            chunk_id=data["chunk_id"],
            metadata=data["metadata"],
            start_char_idx=data.get("start_char_idx"),
            end_char_idx=data.get("end_char_idx"),
            prev_chunk_id=data.get("prev_chunk_id"),
            next_chunk_id=data.get("next_chunk_id"),
            heading_hierarchy=data.get("heading_hierarchy")
        )


class ChunkingStrategy:
    """Base class for chunking strategies."""
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """
        Chunk a document into smaller pieces.
        
        Args:
            text: Document text
            source_document: Source document identifier
            metadata: Document metadata
            
        Returns:
            List of Chunk objects
        """
        raise NotImplementedError("Subclasses must implement chunk_document")


class SimpleChunker(ChunkingStrategy):
    """Chunker that splits text into equal-sized chunks with overlap."""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        Initialize the chunker.
        
        Args:
            chunk_size: Target chunk size in characters
            chunk_overlap: Overlap between chunks in characters
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into fixed size chunks with overlap."""
        chunks = []
        
        start_idx = 0
        text_len = len(text)
        chunk_id_counter = 0
        
        while start_idx < text_len:
            # Determine end index
            end_idx = min(start_idx + self.chunk_size, text_len)
            
            # Adjust end index to avoid breaking in the middle of a word
            if end_idx < text_len:
                # Look for a space to break at
                while end_idx > start_idx and text[end_idx] != ' ':
                    end_idx -= 1
                    
                if end_idx == start_idx:
                    # No suitable breakpoint found, revert to original end
                    end_idx = min(start_idx + self.chunk_size, text_len)
            
            # Create chunk
            chunk_text = text[start_idx:end_idx]
            chunk_id = f"{source_document}_{chunk_id_counter}"
            
            chunk = Chunk(
                text=chunk_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=start_idx,
                end_char_idx=end_idx
            )
            
            # Set up links between chunks
            if chunks:
                chunks[-1].next_chunk_id = chunk_id
                chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(chunk)
            
            # Move to next chunk position, accounting for overlap
            start_idx = end_idx - self.chunk_overlap if end_idx < text_len else text_len
            chunk_id_counter += 1
        
        return chunks


class SentenceChunker(ChunkingStrategy):
    """Chunker that respects sentence boundaries."""
    
    def __init__(self, max_chunk_size: int = 1000, min_chunk_size: int = 100):
        """
        Initialize the chunker.
        
        Args:
            max_chunk_size: Maximum chunk size in characters
            min_chunk_size: Minimum chunk size in characters
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into chunks respecting sentence boundaries."""
        chunks = []
        
        # Split into sentences
        sentences = nltk.sent_tokenize(text)
        
        current_chunk_text = []
        current_chunk_length = 0
        start_idx = 0
        chunk_id_counter = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # If adding this sentence would exceed max size and we have enough for a chunk
            if (current_chunk_length + sentence_length > self.max_chunk_size and 
                current_chunk_length >= self.min_chunk_size):
                # Create new chunk
                chunk_text = " ".join(current_chunk_text)
                
                chunk_id = f"{source_document}_{chunk_id_counter}"
                
                end_idx = start_idx + len(chunk_text)
                
                chunk = Chunk(
                    text=chunk_text,
                    source_document=source_document,
                    chunk_id=chunk_id,
                    metadata=metadata.copy(),
                    start_char_idx=start_idx,
                    end_char_idx=end_idx
                )
                
                # Set up links between chunks
                if chunks:
                    chunks[-1].next_chunk_id = chunk_id
                    chunk.prev_chunk_id = chunks[-1].chunk_id
                    
                chunks.append(chunk)
                
                # Start new chunk
                current_chunk_text = [sentence]
                current_chunk_length = sentence_length
                start_idx = end_idx
                chunk_id_counter += 1
            else:
                # Add to current chunk
                current_chunk_text.append(sentence)
                current_chunk_length += sentence_length
        
        # Add the last chunk if it's not empty
        if current_chunk_text:
            chunk_text = " ".join(current_chunk_text)
            
            chunk_id = f"{source_document}_{chunk_id_counter}"
            
            end_idx = start_idx + len(chunk_text)
            
            chunk = Chunk(
                text=chunk_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=start_idx,
                end_char_idx=end_idx
            )
            
            # Set up links between chunks
            if chunks:
                chunks[-1].next_chunk_id = chunk_id
                chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(chunk)
        
        return chunks


class ParagraphChunker(ChunkingStrategy):
    """Chunker that respects paragraph boundaries."""
    
    def __init__(self, max_chunk_size: int = 1500, min_chunk_size: int = 100):
        """
        Initialize the chunker.
        
        Args:
            max_chunk_size: Maximum chunk size in characters
            min_chunk_size: Minimum chunk size in characters
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.sentence_chunker = SentenceChunker(max_chunk_size, min_chunk_size)
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into chunks respecting paragraph boundaries."""
        chunks = []
        
        # Split into paragraphs (consecutive newlines)
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk_paragraphs = []
        current_chunk_length = 0
        start_idx = 0
        chunk_id_counter = 0
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue  # Skip empty paragraphs
                
            paragraph_length = len(paragraph)
            
            # If this paragraph alone is larger than max size, use sentence chunker
            if paragraph_length > self.max_chunk_size:
                # First, add any accumulated paragraphs as a chunk
                if current_chunk_paragraphs:
                    chunk_text = "\n\n".join(current_chunk_paragraphs)
                    
                    chunk_id = f"{source_document}_{chunk_id_counter}"
                    
                    end_idx = start_idx + len(chunk_text)
                    
                    chunk = Chunk(
                        text=chunk_text,
                        source_document=source_document,
                        chunk_id=chunk_id,
                        metadata=metadata.copy(),
                        start_char_idx=start_idx,
                        end_char_idx=end_idx
                    )
                    
                    # Set up links between chunks
                    if chunks:
                        chunks[-1].next_chunk_id = chunk_id
                        chunk.prev_chunk_id = chunks[-1].chunk_id
                        
                    chunks.append(chunk)
                    
                    chunk_id_counter += 1
                    start_idx = end_idx
                    current_chunk_paragraphs = []
                    current_chunk_length = 0
                
                # Now chunk the large paragraph by sentences
                paragraph_offset = text.find(paragraph, start_idx)
                para_metadata = metadata.copy()
                para_metadata["is_large_paragraph"] = True
                
                sentence_chunks = self.sentence_chunker.chunk_document(
                    paragraph, 
                    source_document, 
                    para_metadata
                )
                
                # Update chunk IDs to continue the sequence
                for i, sentence_chunk in enumerate(sentence_chunks):
                    original_id = sentence_chunk.chunk_id
                    new_id = f"{source_document}_{chunk_id_counter + i}"
                    
                    # Update next/prev references
                    if sentence_chunk.next_chunk_id:
                        next_idx = -1
                        for j, sc in enumerate(sentence_chunks):
                            if sc.chunk_id == sentence_chunk.next_chunk_id:
                                next_idx = j
                                break
                        if next_idx >= 0:
                            sentence_chunk.next_chunk_id = f"{source_document}_{chunk_id_counter + next_idx}"
                    
                    if sentence_chunk.prev_chunk_id:
                        prev_idx = -1
                        for j, sc in enumerate(sentence_chunks):
                            if sc.chunk_id == sentence_chunk.prev_chunk_id:
                                prev_idx = j
                                break
                        if prev_idx >= 0:
                            sentence_chunk.prev_chunk_id = f"{source_document}_{chunk_id_counter + prev_idx}"
                    
                    # Update chunk ID
                    sentence_chunk.chunk_id = new_id
                    
                    # Update character indices to be relative to the whole document
                    if sentence_chunk.start_char_idx is not None:
                        sentence_chunk.start_char_idx += paragraph_offset
                    if sentence_chunk.end_char_idx is not None:
                        sentence_chunk.end_char_idx += paragraph_offset
                
                # Link the last chunk before to the first sentence chunk
                if chunks and sentence_chunks:
                    chunks[-1].next_chunk_id = sentence_chunks[0].chunk_id
                    sentence_chunks[0].prev_chunk_id = chunks[-1].chunk_id
                
                # Add sentence chunks to our chunks list
                chunks.extend(sentence_chunks)
                
                # Update counters
                chunk_id_counter += len(sentence_chunks)
                if sentence_chunks:
                    start_idx = sentence_chunks[-1].end_char_idx
                
            # If adding this paragraph would exceed max size and we have enough for a chunk
            elif (current_chunk_length + paragraph_length > self.max_chunk_size and 
                  current_chunk_length >= self.min_chunk_size):
                # Create new chunk
                chunk_text = "\n\n".join(current_chunk_paragraphs)
                
                chunk_id = f"{source_document}_{chunk_id_counter}"
                
                end_idx = start_idx + len(chunk_text)
                
                chunk = Chunk(
                    text=chunk_text,
                    source_document=source_document,
                    chunk_id=chunk_id,
                    metadata=metadata.copy(),
                    start_char_idx=start_idx,
                    end_char_idx=end_idx
                )
                
                # Set up links between chunks
                if chunks:
                    chunks[-1].next_chunk_id = chunk_id
                    chunk.prev_chunk_id = chunks[-1].chunk_id
                    
                chunks.append(chunk)
                
                # Start new chunk
                current_chunk_paragraphs = [paragraph]
                current_chunk_length = paragraph_length
                start_idx = end_idx
                chunk_id_counter += 1
            else:
                # Add to current chunk
                current_chunk_paragraphs.append(paragraph)
                current_chunk_length += paragraph_length + 2  # +2 for the newlines
        
        # Add the last chunk if it's not empty
        if current_chunk_paragraphs:
            chunk_text = "\n\n".join(current_chunk_paragraphs)
            
            chunk_id = f"{source_document}_{chunk_id_counter}"
            
            end_idx = start_idx + len(chunk_text)
            
            chunk = Chunk(
                text=chunk_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=start_idx,
                end_char_idx=end_idx
            )
            
            # Set up links between chunks
            if chunks:
                chunks[-1].next_chunk_id = chunk_id
                chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(chunk)
        
        return chunks


class SemanticChunker(ChunkingStrategy):
    """
    Advanced chunker that splits text based on semantic boundaries 
    (headings, sections, etc.) and preserves hierarchical structure.
    """
    
    def __init__(self, max_chunk_size: int = 1500, min_chunk_size: int = 100):
        """
        Initialize the chunker.
        
        Args:
            max_chunk_size: Maximum chunk size in characters
            min_chunk_size: Minimum chunk size in characters
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.paragraph_chunker = ParagraphChunker(max_chunk_size, min_chunk_size)
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into chunks based on semantic structure."""
        chunks = []
        
        # First pass: identify structure (headings, sections)
        structure = self._analyze_document_structure(text)
        
        # Second pass: create chunks based on structure
        if structure["headings"]:
            chunks = self._chunk_by_headings(text, source_document, metadata, structure)
        else:
            # Fall back to paragraph chunker if no headings found
            chunks = self.paragraph_chunker.chunk_document(text, source_document, metadata)
        
        return chunks
    
    def _analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """
        Analyze document structure to identify headings, sections, etc.
        
        Args:
            text: Document text
            
        Returns:
            Dictionary with structure information
        """
        structure = {
            "headings": [],  # List of (heading_text, level, start_pos, end_pos)
            "sections": []   # List of (start_pos, end_pos, heading_idx)
        }
        
        # Look for common heading patterns
        # 1. Markdown-style: ## Heading
        md_headings = re.finditer(r'^(#{1,6})\s+(.+)$', text, re.MULTILINE)
        for match in md_headings:
            level = len(match.group(1))
            heading_text = match.group(2).strip()
            start_pos = match.start()
            end_pos = match.end()
            structure["headings"].append({
                "text": heading_text,
                "level": level,
                "start_pos": start_pos,
                "end_pos": end_pos
            })
        
        # 2. Underlined headings: Heading\n=======
        underline_headings = re.finditer(r'^([^\n]+)\n([=\-]+)$', text, re.MULTILINE)
        for match in underline_headings:
            heading_text = match.group(1).strip()
            level = 1 if match.group(2)[0] == '=' else 2  # = for h1, - for h2
            start_pos = match.start()
            end_pos = match.end()
            structure["headings"].append({
                "text": heading_text,
                "level": level,
                "start_pos": start_pos,
                "end_pos": end_pos
            })
        
        # 3. Numbered headings: 1. Heading, 1.1 Heading, etc.
        numbered_headings = re.finditer(r'^(\d+(\.\d+)*)\s+(.+)$', text, re.MULTILINE)
        for match in numbered_headings:
            heading_text = match.group(3).strip()
            level = len(match.group(1).split('.'))
            start_pos = match.start()
            end_pos = match.end()
            structure["headings"].append({
                "text": heading_text,
                "level": level,
                "start_pos": start_pos,
                "end_pos": end_pos
            })
        
        # Sort headings by position
        structure["headings"].sort(key=lambda h: h["start_pos"])
        
        # Identify sections
        if structure["headings"]:
            for i, heading in enumerate(structure["headings"]):
                section_start = heading["end_pos"]
                
                # Section ends at the next heading or end of text
                if i < len(structure["headings"]) - 1:
                    section_end = structure["headings"][i + 1]["start_pos"]
                else:
                    section_end = len(text)
                    
                structure["sections"].append({
                    "start_pos": section_start,
                    "end_pos": section_end,
                    "heading_idx": i
                })
        
        return structure
    
    def _chunk_by_headings(self, text: str, source_document: str, metadata: Dict[str, Any], 
                          structure: Dict[str, Any]) -> List[Chunk]:
        """
        Create chunks based on document headings and sections.
        
        Args:
            text: Document text
            source_document: Source document identifier
            metadata: Document metadata
            structure: Document structure from _analyze_document_structure
            
        Returns:
            List of Chunk objects
        """
        chunks = []
        chunk_id_counter = 0
        
        # Create heading hierarchy list to track context
        heading_stack = []
        current_hierarchy = []
        
        # For each heading and its section
        for i, (heading, section) in enumerate(zip(structure["headings"], structure["sections"])):
            # Update heading hierarchy
            heading_level = heading["level"]
            
            # Pop headings with greater or equal level (replacing them)
            while heading_stack and heading_stack[-1][0] >= heading_level:
                heading_stack.pop()
                if current_hierarchy:
                    current_hierarchy.pop()
            
            # Add current heading to the stack
            heading_stack.append((heading_level, heading["text"]))
            current_hierarchy.append(heading["text"])
            
            # Create a chunk for the heading itself
            heading_text = heading["text"]
            
            chunk_id = f"{source_document}_{chunk_id_counter}"
            chunk_id_counter += 1
            
            heading_chunk = Chunk(
                text=heading_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata={**metadata.copy(), "is_heading": True, "heading_level": heading_level},
                start_char_idx=heading["start_pos"],
                end_char_idx=heading["end_pos"],
                heading_hierarchy=current_hierarchy.copy()
            )
            
            # Link with previous chunk if exists
            if chunks:
                chunks[-1].next_chunk_id = heading_chunk.chunk_id
                heading_chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(heading_chunk)
            
            # Extract section content
            section_text = text[section["start_pos"]:section["end_pos"]].strip()
            
            # If section is empty, continue to next section
            if not section_text:
                continue
                
            # If section is short enough, keep it as one chunk
            if len(section_text) <= self.max_chunk_size:
                section_chunk_id = f"{source_document}_{chunk_id_counter}"
                chunk_id_counter += 1
                
                section_chunk = Chunk(
                    text=section_text,
                    source_document=source_document,
                    chunk_id=section_chunk_id,
                    metadata=metadata.copy(),
                    start_char_idx=section["start_pos"],
                    end_char_idx=section["end_pos"],
                    heading_hierarchy=current_hierarchy.copy()
                )
                
                # Link with previous chunk (the heading)
                chunks[-1].next_chunk_id = section_chunk.chunk_id
                section_chunk.prev_chunk_id = chunks[-1].chunk_id
                
                chunks.append(section_chunk)
            else:
                # Section is too large, recursively chunk it
                section_metadata = metadata.copy()
                section_metadata["parent_heading"] = heading_text
                section_metadata["heading_level"] = heading_level
                
                # Use paragraph chunker for the section content
                section_chunks = self.paragraph_chunker.chunk_document(
                    section_text, 
                    source_document, 
                    section_metadata
                )
                
                # Update chunk IDs to continue the sequence
                for j, section_chunk in enumerate(section_chunks):
                    original_id = section_chunk.chunk_id
                    new_id = f"{source_document}_{chunk_id_counter + j}"
                    
                    # Update next/prev references
                    if section_chunk.next_chunk_id:
                        next_idx = -1
                        for k, sc in enumerate(section_chunks):
                            if sc.chunk_id == section_chunk.next_chunk_id:
                                next_idx = k
                                break
                        if next_idx >= 0:
                            section_chunk.next_chunk_id = f"{source_document}_{chunk_id_counter + next_idx}"
                    
                    if section_chunk.prev_chunk_id:
                        prev_idx = -1
                        for k, sc in enumerate(section_chunks):
                            if sc.chunk_id == section_chunk.prev_chunk_id:
                                prev_idx = k
                                break
                        if prev_idx >= 0:
                            section_chunk.prev_chunk_id = f"{source_document}_{chunk_id_counter + prev_idx}"
                    
                    # Update chunk ID
                    section_chunk.chunk_id = new_id
                    
                    # Update character indices to be relative to the whole document
                    if section_chunk.start_char_idx is not None:
                        section_chunk.start_char_idx += section["start_pos"]
                    if section_chunk.end_char_idx is not None:
                        section_chunk.end_char_idx += section["start_pos"]
                    
                    # Add heading hierarchy
                    section_chunk.heading_hierarchy = current_hierarchy.copy()
                
                # Link the heading chunk to the first section chunk
                if section_chunks:
                    chunks[-1].next_chunk_id = section_chunks[0].chunk_id
                    section_chunks[0].prev_chunk_id = chunks[-1].chunk_id
                
                # Add section chunks to the main chunks list
                chunks.extend(section_chunks)
                
                # Update counter
                chunk_id_counter += len(section_chunks)
        
        return chunks


class RecursiveChunker(ChunkingStrategy):
    """
    Chunker that recursively splits large chunks until they meet size requirements.
    Tries to split on natural boundaries (paragraphs, sentences) in that order.
    """
    
    def __init__(self, max_chunk_size: int = 1500, min_chunk_size: int = 100, 
                 chunk_overlap: int = 50):
        """
        Initialize the chunker.
        
        Args:
            max_chunk_size: Maximum chunk size in characters
            min_chunk_size: Minimum chunk size in characters
            chunk_overlap: Overlap between chunks in characters
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.chunk_overlap = chunk_overlap
    
    def chunk_document(self, text: str, source_document: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Recursively split text into appropriate chunks."""
        return self._chunk_text_recursive(text, source_document, metadata)
    
    def _chunk_text_recursive(self, text: str, source_document: str, metadata: Dict[str, Any], 
                             chunk_id_prefix: str = "") -> List[Chunk]:
        """
        Recursively split text into appropriately sized chunks.
        
        Args:
            text: Text to split
            source_document: Source document identifier
            metadata: Document metadata
            chunk_id_prefix: Prefix for chunk IDs to maintain hierarchy
            
        Returns:
            List of Chunk objects
        """
        chunks = []
        
        # Base case: text is small enough
        if len(text) <= self.max_chunk_size:
            # Create a single chunk
            chunk_id = f"{source_document}{chunk_id_prefix}_0"
            
            chunk = Chunk(
                text=text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=0,  # Relative to this text
                end_char_idx=len(text)
            )
            
            return [chunk]
        
        # Try to split on paragraph boundaries first
        paragraphs = re.split(r'\n\s*\n', text)
        
        # If we have multiple paragraphs, process each one
        if len(paragraphs) > 1:
            current_chunk_paragraphs = []
            current_chunk_length = 0
            chunk_id_counter = 0
            
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if not paragraph:
                    continue  # Skip empty paragraphs
                    
                paragraph_length = len(paragraph)
                
                # If this paragraph alone exceeds max size, recursively chunk it
                if paragraph_length > self.max_chunk_size:
                    # First, add any accumulated paragraphs as a chunk
                    if current_chunk_paragraphs:
                        chunk_text = "\n\n".join(current_chunk_paragraphs)
                        chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
                        chunk_id_counter += 1
                        
                        chunk = Chunk(
                            text=chunk_text,
                            source_document=source_document,
                            chunk_id=chunk_id,
                            metadata=metadata.copy(),
                            start_char_idx=text.find(current_chunk_paragraphs[0]),
                            end_char_idx=text.find(current_chunk_paragraphs[-1]) + len(current_chunk_paragraphs[-1])
                        )
                        
                        # Link with previous chunk if exists
                        if chunks:
                            chunks[-1].next_chunk_id = chunk.chunk_id
                            chunk.prev_chunk_id = chunks[-1].chunk_id
                            
                        chunks.append(chunk)
                        current_chunk_paragraphs = []
                        current_chunk_length = 0
                    
                    # Recursively chunk the paragraph
                    para_chunks = self._chunk_text_by_sentences(
                        paragraph,
                        source_document,
                        metadata.copy(),
                        f"{chunk_id_prefix}_{chunk_id_counter}"
                    )
                    chunk_id_counter += len(para_chunks)
                    
                    # Link with existing chunks
                    if chunks and para_chunks:
                        chunks[-1].next_chunk_id = para_chunks[0].chunk_id
                        para_chunks[0].prev_chunk_id = chunks[-1].chunk_id
                        
                    chunks.extend(para_chunks)
                    
                # If adding this paragraph would make the chunk too large
                elif current_chunk_length + paragraph_length > self.max_chunk_size and current_chunk_paragraphs:
                    # Create a chunk with current accumulated paragraphs
                    chunk_text = "\n\n".join(current_chunk_paragraphs)
                    chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
                    chunk_id_counter += 1
                    
                    chunk = Chunk(
                        text=chunk_text,
                        source_document=source_document,
                        chunk_id=chunk_id,
                        metadata=metadata.copy(),
                        start_char_idx=text.find(current_chunk_paragraphs[0]),
                        end_char_idx=text.find(current_chunk_paragraphs[-1]) + len(current_chunk_paragraphs[-1])
                    )
                    
                    # Link with previous chunk if exists
                    if chunks:
                        chunks[-1].next_chunk_id = chunk.chunk_id
                        chunk.prev_chunk_id = chunks[-1].chunk_id
                        
                    chunks.append(chunk)
                    
                    # Start a new chunk with this paragraph
                    current_chunk_paragraphs = [paragraph]
                    current_chunk_length = paragraph_length
                else:
                    # Add to current chunk
                    current_chunk_paragraphs.append(paragraph)
                    current_chunk_length += paragraph_length + 4  # +4 for the paragraph separators
            
            # Add the last chunk if there are remaining paragraphs
            if current_chunk_paragraphs:
                chunk_text = "\n\n".join(current_chunk_paragraphs)
                chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
                
                chunk = Chunk(
                    text=chunk_text,
                    source_document=source_document,
                    chunk_id=chunk_id,
                    metadata=metadata.copy(),
                    start_char_idx=text.find(current_chunk_paragraphs[0]),
                    end_char_idx=text.find(current_chunk_paragraphs[-1]) + len(current_chunk_paragraphs[-1])
                )
                
                # Link with previous chunk if exists
                if chunks:
                    chunks[-1].next_chunk_id = chunk.chunk_id
                    chunk.prev_chunk_id = chunks[-1].chunk_id
                    
                chunks.append(chunk)
                
            return chunks
            
        # If we can't split on paragraphs, try sentences
        if len(text) > self.max_chunk_size:
            return self._chunk_text_by_sentences(text, source_document, metadata, chunk_id_prefix)
        
        # Fallback: just split by characters with overlap
        return self._chunk_text_by_chars(text, source_document, metadata, chunk_id_prefix)
    
    def _chunk_text_by_sentences(self, text: str, source_document: str, 
                                metadata: Dict[str, Any], chunk_id_prefix: str) -> List[Chunk]:
        """Split text by sentences when paragraphs are too large."""
        chunks = []
        
        # Split into sentences
        sentences = nltk.sent_tokenize(text)
        
        current_chunk_sentences = []
        current_chunk_length = 0
        chunk_id_counter = 0
        
        for i, sentence in enumerate(sentences):
            sentence_length = len(sentence)
            
            # If this sentence alone exceeds max size, we need character splitting
            if sentence_length > self.max_chunk_size:
                # First, add any accumulated sentences as a chunk
                if current_chunk_sentences:
                    chunk_text = " ".join(current_chunk_sentences)
                    chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
                    chunk_id_counter += 1
                    
                    chunk = Chunk(
                        text=chunk_text,
                        source_document=source_document,
                        chunk_id=chunk_id,
                        metadata=metadata.copy(),
                        start_char_idx=text.find(current_chunk_sentences[0]),
                        end_char_idx=text.find(current_chunk_sentences[-1]) + len(current_chunk_sentences[-1])
                    )
                    
                    # Link with previous chunk if exists
                    if chunks:
                        chunks[-1].next_chunk_id = chunk.chunk_id
                        chunk.prev_chunk_id = chunks[-1].chunk_id
                        
                    chunks.append(chunk)
                    current_chunk_sentences = []
                    current_chunk_length = 0
                
                # Split this sentence by characters
                sentence_chunks = self._chunk_text_by_chars(
                    sentence,
                    source_document,
                    metadata.copy(),
                    f"{chunk_id_prefix}_{chunk_id_counter}"
                )
                chunk_id_counter += len(sentence_chunks)
                
                # Link with existing chunks
                if chunks and sentence_chunks:
                    chunks[-1].next_chunk_id = sentence_chunks[0].chunk_id
                    sentence_chunks[0].prev_chunk_id = chunks[-1].chunk_id
                    
                chunks.extend(sentence_chunks)
                
            # If adding this sentence would make the chunk too large
            elif current_chunk_length + sentence_length > self.max_chunk_size and current_chunk_sentences:
                # Create a chunk with current accumulated sentences
                chunk_text = " ".join(current_chunk_sentences)
                chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
                chunk_id_counter += 1
                
                chunk = Chunk(
                    text=chunk_text,
                    source_document=source_document,
                    chunk_id=chunk_id,
                    metadata=metadata.copy(),
                    start_char_idx=text.find(current_chunk_sentences[0]),
                    end_char_idx=text.find(current_chunk_sentences[-1]) + len(current_chunk_sentences[-1])
                )
                
                # Link with previous chunk if exists
                if chunks:
                    chunks[-1].next_chunk_id = chunk.chunk_id
                    chunk.prev_chunk_id = chunks[-1].chunk_id
                    
                chunks.append(chunk)
                
                # Start a new chunk with this sentence
                current_chunk_sentences = [sentence]
                current_chunk_length = sentence_length
            else:
                # Add to current chunk
                current_chunk_sentences.append(sentence)
                current_chunk_length += sentence_length + 1  # +1 for the space
        
        # Add the last chunk if there are remaining sentences
        if current_chunk_sentences:
            chunk_text = " ".join(current_chunk_sentences)
            chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
            
            chunk = Chunk(
                text=chunk_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=text.find(current_chunk_sentences[0]),
                end_char_idx=text.find(current_chunk_sentences[-1]) + len(current_chunk_sentences[-1])
            )
            
            # Link with previous chunk if exists
            if chunks:
                chunks[-1].next_chunk_id = chunk.chunk_id
                chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(chunk)
            
        return chunks
    
    def _chunk_text_by_chars(self, text: str, source_document: str, 
                            metadata: Dict[str, Any], chunk_id_prefix: str) -> List[Chunk]:
        """Split text by characters when sentences are too large."""
        chunks = []
        
        # Split into overlapping character chunks
        text_len = len(text)
        chunk_id_counter = 0
        
        start_idx = 0
        while start_idx < text_len:
            # Calculate end index
            end_idx = min(start_idx + self.max_chunk_size, text_len)
            
            # If we're not at the end of the text, try to find a good break point
            if end_idx < text_len:
                # Try to end at a sentence boundary
                sentence_end = text.rfind('. ', start_idx, end_idx)
                if sentence_end > start_idx + self.min_chunk_size:
                    end_idx = sentence_end + 2  # Include the period and space
                else:
                    # Try to end at a space
                    space = text.rfind(' ', start_idx + self.min_chunk_size, end_idx)
                    if space != -1:
                        end_idx = space + 1  # Include the space
            
            # Create chunk
            chunk_text = text[start_idx:end_idx]
            chunk_id = f"{source_document}{chunk_id_prefix}_{chunk_id_counter}"
            chunk_id_counter += 1
            
            chunk = Chunk(
                text=chunk_text,
                source_document=source_document,
                chunk_id=chunk_id,
                metadata=metadata.copy(),
                start_char_idx=start_idx,
                end_char_idx=end_idx
            )
            
            # Link with previous chunk if exists
            if chunks:
                chunks[-1].next_chunk_id = chunk.chunk_id
                chunk.prev_chunk_id = chunks[-1].chunk_id
                
            chunks.append(chunk)
            
            # Move to next chunk with overlap
            if end_idx < text_len:
                # Find a good starting point for the next chunk
                # Try to start at a sentence boundary in the overlap
                overlap_start = max(start_idx, end_idx - self.chunk_overlap)
                sentence_start = text.find('. ', overlap_start, end_idx)
                if sentence_start != -1:
                    start_idx = sentence_start + 2  # Start after the period and space
                else:
                    # Try to start at a space
                    space = text.find(' ', overlap_start, end_idx)
                    if space != -1:
                        start_idx = space + 1  # Start after the space
                    else:
                        # Just use the overlap
                        start_idx = end_idx - self.chunk_overlap
            else:
                # We've reached the end of the text
                break
        
        return chunks


class ChunkingEngine:
    """Main engine for document chunking with strategy selection."""
    
    def __init__(self, max_chunk_size: int = 1500, min_chunk_size: int = 100,
                 chunk_overlap: int = 50, default_strategy: str = "semantic"):
        """
        Initialize the chunking engine.
        
        Args:
            max_chunk_size: Maximum chunk size in characters
            min_chunk_size: Minimum chunk size in characters
            chunk_overlap: Overlap between chunks in characters
            default_strategy: Default chunking strategy
        """
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.chunk_overlap = chunk_overlap
        self.default_strategy = default_strategy
        
        # Initialize strategies
        self.strategies = {
            "simple": SimpleChunker(max_chunk_size, chunk_overlap),
            "sentence": SentenceChunker(max_chunk_size, min_chunk_size),
            "paragraph": ParagraphChunker(max_chunk_size, min_chunk_size),
            "semantic": SemanticChunker(max_chunk_size, min_chunk_size),
            "recursive": RecursiveChunker(max_chunk_size, min_chunk_size, chunk_overlap)
        }
    
    def chunk_document(self, text: str, source_document: str, metadata: Optional[Dict[str, Any]] = None,
                      strategy: Optional[str] = None) -> List[Chunk]:
        """
        Chunk a document into smaller pieces.
        
        Args:
            text: Document text
            source_document: Source document identifier
            metadata: Document metadata
            strategy: Chunking strategy to use
            
        Returns:
            List of Chunk objects
        """
        if not text:
            return []
            
        if metadata is None:
            metadata = {}
            
        # Determine strategy
        if not strategy:
            strategy = self.default_strategy
            
        # Use auto-selection if strategy is "auto"
        if strategy == "auto":
            strategy = self._select_chunking_strategy(text)
            
        # Get the appropriate chunker
        if strategy in self.strategies:
            chunker = self.strategies[strategy]
        else:
            logger.warning(f"Unknown chunking strategy: {strategy}, using default")
            chunker = self.strategies[self.default_strategy]
            
        # Add chunking metadata
        metadata["chunking_strategy"] = strategy
        
        # Chunk the document
        chunks = chunker.chunk_document(text, source_document, metadata)
        
        # Post-process chunks (e.g., add additional metadata)
        self._post_process_chunks(chunks)
        
        return chunks
    
    def _select_chunking_strategy(self, text: str) -> str:
        """
        Automatically select the best chunking strategy for the text.
        
        Args:
            text: Document text
            
        Returns:
            Selected strategy name
        """
        # Check for headings
        heading_patterns = [
            r'^(#{1,6})\s+(.+)$',                     # Markdown headings
            r'^([^\n]+)\n([=\-]+)$',                  # Underlined headings
            r'^(\d+(\.\d+)*)\s+(.+)$'                 # Numbered headings
        ]
        
        has_headings = False
        for pattern in heading_patterns:
            if re.search(pattern, text, re.MULTILINE):
                has_headings = True
                break
                
        # Check paragraph structure
        paragraphs = re.split(r'\n\s*\n', text)
        avg_paragraph_len = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0
        
        # Select strategy based on document characteristics
        if has_headings:
            return "semantic"  # Use semantic for structured documents
        elif avg_paragraph_len > self.max_chunk_size:
            return "recursive"  # Use recursive for large paragraphs
        elif len(paragraphs) > 5:
            return "paragraph"  # Use paragraph for multi-paragraph documents
        else:
            return "sentence"   # Default to sentence for other cases
    
    def _post_process_chunks(self, chunks: List[Chunk]) -> None:
        """
        Apply post-processing to chunks (in place).
        
        Args:
            chunks: List of Chunk objects
        """
        # Add sequential information
        for i, chunk in enumerate(chunks):
            # Add position info
            chunk.metadata["position"] = i
            chunk.metadata["total_chunks"] = len(chunks)
            
            # Add content length info
            chunk.metadata["char_length"] = len(chunk.text)
            chunk.metadata["word_count"] = len(chunk.text.split())
            
            # Clean up text (remove excessive whitespace, etc.)
            chunk.text = self._clean_chunk_text(chunk.text)
    
    def _clean_chunk_text(self, text: str) -> str:
        """
        Clean chunk text.
        
        Args:
            text: Chunk text
            
        Returns:
            Cleaned text
        """
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove excessive line breaks
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Restore proper spacing after periods
        text = re.sub(r'\.(?=[A-Z])', '. ', text)
        
        return text.strip()
    
    def chunk_multiple_documents(self, documents: Dict[str, Dict[str, Any]]) -> Dict[str, List[Chunk]]:
        """
        Chunk multiple documents at once.
        
        Args:
            documents: Dictionary of document_id -> {text, metadata}
            
        Returns:
            Dictionary of document_id -> list of chunks
        """
        results = {}
        
        for doc_id, doc_info in documents.items():
            text = doc_info.get("text", "")
            metadata = doc_info.get("metadata", {})
            strategy = doc_info.get("strategy", None)
            
            chunks = self.chunk_document(text, doc_id, metadata, strategy)
            results[doc_id] = chunks
            
        return results
    
    def merge_small_chunks(self, chunks: List[Chunk], min_size: Optional[int] = None) -> List[Chunk]:
        """
        Merge small chunks to reduce fragmentation.
        
        Args:
            chunks: List of Chunk objects
            min_size: Minimum acceptable chunk size (defaults to self.min_chunk_size)
            
        Returns:
            List of merged chunks
        """
        if not chunks:
            return []
            
        if min_size is None:
            min_size = self.min_chunk_size
            
        # Create a new list for merged chunks
        merged_chunks = []
        current_chunks = []
        current_size = 0
        
        for chunk in chunks:
            chunk_size = len(chunk.text)
            
            # If this chunk itself is big enough, add it directly
            if chunk_size >= min_size:
                # Flush any accumulated small chunks
                if current_chunks:
                    if current_size >= min_size:
                        # Merge small chunks
                        merged_text = " ".join(c.text for c in current_chunks)
                        start_idx = current_chunks[0].start_char_idx
                        end_idx = current_chunks[-1].end_char_idx
                        
                        merged_chunk = Chunk(
                            text=merged_text,
                            source_document=current_chunks[0].source_document,
                            chunk_id=current_chunks[0].chunk_id,
                            metadata={**current_chunks[0].metadata, "merged": True, 
                                      "merged_count": len(current_chunks)},
                            start_char_idx=start_idx,
                            end_char_idx=end_idx,
                            prev_chunk_id=current_chunks[0].prev_chunk_id,
                            heading_hierarchy=current_chunks[0].heading_hierarchy
                        )
                        
                        merged_chunks.append(merged_chunk)
                    else:
                        # Just add the small chunks as they are
                        merged_chunks.extend(current_chunks)
                    
                    current_chunks = []
                    current_size = 0
                
                # Add this chunk
                merged_chunks.append(chunk)
            else:
                # Add to current small chunks
                current_chunks.append(chunk)
                current_size += chunk_size
                
                # If we now have enough accumulated, merge and add
                if current_size >= min_size:
                    merged_text = " ".join(c.text for c in current_chunks)
                    start_idx = current_chunks[0].start_char_idx
                    end_idx = current_chunks[-1].end_char_idx
                    
                    merged_chunk = Chunk(
                        text=merged_text,
                        source_document=current_chunks[0].source_document,
                        chunk_id=current_chunks[0].chunk_id,
                        metadata={**current_chunks[0].metadata, "merged": True, 
                                  "merged_count": len(current_chunks)},
                        start_char_idx=start_idx,
                        end_char_idx=end_idx,
                        prev_chunk_id=current_chunks[0].prev_chunk_id,
                        heading_hierarchy=current_chunks[0].heading_hierarchy
                    )
                    
                    merged_chunks.append(merged_chunk)
                    current_chunks = []
                    current_size = 0
        
        # Handle any remaining small chunks
        if current_chunks:
            if current_size >= min_size:
                # Merge remaining small chunks
                merged_text = " ".join(c.text for c in current_chunks)
                start_idx = current_chunks[0].start_char_idx
                end_idx = current_chunks[-1].end_char_idx
                
                merged_chunk = Chunk(
                    text=merged_text,
                    source_document=current_chunks[0].source_document,
                    chunk_id=current_chunks[0].chunk_id,
                    metadata={**current_chunks[0].metadata, "merged": True, 
                              "merged_count": len(current_chunks)},
                    start_char_idx=start_idx,
                    end_char_idx=end_idx,
                    prev_chunk_id=current_chunks[0].prev_chunk_id,
                    heading_hierarchy=current_chunks[0].heading_hierarchy
                )
                
                merged_chunks.append(merged_chunk)
            else:
                # Just add the small chunks as they are
                merged_chunks.extend(current_chunks)
        
        # Update next_chunk_id links
        for i in range(len(merged_chunks) - 1):
            merged_chunks[i].next_chunk_id = merged_chunks[i+1].chunk_id
            merged_chunks[i+1].prev_chunk_id = merged_chunks[i].chunk_id
            
        return merged_chunks
    
    def get_context_for_chunk(self, chunk: Chunk, chunks: List[Chunk], 
                              window_size: int = 2) -> List[Chunk]:
        """
        Get surrounding context chunks for a given chunk.
        
        Args:
            chunk: Target chunk
            chunks: List of all chunks
            window_size: Number of chunks to include on each side
            
        Returns:
            List of context chunks
        """
        context = []
        
        # Create a mapping of chunk_id to index in the chunks list
        chunk_id_map = {c.chunk_id: i for i, c in enumerate(chunks)}
        
        if chunk.chunk_id not in chunk_id_map:
            # Chunk not in the list
            return []
            
        idx = chunk_id_map[chunk.chunk_id]
        
        # Get previous chunks
        for i in range(max(0, idx - window_size), idx):
            context.append(chunks[i])
            
        # Get next chunks
        for i in range(idx + 1, min(len(chunks), idx + 1 + window_size)):
            context.append(chunks[i])
            
        return context
    
    def get_section_chunks(self, chunk: Chunk, chunks: List[Chunk]) -> List[Chunk]:
        """
        Get all chunks belonging to the same section as the given chunk.
        Uses heading hierarchy to identify sections.
        
        Args:
            chunk: Target chunk
            chunks: List of all chunks
            
        Returns:
            List of chunks in the same section
        """
        if not chunk.heading_hierarchy:
            # No heading hierarchy, return just this chunk
            return [chunk]
            
        section_chunks = []
        
        # Find chunks with the same heading hierarchy
        for c in chunks:
            if c.heading_hierarchy == chunk.heading_hierarchy:
                section_chunks.append(c)
                
        # Sort by position
        section_chunks.sort(key=lambda c: c.start_char_idx if c.start_char_idx is not None else 0)
        
        return section_chunks








Advanced Hybrid Search System

"""
Advanced hybrid search system combining semantic search with BM25 and additional
ranking techniques for optimal retrieval performance.
"""
import logging
import re
import math
from typing import List, Dict, Any, Optional, Tuple, Union, Callable
import numpy as np
from rank_bm25 import BM25Okapi
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import json
from dataclasses import dataclass, field
import os
from datetime import datetime

try:
    # Try to download necessary NLTK data if not already present
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)

logger = logging.getLogger(__name__)

# Import Chunk class from chunking_engine if in the same project
try:
    from chunking_engine import Chunk
except ImportError:
    # Define a minimal compatible version
    @dataclass
    class Chunk:
        text: str
        source_document: str
        chunk_id: str
        metadata: Dict[str, Any]
        embedding: Optional[np.ndarray] = None


@dataclass
class SearchResult:
    """Class representing a search result."""
    chunk: Chunk
    score: float
    rank: int = 0
    matched_terms: List[str] = field(default_factory=list)
    match_positions: List[Tuple[int, int]] = field(default_factory=list)
    semantic_score: float = 0.0
    lexical_score: float = 0.0
    knowledge_graph_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert search result to dictionary."""
        return {
            "chunk_id": self.chunk.chunk_id,
            "source_document": self.chunk.source_document,
            "score": self.score,
            "rank": self.rank,
            "matched_terms": self.matched_terms,
            "match_positions": self.match_positions,
            "semantic_score": self.semantic_score,
            "lexical_score": self.lexical_score,
            "knowledge_graph_score": self.knowledge_graph_score,
            "text": self.chunk.text,
            "metadata": self.chunk.metadata
        }


class VectorSearchIndex:
    """In-memory vector search index for embedding-based search."""
    
    def __init__(self, dimensions: int = 768, similarity_metric: str = "cosine"):
        """
        Initialize vector search index.
        
        Args:
            dimensions: Embedding dimensions
            similarity_metric: Similarity metric to use (cosine, dot, euclidean)
        """
        self.dimensions = dimensions
        self.similarity_metric = similarity_metric
        self.vectors = []
        self.chunk_ids = []
        self.chunks = {}  # Store original chunks
    
    def add_embeddings(self, embeddings: List[np.ndarray], chunk_ids: List[str], 
                      chunks: Optional[List[Chunk]] = None):
        """
        Add embeddings to the index.
        
        Args:
            embeddings: List of embedding vectors
            chunk_ids: List of corresponding chunk IDs
            chunks: Optional list of original chunks
        """
        if len(embeddings) != len(chunk_ids):
            raise ValueError("Embeddings and chunk_ids must have the same length")
            
        # Store embeddings and chunk IDs
        self.vectors.extend(embeddings)
        self.chunk_ids.extend(chunk_ids)
        
        # Store original chunks if provided
        if chunks:
            for i, chunk_id in enumerate(chunk_ids):
                if chunk_id in self.chunks:
                    logger.warning(f"Duplicate chunk ID: {chunk_id}")
                self.chunks[chunk_id] = chunks[i]
    
    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        Search for similar vectors.
        
        Args:
            query_embedding: Query embedding vector
            top_k: Number of results to return
            
        Returns:
            List of (chunk_id, score) tuples
        """
        if not self.vectors:
            return []
        
        scores = []
        
        # Calculate similarity scores based on chosen metric
        if self.similarity_metric == "cosine":
            # Normalize vectors for cosine similarity
            query_norm = np.linalg.norm(query_embedding)
            if query_norm > 0:
                query_embedding = query_embedding / query_norm
            
            for i, vec in enumerate(self.vectors):
                vec_norm = np.linalg.norm(vec)
                if vec_norm > 0:
                    vec = vec / vec_norm
                # Cosine similarity
                score = np.dot(query_embedding, vec)
                scores.append((self.chunk_ids[i], float(score)))
                
        elif self.similarity_metric == "dot":
            # Simple dot product
            for i, vec in enumerate(self.vectors):
                score = np.dot(query_embedding, vec)
                scores.append((self.chunk_ids[i], float(score)))
                
        elif self.similarity_metric == "euclidean":
            # Euclidean distance (convert to similarity)
            for i, vec in enumerate(self.vectors):
                distance = np.linalg.norm(query_embedding - vec)
                # Convert distance to similarity (1 / (1 + distance))
                similarity = 1.0 / (1.0 + float(distance))
                scores.append((self.chunk_ids[i], similarity))
        
        # Sort by score in descending order
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # Return top_k results
        return scores[:top_k]
    
    def get_chunk(self, chunk_id: str) -> Optional[Chunk]:
        """
        Get original chunk by ID.
        
        Args:
            chunk_id: Chunk ID
            
        Returns:
            Original chunk or None if not found
        """
        return self.chunks.get(chunk_id)
    
    def save(self, file_path: str):
        """
        Save the index to a file.
        
        Args:
            file_path: Path to save the index
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save as numpy arrays
        np.save(f"{file_path}_vectors.npy", np.array(self.vectors))
        
        # Save metadata separately
        metadata = {
            "chunk_ids": self.chunk_ids,
            "dimensions": self.dimensions,
            "similarity_metric": self.similarity_metric,
            "created_at": datetime.now().isoformat()
        }
        
        with open(f"{file_path}_meta.json", "w") as f:
            json.dump(metadata, f)
            
        # Save chunks
        chunk_data = {}
        for chunk_id, chunk in self.chunks.items():
            if hasattr(chunk, 'to_dict'):
                chunk_data[chunk_id] = chunk.to_dict()
            else:
                # For basic chunks, just store text and metadata
                chunk_data[chunk_id] = {
                    "text": chunk.text,
                    "source_document": chunk.source_document,
                    "metadata": chunk.metadata
                }
                
        with open(f"{file_path}_chunks.json", "w") as f:
            json.dump(chunk_data, f)
    
    @classmethod
    def load(cls, file_path: str) -> 'VectorSearchIndex':
        """
        Load the index from a file.
        
        Args:
            file_path: Path to load the index from
            
        Returns:
            Loaded index
        """
        # Load metadata
        with open(f"{file_path}_meta.json", "r") as f:
            metadata = json.load(f)
            
        # Create index instance
        index = cls(
            dimensions=metadata["dimensions"],
            similarity_metric=metadata["similarity_metric"]
        )
        
        # Load vectors
        index.vectors = np.load(f"{file_path}_vectors.npy").tolist()
        index.chunk_ids = metadata["chunk_ids"]
        
        # Load chunks if available
        try:
            with open(f"{file_path}_chunks.json", "r") as f:
                chunk_data = json.load(f)
                
            for chunk_id, data in chunk_data.items():
                if "to_dict" in data:
                    # This was created from Chunk.to_dict()
                    try:
                        from chunking_engine import Chunk
                        index.chunks[chunk_id] = Chunk.from_dict(data)
                    except ImportError:
                        # Fallback to simple chunk
                        index.chunks[chunk_id] = Chunk(
                            text=data["text"],
                            source_document=data["source_document"],
                            chunk_id=chunk_id,
                            metadata=data["metadata"]
                        )
                else:
                    # Direct chunk data
                    index.chunks[chunk_id] = Chunk(
                        text=data["text"],
                        source_document=data["source_document"],
                        chunk_id=chunk_id,
                        metadata=data["metadata"]
                    )
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.warning(f"Could not load chunks: {e}")
            
        return index


class BM25Index:
    """BM25 index for lexical search."""
    
    def __init__(self, tokenizer: Optional[Callable] = None, b: float = 0.75, k1: float = 1.6):
        """
        Initialize BM25 index.
        
        Args:
            tokenizer: Custom tokenizer function (defaults to simple tokenizer)
            b: Free parameter for document length normalization (0.75 is default)
            k1: Free parameter for term frequency scaling (1.6 is empirically better than 1.2)
        """
        self.b = b
        self.k1 = k1
        self.index = None
        self.tokenizer = tokenizer or self._default_tokenizer
        self.processed_corpus = []
        self.chunk_ids = []
        self.chunks = {}
        self.stemmer = PorterStemmer()
        self.stopwords = set(stopwords.words('english'))
    
    def _default_tokenizer(self, text: str) -> List[str]:
        """
        Default tokenization function.
        
        Args:
            text: Text to tokenize
            
        Returns:
            List of tokens
        """
        # Convert to lowercase
        text = text.lower()
        
        # Tokenize
        tokens = nltk.word_tokenize(text)
        
        # Remove stopwords and punctuation
        tokens = [self.stemmer.stem(token) for token in tokens 
                 if token.isalnum() and token not in self.stopwords]
                 
        return tokens
    
    def add_documents(self, texts: List[str], chunk_ids: List[str],
                     chunks: Optional[List[Chunk]] = None):
        """
        Add documents to the index.
        
        Args:
            texts: List of document texts
            chunk_ids: List of corresponding chunk IDs
            chunks: Optional list of original chunks
        """
        if len(texts) != len(chunk_ids):
            raise ValueError("Texts and chunk_ids must have the same length")
            
        # Tokenize documents
        processed_docs = [self.tokenizer(text) for text in texts]
        
        # Store original chunks if provided
        if chunks:
            for i, chunk_id in enumerate(chunk_ids):
                if chunk_id in self.chunks:
                    logger.warning(f"Duplicate chunk ID: {chunk_id}")
                self.chunks[chunk_id] = chunks[i]
                
        # If index already exists, extend it
        if self.index is not None:
            # Save original corpus and chunk IDs
            original_corpus = self.processed_corpus
            original_chunk_ids = self.chunk_ids
            
            # Extend with new documents
            self.processed_corpus.extend(processed_docs)
            self.chunk_ids.extend(chunk_ids)
            
            # Create a new index with all documents
            self.index = BM25Okapi(self.processed_corpus, b=self.b, k1=self.k1)
        else:
            # Create new index
            self.processed_corpus = processed_docs
            self.chunk_ids = chunk_ids
            self.index = BM25Okapi(processed_docs, b=self.b, k1=self.k1)
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        Search for documents matching the query.
        
        Args:
            query: Query string
            top_k: Number of results to return
            
        Returns:
            List of (chunk_id, score) tuples
        """
        if not self.index:
            return []
            
        # Tokenize query
        query_tokens = self.tokenizer(query)
        
        # Get BM25 scores
        scores = self.index.get_scores(query_tokens)
        
        # Create (chunk_id, score) pairs
        results = [(self.chunk_ids[i], float(score)) for i, score in enumerate(scores)]
        
        # Sort by score in descending order
        results.sort(key=lambda x: x[1], reverse=True)
        
        # Return top_k results
        return results[:top_k]
    
    def get_matched_terms(self, query: str, chunk_id: str) -> List[str]:
        """
        Get terms from the query that matched in the document.
        
        Args:
            query: Query string
            chunk_id: Chunk ID
            
        Returns:
            List of matched terms
        """
        if not self.index:
            return []
            
        # Tokenize query
        query_tokens = self.tokenizer(query)
        
        # Find chunk index
        try:
            doc_idx = self.chunk_ids.index(chunk_id)
        except ValueError:
            return []
            
        # Get document tokens
        doc_tokens = self.processed_corpus[doc_idx]
        
        # Find matches
        matched_terms = [token for token in query_tokens if token in doc_tokens]
        
        return matched_terms
    
    def get_match_positions(self, query: str, chunk_id: str) -> List[Tuple[int, int]]:
        """
        Get positions of matched terms in the document.
        
        Args:
            query: Query string
            chunk_id: Chunk ID
            
        Returns:
            List of (start, end) positions
        """
        if not self.index or chunk_id not in self.chunks:
            return []
            
        # Get original document text
        doc_text = self.chunks[chunk_id].text.lower()
        
        # Tokenize query (without stemming for highlighting)
        query_tokens = [token.lower() for token in nltk.word_tokenize(query) 
                       if token.isalnum() and token.lower() not in self.stopwords]
        
        # Find positions
        positions = []
        for token in query_tokens:
            # Find all occurrences
            for match in re.finditer(r'\b' + re.escape(token) + r'\b', doc_text):
                positions.append((match.start(), match.end()))
                
        # Sort by position
        positions.sort()
        
        return positions
    
    def get_chunk(self, chunk_id: str) -> Optional[Chunk]:
        """
        Get original chunk by ID.
        
        Args:
            chunk_id: Chunk ID
            
        Returns:
            Original chunk or None if not found
        """
        return self.chunks.get(chunk_id)
    
    def save(self, file_path: str):
        """
        Save the index to a file.
        
        Args:
            file_path: Path to save the index
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save metadata and corpus
        data = {
            "processed_corpus": self.processed_corpus,
            "chunk_ids": self.chunk_ids,
            "b": self.b,
            "k1": self.k1,
            "created_at": datetime.now().isoformat()
        }
        
        with open(f"{file_path}_corpus.json", "w") as f:
            json.dump(data, f)
            
        # Save chunks
        chunk_data = {}
        for chunk_id, chunk in self.chunks.items():
            if hasattr(chunk, 'to_dict'):
                chunk_data[chunk_id] = chunk.to_dict()
            else:
                # For basic chunks, just store text and metadata
                chunk_data[chunk_id] = {
                    "text": chunk.text,
                    "source_document": chunk.source_document,
                    "metadata": chunk.metadata
                }
                
        with open(f"{file_path}_chunks.json", "w") as f:
            json.dump(chunk_data, f)
    
    @classmethod
    def load(cls, file_path: str) -> 'BM25Index':
        """
        Load the index from a file.
        
        Args:
            file_path: Path to load the index from
            
        Returns:
            Loaded index
        """
        # Load corpus and metadata
        with open(f"{file_path}_corpus.json", "r") as f:
            data = json.load(f)
            
        # Create index instance
        index = cls(b=data["b"], k1=data["k1"])
        index.processed_corpus = data["processed_corpus"]
        index.chunk_ids = data["chunk_ids"]
        
        # Initialize BM25 index
        index.index = BM25Okapi(index.processed_corpus, b=index.b, k1=index.k1)
        
        # Load chunks if available
        try:
            with open(f"{file_path}_chunks.json", "r") as f:
                chunk_data = json.load(f)
                
            for chunk_id, data in chunk_data.items():
                if "to_dict" in data:
                    # This was created from Chunk.to_dict()
                    try:
                        from chunking_engine import Chunk
                        index.chunks[chunk_id] = Chunk.from_dict(data)
                    except ImportError:
                        # Fallback to simple chunk
                        index.chunks[chunk_id] = Chunk(
                            text=data["text"],
                            source_document=data["source_document"],
                            chunk_id=chunk_id,
                            metadata=data["metadata"]
                        )
                else:
                    # Direct chunk data
                    index.chunks[chunk_id] = Chunk(
                        text=data["text"],
                        source_document=data["source_document"],
                        chunk_id=chunk_id,
                        metadata=data["metadata"]
                    )
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.warning(f"Could not load chunks: {e}")
            
        return index


class HybridSearcher:
    """
    Advanced hybrid search system combining semantic and lexical search
    with additional ranking techniques.
    """
    
    def __init__(self, vector_index: Optional[VectorSearchIndex] = None,
                 bm25_index: Optional[BM25Index] = None,
                 semantic_weight: float = 0.7,
                 use_knowledge_graph: bool = False):
        """
        Initialize hybrid searcher.
        
        Args:
            vector_index: Vector index for semantic search
            bm25_index: BM25 index for lexical search
            semantic_weight: Weight for semantic search scores (0-1)
            use_knowledge_graph: Whether to use knowledge graph for ranking
        """
        self.vector_index = vector_index or VectorSearchIndex()
        self.bm25_index = bm25_index or BM25Index()
        self.semantic_weight = semantic_weight
        self.lexical_weight = 1.0 - semantic_weight
        self.use_knowledge_graph = use_knowledge_graph
        self.knowledge_graph = None  # Placeholder for knowledge graph
    
    def add_chunks(self, chunks: List[Chunk], embeddings: Optional[List[np.ndarray]] = None):
        """
        Add chunks to search indices.
        
        Args:
            chunks: List of chunks to add
            embeddings: Optional pre-computed embeddings for chunks
        """
        # Extract texts and IDs
        texts = [chunk.text for chunk in chunks]
        chunk_ids = [chunk.chunk_id for chunk in chunks]
        
        # Add to BM25 index
        self.bm25_index.add_documents(texts, chunk_ids, chunks)
        
        # Add to vector index if embeddings are provided
        if embeddings is not None:
            if len(embeddings) != len(chunks):
                raise ValueError("Number of embeddings must match number of chunks")
                
            self.vector_index.add_embeddings(embeddings, chunk_ids, chunks)
    
    def search(self, query: str, query_embedding: Optional[np.ndarray] = None,
              top_k: int = 10, filter_func: Optional[Callable] = None,
              rerank: bool = True) -> List[SearchResult]:
        """
        Perform hybrid search.
        
        Args:
            query: Query string
            query_embedding: Optional pre-computed query embedding
            top_k: Number of results to return
            filter_func: Optional function to filter results
            rerank: Whether to apply reranking
            
        Returns:
            List of SearchResult objects
        """
        # Get more results than needed for better reranking
        semantic_top_k = top_k * 2
        lexical_top_k = top_k * 2
        
        # Semantic search results
        semantic_results = []
        if query_embedding is not None:
            semantic_results = self.vector_index.search(query_embedding, semantic_top_k)
        
        # Lexical search results
        lexical_results = self.bm25_index.search(query, lexical_top_k)
        
        # Combine results
        combined_results = self._combine_results(
            semantic_results, 
            lexical_results, 
            query,
            filter_func
        )
        
        # Apply reranking if requested
        if rerank:
            combined_results = self._rerank_results(combined_results, query)
            
        # Return top results
        return combined_results[:top_k]
    
    def _combine_results(self, semantic_results: List[Tuple[str, float]],
                        lexical_results: List[Tuple[str, float]],
                        query: str, 
                        filter_func: Optional[Callable] = None) -> List[SearchResult]:
        """
        Combine results from semantic and lexical search.
        
        Args:
            semantic_results: Results from semantic search
            lexical_results: Results from lexical search
            query: Original query string
            filter_func: Optional function to filter results
            
        Returns:
            List of combined SearchResult objects
        """
        # Create a map of chunk_id to scores
        combined_map = {}
        
        # Process semantic results
        for chunk_id, score in semantic_results:
            # Get chunk from either index
            chunk = self.vector_index.get_chunk(chunk_id) or self.bm25_index.get_chunk(chunk_id)
            
            if not chunk:
                continue
                
            # Apply filter if provided
            if filter_func and not filter_func(chunk):
                continue
                
            # Initialize combined result
            combined_map[chunk_id] = SearchResult(
                chunk=chunk,
                score=score * self.semantic_weight,
                semantic_score=score,
                lexical_score=0.0
            )
            
        # Process lexical results
        for chunk_id, score in lexical_results:
            # Get chunk from either index
            chunk = self.bm25_index.get_chunk(chunk_id) or self.vector_index.get_chunk(chunk_id)
            
            if not chunk:
                continue
                
            # Apply filter if provided
            if filter_func and not filter_func(chunk):
                continue
            
            if chunk_id in combined_map:
                # Update existing entry
                combined_map[chunk_id].score += score * self.lexical_weight
                combined_map[chunk_id].lexical_score = score
            else:
                # Create new entry
                combined_map[chunk_id] = SearchResult(
                    chunk=chunk,
                    score=score * self.lexical_weight,
                    semantic_score=0.0,
                    lexical_score=score
                )
            
            # Add match information for lexical search
            combined_map[chunk_id].matched_terms = self.bm25_index.get_matched_terms(query, chunk_id)
            combined_map[chunk_id].match_positions = self.bm25_index.get_match_positions(query, chunk_id)
        
        # Add knowledge graph scores if enabled
        if self.use_knowledge_graph and self.knowledge_graph:
            self._add_knowledge_graph_scores(combined_map, query)
            
        # Convert map to list and sort
        results = list(combined_map.values())
        results.sort(key=lambda x: x.score, reverse=True)
        
        # Assign ranks
        for i, result in enumerate(results):
            result.rank = i + 1
            
        return results
    
    def _add_knowledge_graph_scores(self, results_map: Dict[str, SearchResult], query: str):
        """
        Add knowledge graph scores to search results.
        
        Args:
            results_map: Map of chunk_id to SearchResult
            query: Original query string
        """
        # This is a placeholder - actual implementation would depend on knowledge graph interface
        pass
    
    def _rerank_results(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """
        Rerank results using advanced ranking techniques.
        
        Args:
            results: List of initial search results
            query: Original query
            
        Returns:
            Reranked results
        """
        if not results:
            return []
            
        # Apply multiple reranking factors
        for result in results:
            # Base score from semantic and lexical combination
            final_score = result.score
            
            # Factor 1: Boost results with more matches
            match_count = len(result.matched_terms)
            if match_count > 0:
                match_boost = min(0.1 * match_count, 0.3)  # Up to 0.3 boost
                final_score += match_boost
                
            # Factor 2: Boost by content type relevance
            content_boost = self._calculate_content_type_boost(result.chunk, query)
            final_score += content_boost
            
            # Factor 3: Boost by metadata matches
            metadata_boost = self._calculate_metadata_boost(result.chunk, query)
            final_score += metadata_boost
                
            # Factor 4: Boost by document freshness (if available)
            if "created_at" in result.chunk.metadata or "updated_at" in result.chunk.metadata:
                freshness_boost = self._calculate_freshness_boost(result.chunk)
                final_score += freshness_boost
                
            # Update the final score
            result.score = final_score
            
        # Re-sort results
        results.sort(key=lambda x: x.score, reverse=True)
        
        # Update ranks
        for i, result in enumerate(results):
            result.rank = i + 1
            
        return results
    
    def _calculate_content_type_boost(self, chunk: Chunk, query: str) -> float:
        """
        Calculate boost based on content type relevance to query.
        
        Args:
            chunk: Document chunk
            query: Query string
            
        Returns:
            Boost score (0-0.3)
        """
        boost = 0.0
        query_lower = query.lower()
        
        # Get content type
        content_type = chunk.metadata.get("content_type", "")
        
        # Boost tables for queries about data
        if content_type == "table" and any(term in query_lower for term in 
                                         ["data", "numbers", "statistics", "table"]):
            boost += 0.3
        
        # Boost code for code-related queries
        elif content_type == "code" and any(term in query_lower for term in 
                                          ["code", "function", "class", "implementation"]):
            boost += 0.3
            
        # Boost images for image queries
        elif content_type == "image" and any(term in query_lower for term in 
                                           ["image", "picture", "figure", "diagram"]):
            boost += 0.3
            
        # Boost headings (they often contain key information)
        elif chunk.metadata.get("is_heading", False):
            boost += 0.2
            
        return boost
    
    def _calculate_metadata_boost(self, chunk: Chunk, query: str) -> float:
        """
        Calculate boost based on metadata matches.
        
        Args:
            chunk: Document chunk
            query: Query string
            
        Returns:
            Boost score (0-0.2)
        """
        boost = 0.0
        query_terms = query.lower().split()
        
        # Check for source document name match
        source_doc = chunk.source_document.lower()
        if any(term in source_doc for term in query_terms):
            boost += 0.1
            
        # Check for metadata matches
        for key, value in chunk.metadata.items():
            if isinstance(value, str):
                value_lower = value.lower()
                if any(term in value_lower for term in query_terms):
                    boost += 0.05
                    break
                    
        return min(boost, 0.2)  # Cap at 0.2
    
    def _calculate_freshness_boost(self, chunk: Chunk) -> float:
        """
        Calculate boost based on document freshness.
        
        Args:
            chunk: Document chunk
            
        Returns:
            Boost score (0-0.1)
        """
        # Get created/updated timestamp
        timestamp = None
        if "updated_at" in chunk.metadata:
            timestamp = chunk.metadata["updated_at"]
        elif "created_at" in chunk.metadata:
            timestamp = chunk.metadata["created_at"]
            
        if not timestamp:
            return 0.0
            
        # Parse timestamp
        try:
            if isinstance(timestamp, str):
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = timestamp
                
            # Calculate age in days
            age_days = (datetime.now() - dt).days
            
            # Apply decay function (newer documents get higher boost)
            # 0.1 boost for very recent documents, decaying to 0 for older ones
            boost = 0.1 * math.exp(-age_days / 30)  # 30 day half-life
            
            return boost
        except:
            return 0.0
    
    def highlight_matches(self, result: SearchResult, max_context: int = 50) -> str:
        """
        Create highlighted text snippets around matches.
        
        Args:
            result: Search result
            max_context: Maximum context size around matches
            
        Returns:
            Highlighted text with match context
        """
        if not result.match_positions:
            # No matches, return a snippet from the beginning
            text = result.chunk.text
            return text[:min(len(text), 200)] + "..."
            
        # Sort match positions
        positions = sorted(result.match_positions)
        
        # Merge overlapping positions
        merged = []
        current = positions[0]
        
        for pos in positions[1:]:
            if pos[0] <= current[1] + 5:  # Allow small gaps
                current = (current[0], max(current[1], pos[1]))
            else:
                merged.append(current)
                current = pos
                
        merged.append(current)
        
        # Create snippets with context
        text = result.chunk.text
        snippets = []
        
        for start, end in merged:
            # Add context around match
            context_start = max(0, start - max_context)
            context_end = min(len(text), end + max_context)
            
            # Extract snippet
            snippet = text[context_start:start] + "**" + text[start:end] + "**" + text[end:context_end]
            
            # Add ellipsis if needed
            if context_start > 0:
                snippet = "..." + snippet
            if context_end < len(text):
                snippet = snippet + "..."
                
            snippets.append(snippet)
            
        # Combine snippets (limit to 3 for readability)
        return " [...] ".join(snippets[:3])
    
    def save_indices(self, base_path: str):
        """
        Save search indices to disk.
        
        Args:
            base_path: Base path for saving indices
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(base_path), exist_ok=True)
        
        # Save vector index
        self.vector_index.save(f"{base_path}_vector")
        
        # Save BM25 index
        self.bm25_index.save(f"{base_path}_bm25")
        
        # Save configuration
        config = {
            "semantic_weight": self.semantic_weight,
            "lexical_weight": self.lexical_weight,
            "use_knowledge_graph": self.use_knowledge_graph,
            "saved_at": datetime.now().isoformat()
        }
        
        with open(f"{base_path}_config.json", "w") as f:
            json.dump(config, f)
    
    @classmethod
    def load_indices(cls, base_path: str) -> 'HybridSearcher':
        """
        Load search indices from disk.
        
        Args:
            base_path: Base path for loading indices
            
        Returns:
            Initialized HybridSearcher
        """
        # Load configuration
        try:
            with open(f"{base_path}_config.json", "r") as f:
                config = json.load(f)
        except FileNotFoundError:
            # Use defaults if config not found
            config = {
                "semantic_weight": 0.7,
                "lexical_weight": 0.3,
                "use_knowledge_graph": False
            }
            
        # Load vector index
        try:
            vector_index = VectorSearchIndex.load(f"{base_path}_vector")
        except FileNotFoundError:
            vector_index = VectorSearchIndex()
            
        # Load BM25 index
        try:
            bm25_index = BM25Index.load(f"{base_path}_bm25")
        except FileNotFoundError:
            bm25_index = BM25Index()
            
        # Create and return searcher
        return cls(
            vector_index=vector_index,
            bm25_index=bm25_index,
            semantic_weight=config.get("semantic_weight", 0.7),
            use_knowledge_graph=config.get("use_knowledge_graph", False)
        )

















Answer Generator with Non-Hallucination Guarantees


"""
Advanced answer generator that creates coherent, non-hallucinated responses from retrieved documents.
Uses evidence ranking, fact validation, and careful synthesis to ensure accuracy.
"""
import logging
import re
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np
from dataclasses import dataclass, field
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import math
from collections import Counter, defaultdict
import string

try:
    # Try to download necessary NLTK data if not already present
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)

logger = logging.getLogger(__name__)

# Import SearchResult class if available, otherwise define a compatible version
try:
    from hybrid_search import SearchResult
except ImportError:
    @dataclass
    class SearchResult:
        """Class representing a search result."""
        chunk: Any  # Should have text and metadata
        score: float
        rank: int = 0
        matched_terms: List[str] = field(default_factory=list)
        match_positions: List[Tuple[int, int]] = field(default_factory=list)
        semantic_score: float = 0.0
        lexical_score: float = 0.0


@dataclass
class Evidence:
    """Class representing a piece of evidence extracted from search results."""
    text: str
    source: str
    relevance_score: float
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    supporting_facts: List[str] = field(default_factory=list)
    contradicting_facts: List[str] = field(default_factory=list)


@dataclass
class AnswerPlan:
    """Class representing a plan for constructing an answer."""
    query: str
    sections: List[Dict[str, Any]] = field(default_factory=list)
    evidence: List[Evidence] = field(default_factory=list)
    opening: Optional[str] = None
    conclusion: Optional[str] = None
    answer_type: str = "informative"


@dataclass
class GeneratedAnswer:
    """Class representing a generated answer."""
    text: str
    source_documents: List[str]
    confidence: float
    supporting_evidence: List[Evidence]
    answer_type: str
    query: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class EvidenceExtractor:
    """Extracts and ranks evidence from search results."""
    
    def __init__(self):
        """Initialize the evidence extractor."""
        self.stopwords = set(stopwords.words('english'))
    
    def extract_evidence(self, search_results: List[SearchResult], 
                         query: str, max_evidence: int = 15) -> List[Evidence]:
        """
        Extract evidence from search results.
        
        Args:
            search_results: List of search results
            query: Original query
            max_evidence: Maximum number of evidence pieces to extract
            
        Returns:
            List of Evidence objects
        """
        evidence_list = []
        
        # Compute query terms for matching
        query_terms = self._extract_terms(query)
        
        # First pass: Extract sentences or paragraphs that might contain evidence
        for result in search_results:
            # Get text and break into sentences
            text = result.chunk.text
            
            # For very short results, keep them as is
            if len(text.split()) < 30:
                source = self._format_source(result)
                confidence = min(0.8, result.score)  # Cap confidence at 0.8
                
                evidence_list.append(Evidence(
                    text=text,
                    source=source,
                    relevance_score=result.score,
                    confidence=confidence,
                    metadata={"rank": result.rank, "matched_terms": result.matched_terms}
                ))
                continue
                
            # Break longer results into sentences
            sentences = sent_tokenize(text)
            
            # Score each sentence for relevance to the query
            for sentence in sentences:
                if len(sentence.split()) < 4:  # Skip very short sentences
                    continue
                    
                # Calculate relevance score for this sentence
                sentence_score = self._calculate_evidence_score(sentence, query_terms, result.score)
                
                if sentence_score > 0.3:  # Only consider somewhat relevant sentences
                    source = self._format_source(result)
                    
                    # Calculate confidence based on source rank and sentence matching
                    confidence = sentence_score * 0.7 + (1.0 / (result.rank + 1)) * 0.3
                    
                    evidence_list.append(Evidence(
                        text=sentence,
                        source=source,
                        relevance_score=sentence_score,
                        confidence=confidence,
                        metadata={"rank": result.rank, "original_score": result.score}
                    ))
        
        # Second pass: Group very similar evidence
        grouped_evidence = self._group_similar_evidence(evidence_list)
        
        # Third pass: Detect and mark contradictions
        final_evidence = self._detect_contradictions(grouped_evidence)
        
        # Sort by relevance score and limit to max_evidence
        final_evidence.sort(key=lambda e: e.relevance_score, reverse=True)
        return final_evidence[:max_evidence]
    
    def _extract_terms(self, text: str) -> List[str]:
        """Extract significant terms from text."""
        # Tokenize and lowercase
        tokens = text.lower().split()
        
        # Remove stopwords and punctuation
        terms = [token for token in tokens 
                if token not in self.stopwords 
                and token not in string.punctuation
                and len(token) > 1]
                
        return terms
    
    def _calculate_evidence_score(self, sentence: str, query_terms: List[str], 
                                 base_score: float) -> float:
        """
        Calculate a relevance score for a potential evidence sentence.
        
        Args:
            sentence: Candidate evidence sentence
            query_terms: Extracted terms from query
            base_score: Base score from search result
            
        Returns:
            Evidence relevance score
        """
        # Extract terms from sentence
        sentence_terms = self._extract_terms(sentence)
        
        # Calculate overlap
        matching_terms = set(query_terms).intersection(sentence_terms)
        
        if not matching_terms:
            return 0.1 * base_score  # Low score if no term matches
            
        # Calculate match ratio (what proportion of query terms are matched)
        match_ratio = len(matching_terms) / len(query_terms) if query_terms else 0
        
        # Calculate density (what proportion of sentence terms are query terms)
        density = len(matching_terms) / len(sentence_terms) if sentence_terms else 0
        
        # Combine factors with base score
        score = (0.4 * match_ratio + 0.3 * density + 0.3 * base_score)
        
        return score
    
    def _format_source(self, result: SearchResult) -> str:
        """Format source information from search result."""
        # Extract document name
        doc_name = result.chunk.source_document
        
        # Remove file extensions if present
        if '.' in doc_name:
            doc_name = doc_name.rsplit('.', 1)[0]
            
        # Clean up formatting
        doc_name = doc_name.replace('_', ' ').replace('-', ' ')
        
        # Get additional metadata if available
        metadata = result.chunk.metadata
        
        # Add section info if available
        if 'heading_hierarchy' in metadata and metadata['heading_hierarchy']:
            headings = metadata['heading_hierarchy']
            section = headings[-1] if headings else ""
            return f"{doc_name} - {section}"
            
        return doc_name
    
    def _group_similar_evidence(self, evidence_list: List[Evidence]) -> List[Evidence]:
        """Group and merge very similar evidence items."""
        if not evidence_list:
            return []
            
        # Sort by relevance score
        sorted_evidence = sorted(evidence_list, key=lambda e: e.relevance_score, reverse=True)
        
        # Initialize result with first item
        result = [sorted_evidence[0]]
        
        # Check each evidence against already included items
        for evidence in sorted_evidence[1:]:
            # Check if this evidence is too similar to an existing one
            is_similar = False
            
            for existing in result:
                similarity = self._calculate_text_similarity(evidence.text, existing.text)
                
                if similarity > 0.7:  # Threshold for considering items similar
                    # If this one is better, replace the existing one
                    if evidence.relevance_score > existing.relevance_score:
                        existing.text = evidence.text
                        existing.relevance_score = evidence.relevance_score
                        existing.confidence = evidence.confidence
                        existing.metadata = evidence.metadata
                        # Preserve source as a list of sources
                        if 'sources' in existing.metadata:
                            existing.metadata['sources'].append(evidence.source)
                        else:
                            existing.metadata['sources'] = [existing.source, evidence.source]
                    
                    is_similar = True
                    break
                    
            if not is_similar:
                result.append(evidence)
                
        return result
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two text strings."""
        # Simple Jaccard similarity on word sets
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _detect_contradictions(self, evidence_list: List[Evidence]) -> List[Evidence]:
        """Detect potential contradictions between evidence items."""
        if len(evidence_list) < 2:
            return evidence_list
            
        # Extract numeric values from each evidence
        for evidence in evidence_list:
            evidence.metadata['numbers'] = self._extract_numbers(evidence.text)
            
        # Check for contradictions
        for i, evidence1 in enumerate(evidence_list):
            for j in range(i + 1, len(evidence_list)):
                evidence2 = evidence_list[j]
                
                # Compare number values if they exist
                if (evidence1.metadata['numbers'] and evidence2.metadata['numbers']):
                    if self._check_numeric_contradiction(
                        evidence1.metadata['numbers'], 
                        evidence2.metadata['numbers']
                    ):
                        # Note the contradiction in both pieces of evidence
                        evidence1.contradicting_facts.append(evidence2.text)
                        evidence2.contradicting_facts.append(evidence1.text)
                        
                        # Reduce confidence for both
                        evidence1.confidence *= 0.8
                        evidence2.confidence *= 0.8
                        
                # Add more contradiction detection here (e.g., comparing positive/negative statements)
                
        return evidence_list
    
    def _extract_numbers(self, text: str) -> Dict[str, float]:
        """Extract numeric values with their context."""
        number_pattern = r'(\d+(?:\.\d+)?(?:\s*%)?)'
        
        # Find all matches with context
        numbers = {}
        
        matches = re.finditer(number_pattern, text)
        for match in matches:
            number_str = match.group(1)
            
            # Get context (5 words before and after)
            start = max(0, match.start() - 30)
            end = min(len(text), match.end() + 30)
            context = text[start:end]
            
            # Convert to float and store with context
            try:
                if '%' in number_str:
                    # Handle percentages
                    value = float(number_str.replace('%', '').strip()) / 100.0
                    numbers[context] = {'value': value, 'is_percentage': True}
                else:
                    value = float(number_str)
                    numbers[context] = {'value': value, 'is_percentage': False}
            except ValueError:
                pass
                
        return numbers
    
    def _check_numeric_contradiction(self, numbers1: Dict[str, Dict[str, Any]], 
                                    numbers2: Dict[str, Dict[str, Any]]) -> bool:
        """Check if two sets of numeric values contradict each other."""
        for context1, info1 in numbers1.items():
            for context2, info2 in numbers2.items():
                # Check for same context (some word overlap)
                words1 = set(context1.lower().split())
                words2 = set(context2.lower().split())
                
                if len(words1.intersection(words2)) >= 3:  # At least 3 words in common
                    # Compare values
                    v1, v2 = info1['value'], info2['value']
                    both_percentages = info1.get('is_percentage', False) and info2.get('is_percentage', False)
                    
                    # Different thresholds for contradiction depending on value type
                    if both_percentages:
                        # For percentages, 5 percentage points difference is significant
                        if abs(v1 - v2) > 0.05:
                            return True
                    else:
                        # For regular numbers, 20% relative difference is significant
                        if max(v1, v2) > 0:
                            relative_diff = abs(v1 - v2) / max(v1, v2)
                            if relative_diff > 0.2:
                                return True
                                
        return False


class AnswerPlanner:
    """Plans the structure of an answer based on evidence and query analysis."""
    
    def __init__(self):
        """Initialize the answer planner."""
        pass
    
    def create_answer_plan(self, query: str, evidence_list: List[Evidence]) -> AnswerPlan:
        """
        Create a plan for constructing an answer.
        
        Args:
            query: Original query
            evidence_list: List of evidence items
            
        Returns:
            AnswerPlan object
        """
        # Analyze query to determine answer type
        query_info = self._analyze_query(query)
        
        # Determine best organization method
        organization_method = self._determine_organization(query_info, evidence_list)
        
        # Initialize plan
        plan = AnswerPlan(
            query=query,
            evidence=evidence_list,
            answer_type=query_info['type']
        )
        
        # Create opening statement
        plan.opening = self._create_opening(query_info, evidence_list)
        
        # Organize evidence into sections
        if organization_method == 'topical':
            plan.sections = self._organize_by_topic(evidence_list, query_info)
        elif organization_method == 'comparison':
            plan.sections = self._organize_by_comparison(evidence_list, query_info)
        elif organization_method == 'chronological':
            plan.sections = self._organize_chronologically(evidence_list)
        elif organization_method == 'factual_list':
            plan.sections = self._organize_as_factual_list(evidence_list, query_info)
        else:  # Default to relevance
            plan.sections = self._organize_by_relevance(evidence_list)
            
        # Create conclusion
        plan.conclusion = self._create_conclusion(query_info, evidence_list, plan.sections)
        
        return plan
    
    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze the query to determine the appropriate answer approach.
        
        Args:
            query: Original query
            
        Returns:
            Dictionary with query analysis
        """
        query_lower = query.lower()
        
        # Check for question type
        is_question = '?' in query
        
        # Determine question or instruction type
        query_type = 'informative'  # Default type
        
        # Common question word patterns
        question_patterns = {
            'what': 'descriptive',
            'who': 'entity',
            'where': 'location',
            'when': 'temporal',
            'why': 'explanatory',
            'how': 'procedural',
            'which': 'selection',
            'can': 'capability',
            'does': 'verification',
            'is': 'verification'
        }
        
        # Check for question patterns
        if is_question:
            first_word = query_lower.split()[0]
            if first_word in question_patterns:
                query_type = question_patterns[first_word]
                
        # Check for specific instructions or patterns
        if 'explain' in query_lower or 'describe' in query_lower:
            query_type = 'explanatory'
        elif 'step' in query_lower or 'process' in query_lower or 'how to' in query_lower:
            query_type = 'procedural'
        elif 'compare' in query_lower or 'difference between' in query_lower or ' vs ' in query_lower:
            query_type = 'comparison'
        elif 'list' in query_lower or 'what are' in query_lower:
            query_type = 'list'
            
        # Check for factual vs opinion
        is_opinion = any(term in query_lower for term in 
                        ['opinion', 'think', 'better', 'best', 'should', 'recommend'])
        
        # Identify key entities in the query
        entities = self._extract_entities(query)
        
        return {
            'type': query_type,
            'is_question': is_question,
            'is_opinion': is_opinion,
            'entities': entities
        }
    
    def _extract_entities(self, query: str) -> List[str]:
        """Extract potential entities from the query."""
        # This is a simplified version - in a real system, you'd use NER
        
        # Look for capitalized words as potential entities
        entities = []
        words = query.split()
        for word in words:
            if word[0].isupper() and len(word) > 1 and word.lower() not in ['what', 'who', 'where', 'when', 'why', 'how']:
                entities.append(word)
                
        # Look for quoted terms
        quoted = re.findall(r'"([^"]*)"', query)
        entities.extend(quoted)
        
        return entities
    
    def _determine_organization(self, query_info: Dict[str, Any], 
                              evidence_list: List[Evidence]) -> str:
        """
        Determine the best organization method for the answer.
        
        Args:
            query_info: Query analysis
            evidence_list: List of evidence items
            
        Returns:
            Organization method name
        """
        query_type = query_info['type']
        
        # Clear mappings from query type to organization method
        if query_type == 'comparison':
            return 'comparison'
        elif query_type == 'list':
            return 'factual_list'
        elif query_type == 'procedural':
            return 'chronological'
        elif query_type == 'descriptive' or query_type == 'explanatory':
            # Try to identify topics in the evidence
            if self._can_organize_by_topic(evidence_list):
                return 'topical'
            else:
                return 'relevance'
        
        # Default to organizing by relevance
        return 'relevance'
    
    def _can_organize_by_topic(self, evidence_list: List[Evidence]) -> bool:
        """
        Determine if evidence can be organized by topic.
        
        Args:
            evidence_list: List of evidence items
            
        Returns:
            Whether topical organization is possible
        """
        # This would use more sophisticated topic modeling in a real system
        # For now, just check if we have enough evidence
        return len(evidence_list) >= 5
    
    def _create_opening(self, query_info: Dict[str, Any], 
                       evidence_list: List[Evidence]) -> str:
        """
        Create an opening statement for the answer.
        
        Args:
            query_info: Query analysis
            evidence_list: List of evidence items
            
        Returns:
            Opening statement
        """
        query_type = query_info['type']
        
        # Different openings based on query type
        if query_type == 'descriptive':
            return "Based on the available information:"
        elif query_type == 'explanatory':
            return "Here's an explanation based on the information:"
        elif query_type == 'procedural':
            return "Here are the steps based on the available information:"
        elif query_type == 'comparison':
            entities = query_info['entities']
            if len(entities) >= 2:
                return f"Here's a comparison between {entities[0]} and {entities[1]} based on the available information:"
            else:
                return "Here's a comparison based on the available information:"
        elif query_type == 'verification':
            return "Based on the available information:"
        elif query_type == 'list':
            return "Here's a list based on the available information:"
        else:
            return "Here's what I found in the available information:"
    
    def _create_conclusion(self, query_info: Dict[str, Any], 
                         evidence_list: List[Evidence],
                         sections: List[Dict[str, Any]]) -> str:
        """
        Create a conclusion for the answer.
        
        Args:
            query_info: Query analysis
            evidence_list: List of evidence items
            sections: Answer sections
            
        Returns:
            Conclusion statement
        """
        query_type = query_info['type']
        
        # Simple conclusion based on query type
        if query_type == 'explanatory':
            return "This explanation is based on the information available in the documents."
        elif query_type == 'comparison':
            return "This comparison is based on the information available in the documents."
        elif query_type == 'procedural':
            return "These steps are based on the procedures described in the documents."
        else:
            # Check if there were contradictions
            contradictions = any(len(e.contradicting_facts) > 0 for e in evidence_list)
            
            if contradictions:
                return "Note that some of the information sources contained different details. The answer provided focuses on the most consistent and well-supported information."
            else:
                return "This information is based on the content found in the documents."
    
    def _organize_by_relevance(self, evidence_list: List[Evidence]) -> List[Dict[str, Any]]:
        """
        Organize evidence by relevance score.
        
        Args:
            evidence_list: List of evidence items
            
        Returns:
            List of section dictionaries
        """
        # Just one section with all evidence in relevance order
        return [{
            'title': None,
            'evidence': sorted(evidence_list, key=lambda e: e.relevance_score, reverse=True)
        }]
    
    def _organize_by_topic(self, evidence_list: List[Evidence], 
                         query_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Organize evidence by topic.
        
        Args:
            evidence_list: List of evidence items
            query_info: Query analysis
            
        Returns:
            List of section dictionaries
        """
        # This would use more sophisticated topic clustering in a real system
        # For now, use a simple approach based on term frequency
        
        # Extract and count all significant terms
        all_terms = []
        for evidence in evidence_list:
            terms = evidence.text.lower().split()
            # Skip stopwords and short terms
            terms = [t for t in terms if t not in self.stopwords and len(t) > 3]
            all_terms.extend(terms)
            
        # Count term frequency
        term_counter = Counter(all_terms)
        
        # Get top terms (potential topics)
        top_terms = [term for term, count in term_counter.most_common(5) 
                    if count >= 2]  # Must appear at least twice
        
        # If no good topics found, fall back to relevance
        if not top_terms:
            return self._organize_by_relevance(evidence_list)
            
        # Create sections based on top terms
        sections = []
        used_evidence = set()
        
        for term in top_terms:
            # Find evidence related to this term
            term_evidence = []
            for evidence in evidence_list:
                # Skip already used evidence
                if id(evidence) in used_evidence:
                    continue
                    
                # Check if this term appears in the evidence
                if term in evidence.text.lower():
                    term_evidence.append(evidence)
                    used_evidence.add(id(evidence))
                    
            # Only create a section if we have at least 2 pieces of evidence
            if len(term_evidence) >= 2:
                # Capitalize term for section title
                title = term.capitalize()
                sections.append({
                    'title': title,
                    'evidence': sorted(term_evidence, key=lambda e: e.relevance_score, reverse=True)
                })
                
        # Add remaining evidence in a general section
        remaining = [e for e in evidence_list if id(e) not in used_evidence]
        if remaining:
            sections.append({
                'title': "Additional Information",
                'evidence': sorted(remaining, key=lambda e: e.relevance_score, reverse=True)
            })
            
        return sections
    
    def _organize_by_comparison(self, evidence_list: List[Evidence], 
                              query_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Organize evidence for comparison queries.
        
        Args:
            evidence_list: List of evidence items
            query_info: Query analysis
            
        Returns:
            List of section dictionaries
        """
        entities = query_info['entities']
        
        # If we don't have entities to compare, fall back to relevance
        if len(entities) < 2:
            return self._organize_by_relevance(evidence_list)
            
        # Create sections for each entity
        sections = []
        entity_evidence = {entity: [] for entity in entities}
        shared_evidence = []
        
        # Assign evidence to entities
        for evidence in evidence_list:
            assigned = False
            for entity in entities:
                if entity.lower() in evidence.text.lower():
                    entity_evidence[entity].append(evidence)
                    assigned = True
                    
            # If evidence mentions multiple entities, add to all relevant ones
            if not assigned:
                shared_evidence.append(evidence)
                
        # Create a section for each entity
        for entity in entities:
            if entity_evidence[entity]:
                sections.append({
                    'title': entity,
                    'evidence': sorted(entity_evidence[entity], 
                                     key=lambda e: e.relevance_score, reverse=True)
                })
                
        # Add shared evidence if any
        if shared_evidence:
            sections.append({
                'title': "Comparison",
                'evidence': sorted(shared_evidence, key=lambda e: e.relevance_score, reverse=True)
            })
            
        return sections
    
    def _organize_chronologically(self, evidence_list: List[Evidence]) -> List[Dict[str, Any]]:
        """
        Organize evidence in chronological order (for procedures, etc.)
        
        Args:
            evidence_list: List of evidence items
            
        Returns:
            List of section dictionaries
        """
        # Look for chronological indicators
        indicators = [
            ("first", 1), ("1st", 1), ("1\\.", 1), ("step 1", 1),
            ("second", 2), ("2nd", 2), ("2\\.", 2), ("step 2", 2),
            ("third", 3), ("3rd", 3), ("3\\.", 3), ("step 3", 3),
            ("fourth", 4), ("4th", 4), ("4\\.", 4), ("step 4", 4),
            ("fifth", 5), ("5th", 5), ("5\\.", 5), ("step 5", 5),
            ("next", 10), ("then", 10), ("after", 10), 
            ("finally", 100), ("lastly", 100)
        ]
        
        # Assign positions to evidence based on chronological indicators
        positioned_evidence = []
        unpositioned_evidence = []
        
        for evidence in evidence_list:
            # Check for indicators
            position = None
            text_lower = evidence.text.lower()
            
            for indicator, pos in indicators:
                if re.search(r'\b' + indicator + r'\b', text_lower):
                    # Found an indicator
                    position = pos
                    break
                    
            if position is not None:
                positioned_evidence.append((position, evidence))
            else:
                unpositioned_evidence.append(evidence)
                
        # Sort positioned evidence
        positioned_evidence.sort()
        
        # Check if we have enough positioned evidence
        if len(positioned_evidence) >= 3:
            # Create a chronological section
            chronological = [e for _, e in positioned_evidence]
            
            sections = [{
                'title': "Steps",
                'evidence': chronological
            }]
            
            # Add unpositioned evidence in a separate section if needed
            if unpositioned_evidence:
                sections.append({
                    'title': "Additional Information",
                    'evidence': sorted(unpositioned_evidence, 
                                     key=lambda e: e.relevance_score, reverse=True)
                })
                
            return sections
        else:
            # Not enough chronological evidence, fall back to relevance
            return self._organize_by_relevance(evidence_list)
    
    def _organize_as_factual_list(self, evidence_list: List[Evidence], 
                                 query_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Organize evidence as a factual list.
        
        Args:
            evidence_list: List of evidence items
            query_info: Query analysis
            
        Returns:
            List of section dictionaries
        """
        # Sort evidence by relevance
        sorted_evidence = sorted(evidence_list, key=lambda e: e.relevance_score, reverse=True)
        
        # Just one section with all evidence as list items
        return [{
            'title': None,
            'evidence': sorted_evidence,
            'format': 'list'
        }]


class AnswerGenerator:
    """Generates coherent answers from evidence and answer plans."""
    
    def __init__(self):
        """Initialize the answer generator."""
        self.evidence_extractor = EvidenceExtractor()
        self.answer_planner = AnswerPlanner()
    
    def generate_answer(self, query: str, search_results: List[SearchResult]) -> GeneratedAnswer:
        """
        Generate a coherent answer based on search results.
        
        Args:
            query: Original query
            search_results: List of search results
            
        Returns:
            GeneratedAnswer object
        """
        # Extract and rank evidence
        evidence = self.evidence_extractor.extract_evidence(search_results, query)
        
        # Validate evidence for contradictions and quality
        validated_evidence = self._validate_evidence(evidence)
        
        # Create answer plan
        answer_plan = self.answer_planner.create_answer_plan(query, validated_evidence)
        
        # Generate the final answer text
        answer_text = self._realize_answer(answer_plan)
        
        # Calculate overall confidence
        confidence = self._calculate_confidence(validated_evidence, answer_plan)
        
        # Get source documents
        source_documents = self._get_source_documents(validated_evidence)
        
        # Create metadata
        metadata = {
            "evidence_count": len(validated_evidence),
            "contradictions_found": any(len(e.contradicting_facts) > 0 for e in validated_evidence),
            "answer_type": answer_plan.answer_type
        }
        
        # Create the final answer
        generated_answer = GeneratedAnswer(
            text=answer_text,
            source_documents=source_documents,
            confidence=confidence,
            supporting_evidence=validated_evidence,
            answer_type=answer_plan.answer_type,
            query=query,
            metadata=metadata
        )
        
        return generated_answer
    
    def _validate_evidence(self, evidence: List[Evidence]) -> List[Evidence]:
        """
        Validate evidence for quality and consistency.
        
        Args:
            evidence: List of evidence items
            
        Returns:
            Validated evidence list
        """
        if not evidence:
            return []
            
        # Filter out low-confidence evidence
        threshold = 0.3
        filtered_evidence = [e for e in evidence if e.confidence >= threshold]
        
        # If we're being too strict and filtering everything, use the original list
        if not filtered_evidence and evidence:
            filtered_evidence = evidence
            
        return filtered_evidence
    
    def _realize_answer(self, plan: AnswerPlan) -> str:
        """
        Generate the final answer text from the plan.
        
        Args:
            plan: Answer plan
            
        Returns:
            Formatted answer text
        """
        parts = []
        
        # Add opening
        if plan.opening:
            parts.append(plan.opening)
            
        # Add sections
        for section in plan.sections:
            # Add section title if available
            if section.get('title'):
                parts.append(f"\n**{section['title']}**")
                
            # Add evidence
            if section.get('format') == 'list':
                # Format as a list
                parts.append("")
                for evidence in section['evidence']:
                    parts.append(f" {self._format_evidence_text(evidence)}")
            else:
                # Format as paragraphs
                for evidence in section['evidence']:
                    parts.append(f"{self._format_evidence_text(evidence)}")
                    
        # Add conclusion
        if plan.conclusion:
            parts.append(f"\n{plan.conclusion}")
            
        # Combine everything
        answer = "\n\n".join(parts)
        
        return answer
    
    def _format_evidence_text(self, evidence: Evidence) -> str:
        """Format evidence text for inclusion in the answer."""
        # Simple formatting - just return the text
        # In a more advanced system, you might add citation markers or formatting
        return evidence.text
    
    def _calculate_confidence(self, evidence: List[Evidence], plan: AnswerPlan) -> float:
        """
        Calculate overall confidence in the answer.
        
        Args:
            evidence: Validated evidence list
            plan: Answer plan
            
        Returns:
            Confidence score (0-1)
        """
        if not evidence:
            return 0.0
            
        # Base confidence on evidence confidence
        evidence_confidence = sum(e.confidence for e in evidence) / len(evidence)
        
        # Adjust based on contradictions
        contradiction_penalty = 0.0
        for e in evidence:
            if e.contradicting_facts:
                contradiction_penalty += 0.1 * min(len(e.contradicting_facts), 3)
                
        # Adjust based on amount of evidence
        evidence_count_factor = min(len(evidence) / 10, 1.0)  # Max out at 10 evidence items
        
        # Calculate final confidence
        confidence = evidence_confidence * 0.7 + evidence_count_factor * 0.3 - contradiction_penalty
        
        # Cap at 0.95 to acknowledge some uncertainty
        return min(max(confidence, 0.0), 0.95)
    
    def _get_source_documents(self, evidence: List[Evidence]) -> List[str]:
        """
        Get unique list of source documents.
        
        Args:
            evidence: Evidence list
            
        Returns:
            List of source document identifiers
        """
        sources = set()
        for e in evidence:
            sources.add(e.source)
            
        return list(sources)
    
    def generate_answer_with_embedding(self, query: str, search_results: List[SearchResult], 
                                     query_embedding: np.ndarray) -> GeneratedAnswer:
        """
        Generate an answer using query embedding for better selection.
        
        Args:
            query: Original query
            search_results: List of search results
            query_embedding: Query embedding vector
            
        Returns:
            GeneratedAnswer object
        """
        # Extract and rank evidence, using embedding for better matching
        evidence = self._extract_evidence_with_embedding(search_results, query, query_embedding)
        
        # Continue with normal answer generation pipeline
        validated_evidence = self._validate_evidence(evidence)
        answer_plan = self.answer_planner.create_answer_plan(query, validated_evidence)
        answer_text = self._realize_answer(answer_plan)
        confidence = self._calculate_confidence(validated_evidence, answer_plan)
        source_documents = self._get_source_documents(validated_evidence)
        
        metadata = {
            "evidence_count": len(validated_evidence),
            "contradictions_found": any(len(e.contradicting_facts) > 0 for e in validated_evidence),
            "answer_type": answer_plan.answer_type,
            "used_embedding": True
        }
        
        return GeneratedAnswer(
            text=answer_text,
            source_documents=source_documents,
            confidence=confidence,
            supporting_evidence=validated_evidence,
            answer_type=answer_plan.answer_type,
            query=query,
            metadata=metadata
        )
    
    def _extract_evidence_with_embedding(self, search_results: List[SearchResult],
                                       query: str, query_embedding: np.ndarray) -> List[Evidence]:
        """
        Extract evidence using both lexical and semantic similarity.
        
        Args:
            search_results: List of search results
            query: Original query
            query_embedding: Query embedding vector
            
        Returns:
            List of Evidence objects
        """
        evidence_list = []
        
        # First extract evidence normally
        base_evidence = self.evidence_extractor.extract_evidence(search_results, query)
        
        # Then enhance with embedding-based similarity if chunks have embeddings
        for result in search_results:
            if hasattr(result.chunk, 'embedding') and result.chunk.embedding is not None:
                # Calculate cosine similarity with query embedding
                chunk_embedding = result.chunk.embedding
                similarity = self._cosine_similarity(query_embedding, chunk_embedding)
                
                # If high similarity, ensure this chunk is represented in evidence
                if similarity > 0.8:  # High similarity threshold
                    # Check if we already have evidence from this chunk
                    chunk_id = result.chunk.chunk_id
                    has_evidence = any(e.metadata.get('chunk_id') == chunk_id 
                                      for e in base_evidence)
                    
                    if not has_evidence:
                        # Add new evidence from this high-similarity chunk
                        source = self.evidence_extractor._format_source(result)
                        
                        evidence_list.append(Evidence(
                            text=result.chunk.text,
                            source=source,
                            relevance_score=similarity,
                            confidence=similarity * 0.9,  # Slightly reduce confidence
                            metadata={"rank": result.rank, "chunk_id": chunk_id, 
                                      "semantic_similarity": similarity}
                        ))
        
        # Combine base and embedding-enhanced evidence
        combined_evidence = base_evidence + evidence_list
        
        # Sort by relevance score
        combined_evidence.sort(key=lambda e: e.relevance_score, reverse=True)
        
        return combined_evidence
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors."""
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return np.dot(vec1, vec2) / (norm1 * norm2)









Advanced Knowledge Graph with Entity Extraction and Relationship Management


"""
Advanced knowledge graph for maintaining entity relationships and enhancing retrieval quality.
Implements entity extraction, relationship management, and graph-based search capabilities.
"""
import logging
import re
import os
import json
import pickle
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from datetime import datetime
import hashlib
import networkx as nx
import spacy
from collections import defaultdict

# Try to import advanced NER libraries
try:
    import flair
    from flair.data import Sentence
    from flair.models import SequenceTagger
    FLAIR_AVAILABLE = True
except ImportError:
    FLAIR_AVAILABLE = False
    logging.warning("Flair not installed. Advanced NER will not be available.")

# Try to import transformers for zero-shot relationship extraction
try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers not installed. Advanced relationship extraction will not be available.")

logger = logging.getLogger(__name__)

class EntityType:
    """Entity type constants."""
    PERSON = "PERSON"
    ORGANIZATION = "ORG"
    LOCATION = "LOC"
    DATE = "DATE"
    PRODUCT = "PRODUCT"
    EVENT = "EVENT"
    WORK = "WORK"
    TICKET = "TICKET"
    CONCEPT = "CONCEPT"
    TECHNOLOGY = "TECHNOLOGY"
    UNKNOWN = "UNKNOWN"

class RelationType:
    """Relationship type constants."""
    WORKS_FOR = "WORKS_FOR"
    CREATED_BY = "CREATED_BY"
    ASSIGNED_TO = "ASSIGNED_TO"
    PART_OF = "PART_OF"
    RELATED_TO = "RELATED_TO"
    LOCATED_IN = "LOCATED_IN"
    REPORTS_TO = "REPORTS_TO"
    MANAGES = "MANAGES"
    MENTIONS = "MENTIONS"
    AUTHORED = "AUTHORED"
    CONTAINS = "CONTAINS"
    DEPENDS_ON = "DEPENDS_ON"
    USES = "USES"
    SUPPORTS = "SUPPORTS"
    REFERS_TO = "REFERS_TO"
    UNKNOWN = "UNKNOWN"

class Entity:
    """Class representing an entity in the knowledge graph."""
    
    def __init__(self, id: str, name: str, entity_type: str, 
                 metadata: Optional[Dict[str, Any]] = None,
                 aliases: Optional[List[str]] = None,
                 source_documents: Optional[List[str]] = None):
        """
        Initialize an entity.
        
        Args:
            id: Unique entity ID
            name: Entity name
            entity_type: Type of entity (PERSON, ORG, etc.)
            metadata: Additional metadata
            aliases: Alternative names for the entity
            source_documents: Documents where this entity was mentioned
        """
        self.id = id
        self.name = name
        self.entity_type = entity_type
        self.metadata = metadata or {}
        self.aliases = aliases or []
        self.source_documents = source_documents or []
        self.last_updated = datetime.now()
        
    def add_alias(self, alias: str) -> None:
        """Add an alias for the entity."""
        if alias not in self.aliases and alias != self.name:
            self.aliases.append(alias)
            
    def add_source_document(self, document_id: str) -> None:
        """Add a source document reference."""
        if document_id not in self.source_documents:
            self.source_documents.append(document_id)
            
    def update_metadata(self, metadata: Dict[str, Any]) -> None:
        """Update entity metadata."""
        self.metadata.update(metadata)
        self.last_updated = datetime.now()
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "id": self.id,
            "name": self.name,
            "entity_type": self.entity_type,
            "metadata": self.metadata,
            "aliases": self.aliases,
            "source_documents": self.source_documents,
            "last_updated": self.last_updated.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Entity':
        """Create from dictionary representation."""
        entity = cls(
            id=data["id"],
            name=data["name"],
            entity_type=data["entity_type"],
            metadata=data["metadata"],
            aliases=data["aliases"],
            source_documents=data["source_documents"]
        )
        if "last_updated" in data:
            entity.last_updated = datetime.fromisoformat(data["last_updated"])
        return entity
    
    def matches(self, text: str) -> bool:
        """Check if text matches this entity (name or aliases)."""
        if self.name.lower() == text.lower():
            return True
            
        for alias in self.aliases:
            if alias.lower() == text.lower():
                return True
                
        return False
    
    def similarity(self, text: str) -> float:
        """Calculate similarity score between entity and text."""
        # Simple Jaccard similarity for now
        text_tokens = set(text.lower().split())
        name_tokens = set(self.name.lower().split())
        
        # Check name similarity
        if not text_tokens or not name_tokens:
            name_sim = 0.0
        else:
            intersection = text_tokens.intersection(name_tokens)
            union = text_tokens.union(name_tokens)
            name_sim = len(intersection) / len(union)
        
        # Check alias similarity
        alias_sim = 0.0
        for alias in self.aliases:
            alias_tokens = set(alias.lower().split())
            if not alias_tokens:
                continue
                
            intersection = text_tokens.intersection(alias_tokens)
            union = text_tokens.union(alias_tokens)
            current_sim = len(intersection) / len(union)
            alias_sim = max(alias_sim, current_sim)
        
        # Return best match
        return max(name_sim, alias_sim)

class Relationship:
    """Class representing a relationship between entities."""
    
    def __init__(self, id: str, source_id: str, target_id: str, relation_type: str,
                 metadata: Optional[Dict[str, Any]] = None,
                 confidence: float = 1.0,
                 source_documents: Optional[List[str]] = None):
        """
        Initialize a relationship.
        
        Args:
            id: Unique relationship ID
            source_id: Source entity ID
            target_id: Target entity ID
            relation_type: Type of relationship
            metadata: Additional metadata
            confidence: Confidence score (0-1)
            source_documents: Documents where this relationship was extracted
        """
        self.id = id
        self.source_id = source_id
        self.target_id = target_id
        self.relation_type = relation_type
        self.metadata = metadata or {}
        self.confidence = confidence
        self.source_documents = source_documents or []
        self.last_updated = datetime.now()
        
    def add_source_document(self, document_id: str) -> None:
        """Add a source document reference."""
        if document_id not in self.source_documents:
            self.source_documents.append(document_id)
            
    def update_confidence(self, new_confidence: float) -> None:
        """Update confidence score with new evidence."""
        # Weighted average, giving more weight to higher confidence
        if new_confidence > self.confidence:
            self.confidence = (self.confidence + new_confidence * 2) / 3
        else:
            self.confidence = (self.confidence * 2 + new_confidence) / 3
        
        self.last_updated = datetime.now()
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "id": self.id,
            "source_id": self.source_id,
            "target_id": self.target_id,
            "relation_type": self.relation_type,
            "metadata": self.metadata,
            "confidence": self.confidence,
            "source_documents": self.source_documents,
            "last_updated": self.last_updated.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Relationship':
        """Create from dictionary representation."""
        relationship = cls(
            id=data["id"],
            source_id=data["source_id"],
            target_id=data["target_id"],
            relation_type=data["relation_type"],
            metadata=data["metadata"],
            confidence=data["confidence"],
            source_documents=data["source_documents"]
        )
        if "last_updated" in data:
            relationship.last_updated = datetime.fromisoformat(data["last_updated"])
        return relationship

class EntityExtractor:
    """Entity extraction from text using various NLP approaches."""
    
    def __init__(self, use_flair: bool = True, use_custom_rules: bool = True):
        """
        Initialize entity extractor.
        
        Args:
            use_flair: Whether to use Flair for NER (if available)
            use_custom_rules: Whether to use custom rules and patterns
        """
        self.use_flair = use_flair and FLAIR_AVAILABLE
        self.use_custom_rules = use_custom_rules
        
        # Initialize spaCy model
        try:
            # Try to load a larger model first
            self.nlp = spacy.load("en_core_web_md")
        except:
            # Fall back to smaller model
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except:
                logger.error("Could not load spaCy model. Using blank model.")
                self.nlp = spacy.blank("en")
        
        # Initialize Flair NER model
        self.flair_tagger = None
        if self.use_flair:
            try:
                self.flair_tagger = SequenceTagger.load("flair/ner-english-large")
                logger.info("Loaded Flair NER model")
            except Exception as e:
                logger.warning(f"Could not load Flair NER model: {str(e)}")
                self.use_flair = False
        
        # Custom entity patterns
        self.custom_patterns = {
            # Ticket ID patterns
            EntityType.TICKET: [
                r'([A-Z]{2,10}-\d{4,})',  # JIRA-style tickets (PROJ-1234)
                r'(INC\d{6,})',            # Incident IDs
                r'(REQ\d{6,})',            # Request IDs
                r'(TASK\d{6,})'            # Task IDs
            ],
            # Technology patterns
            EntityType.TECHNOLOGY: [
                r'\b(AWS|Azure|GCP)\b',
                r'\b(Docker|Kubernetes|K8s)\b',
                r'\b(Python|Java|JavaScript|TypeScript|React|Angular|Vue)\b',
                r'\b(SQL|NoSQL|MongoDB|PostgreSQL|MySQL|Oracle)\b',
                r'\b(REST|GraphQL|SOAP|API)\b',
                r'\b(CI/CD|Jenkins|GitHub Actions|ArgoCD)\b',
                r'\b(Linux|Windows|MacOS|Unix)\b'
            ],
            # Product patterns
            EntityType.PRODUCT: [
                r'\b(Confluence|Jira|Bitbucket|Bamboo)\b',
                r'\b(Microsoft Office|Word|Excel|PowerPoint|Outlook)\b',
                r'\b(Google Drive|Docs|Sheets|Slides)\b',
                r'\b(Slack|Teams|Zoom|WebEx)\b',
                r'\b(ServiceNow|Remedy|Zendesk)\b'
            ]
        }
    
    def extract_entities(self, text: str) -> List[Tuple[str, str, float]]:
        """
        Extract entities from text.
        
        Args:
            text: Text to analyze
            
        Returns:
            List of (entity_text, entity_type, confidence) tuples
        """
        if not text:
            return []
            
        entities = []
        
        # Use Flair (high accuracy)
        if self.use_flair and self.flair_tagger:
            entities.extend(self._extract_entities_flair(text))
            
        # Use spaCy (good baseline)
        spacy_entities = self._extract_entities_spacy(text)
        
        # Use custom rules (domain-specific)
        if self.use_custom_rules:
            custom_entities = self._extract_entities_custom(text)
            
            # Combine results, with custom rules taking precedence for overlaps
            spacy_custom_combined = []
            
            # Index custom entities for quick lookup
            custom_spans = {}
            for entity_text, entity_type, confidence in custom_entities:
                # Find all occurrences
                for match in re.finditer(re.escape(entity_text), text):
                    span = (match.start(), match.end())
                    custom_spans[span] = (entity_text, entity_type, confidence)
            
            # Add spaCy entities that don't overlap with custom entities
            for entity_text, entity_type, confidence in spacy_entities:
                # Find all occurrences
                add_entity = True
                for match in re.finditer(re.escape(entity_text), text):
                    span = (match.start(), match.end())
                    # Check for overlaps with custom entities
                    for custom_span, custom_entity in custom_spans.items():
                        if (span[0] < custom_span[1] and span[1] > custom_span[0]):
                            # Overlap found, don't add this entity
                            add_entity = False
                            break
                    if not add_entity:
                        break
                        
                if add_entity:
                    spacy_custom_combined.append((entity_text, entity_type, confidence))
                    
            # Add all custom entities
            spacy_custom_combined.extend(custom_entities)
            
            # Combine with Flair results
            flair_spans = {}
            for entity_text, entity_type, confidence in entities:
                # Find all occurrences
                for match in re.finditer(re.escape(entity_text), text):
                    span = (match.start(), match.end())
                    flair_spans[span] = (entity_text, entity_type, confidence)
                    
            # Add spaCy+custom entities that don't overlap with Flair entities
            for entity_text, entity_type, confidence in spacy_custom_combined:
                add_entity = True
                for match in re.finditer(re.escape(entity_text), text):
                    span = (match.start(), match.end())
                    # Check for overlaps with Flair entities
                    for flair_span, flair_entity in flair_spans.items():
                        if (span[0] < flair_span[1] and span[1] > flair_span[0]):
                            # Overlap found, check confidence
                            if confidence > flair_entity[2]:
                                # Higher confidence, replace Flair entity
                                flair_spans[flair_span] = (entity_text, entity_type, confidence)
                            add_entity = False
                            break
                    if not add_entity:
                        break
                        
                if add_entity:
                    entities.append((entity_text, entity_type, confidence))
                    
            # Update entities list with modified Flair entities
            entities = []
            for span, entity in flair_spans.items():
                entities.append(entity)
                
            # Add remaining spaCy+custom entities
            entities.extend(spacy_custom_combined)
            
        else:
            # Simply combine Flair and spaCy results
            entities.extend(spacy_entities)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            key = (entity[0], entity[1])
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
                
        return unique_entities
    
    def _extract_entities_flair(self, text: str) -> List[Tuple[str, str, float]]:
        """Extract entities using Flair NER."""
        entities = []
        try:
            # Create sentence
            sentence = Sentence(text)
            
            # Run NER
            self.flair_tagger.predict(sentence)
            
            # Extract entities
            for entity in sentence.get_spans('ner'):
                # Map Flair entity types to our types
                entity_type = self._map_flair_entity_type(entity.tag)
                
                # Use Flair's score as confidence
                entities.append((entity.text, entity_type, entity.score))
                
            return entities
            
        except Exception as e:
            logger.warning(f"Error extracting entities with Flair: {str(e)}")
            return []
    
    def _map_flair_entity_type(self, flair_type: str) -> str:
        """Map Flair entity types to our entity types."""
        mapping = {
            'PER': EntityType.PERSON,
            'PERSON': EntityType.PERSON,
            'ORG': EntityType.ORGANIZATION,
            'ORGANIZATION': EntityType.ORGANIZATION,
            'LOC': EntityType.LOCATION,
            'LOCATION': EntityType.LOCATION,
            'GPE': EntityType.LOCATION,
            'DATE': EntityType.DATE,
            'TIME': EntityType.DATE,
            'WORK_OF_ART': EntityType.WORK,
            'PRODUCT': EntityType.PRODUCT,
            'EVENT': EntityType.EVENT,
            'MISC': EntityType.CONCEPT
        }
        return mapping.get(flair_type, EntityType.UNKNOWN)
    
    def _extract_entities_spacy(self, text: str) -> List[Tuple[str, str, float]]:
        """Extract entities using spaCy NER."""
        entities = []
        
        try:
            # Process text with spaCy
            doc = self.nlp(text)
            
            # Extract named entities
            for ent in doc.ents:
                # Map spaCy entity types to our types
                entity_type = self._map_spacy_entity_type(ent.label_)
                
                # Add entity
                entities.append((ent.text, entity_type, 0.8))  # Default confidence for spaCy
                
            return entities
            
        except Exception as e:
            logger.warning(f"Error extracting entities with spaCy: {str(e)}")
            return []
    
    def _map_spacy_entity_type(self, spacy_type: str) -> str:
        """Map spaCy entity types to our entity types."""
        mapping = {
            'PERSON': EntityType.PERSON,
            'ORG': EntityType.ORGANIZATION,
            'GPE': EntityType.LOCATION,
            'LOC': EntityType.LOCATION,
            'DATE': EntityType.DATE,
            'TIME': EntityType.DATE,
            'WORK_OF_ART': EntityType.WORK,
            'PRODUCT': EntityType.PRODUCT,
            'EVENT': EntityType.EVENT,
            'NORP': EntityType.ORGANIZATION,
            'FAC': EntityType.LOCATION,
            'LANGUAGE': EntityType.CONCEPT
        }
        return mapping.get(spacy_type, EntityType.UNKNOWN)
    
    def _extract_entities_custom(self, text: str) -> List[Tuple[str, str, float]]:
        """Extract entities using custom patterns."""
        entities = []
        
        # Apply patterns for each entity type
        for entity_type, patterns in self.custom_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    # Use first capturing group if present, otherwise use whole match
                    if match.groups():
                        entity_text = match.group(1)
                    else:
                        entity_text = match.group(0)
                        
                    entities.append((entity_text, entity_type, 0.9))  # High confidence for rule-based
        
        return entities

class RelationshipExtractor:
    """Relationship extraction from text using various NLP approaches."""
    
    def __init__(self, use_transformers: bool = True, use_custom_rules: bool = True):
        """
        Initialize relationship extractor.
        
        Args:
            use_transformers: Whether to use transformers for relation extraction
            use_custom_rules: Whether to use custom rules and patterns
        """
        self.use_transformers = use_transformers and TRANSFORMERS_AVAILABLE
        self.use_custom_rules = use_custom_rules
        
        # Initialize spaCy model (for dependency parsing)
        try:
            self.nlp = spacy.load("en_core_web_md")
        except:
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except:
                logger.error("Could not load spaCy model. Using blank model.")
                self.nlp = spacy.blank("en")
        
        # Initialize zero-shot classifier for relations if transformers available
        self.relation_classifier = None
        if self.use_transformers:
            try:
                self.relation_classifier = pipeline("zero-shot-classification")
                logger.info("Loaded transformers zero-shot classifier")
            except Exception as e:
                logger.warning(f"Could not load transformers zero-shot classifier: {str(e)}")
                self.use_transformers = False
        
        # Define common relation patterns
        self.relation_patterns = {
            # Employment patterns
            RelationType.WORKS_FOR: [
                r'(\w+)\s+(?:works|worked)\s+(?:for|at)\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:employed|employed by|hired by)\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:an employee|a member)\s+of\s+(\w+)',
                r'(\w+)\s+from\s+(\w+)'  # "John from Microsoft"
            ],
            # Creation patterns
            RelationType.CREATED_BY: [
                r'(\w+)\s+(?:created|developed|built|made)\s+by\s+(\w+)',
                r'(\w+)\s+(?:created|developed|built|made)\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:the creator|the author|the developer)\s+of\s+(\w+)'
            ],
            # Assignment patterns
            RelationType.ASSIGNED_TO: [
                r'(\w+)\s+(?:assigned|delegated|allocated)\s+to\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:responsible|accountable)\s+for\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:assigned|tasked)\s+with\s+(\w+)'
            ],
            # Hierarchy patterns
            RelationType.PART_OF: [
                r'(\w+)\s+(?:is|are|was|were)\s+(?:part of|included in|contained in)\s+(\w+)',
                r'(\w+)\s+(?:contains|includes|consists of|comprised of)\s+(\w+)',
                r'(\w+)\s+(?:belongs|belonged)\s+to\s+(\w+)'
            ],
            # Location patterns
            RelationType.LOCATED_IN: [
                r'(\w+)\s+(?:is|are|was|were)\s+(?:located|based|situated)\s+in\s+(\w+)',
                r'(\w+)\s+(?:is|are|was|were)\s+(?:in|at|near)\s+(\w+)'
            ],
            # Reporting patterns
            RelationType.REPORTS_TO: [
                r'(\w+)\s+(?:reports|reported)\s+to\s+(\w+)',
                r'(\w+)\s+(?:is|was)\s+(?:managed by|supervised by|directed by)\s+(\w+)',
                r'(\w+)\s+(?:manages|supervises|directs)\s+(\w+)'  # Inverse relation
            ],
            # Dependency patterns
            RelationType.DEPENDS_ON: [
                r'(\w+)\s+(?:depends|depended|relies|relied)\s+on\s+(\w+)',
                r'(\w+)\s+(?:requires|required|needs|needed)\s+(\w+)',
                r'(\w+)\s+(?:is|are|was|were)\s+(?:dependent|reliant)\s+on\s+(\w+)'
            ],
            # Usage patterns
            RelationType.USES: [
                r'(\w+)\s+(?:uses|used|utilizes|utilized)\s+(\w+)',
                r'(\w+)\s+(?:is|are|was|were)\s+(?:used|utilized)\s+by\s+(\w+)',
                r'(\w+)\s+(?:with|using)\s+(\w+)'
            ]
        }
        
        # Define relation candidates for zero-shot classification
        self.relation_candidates = [
            RelationType.WORKS_FOR,
            RelationType.CREATED_BY,
            RelationType.ASSIGNED_TO,
            RelationType.PART_OF,
            RelationType.RELATED_TO,
            RelationType.LOCATED_IN,
            RelationType.REPORTS_TO,
            RelationType.MANAGES,
            RelationType.AUTHORED,
            RelationType.CONTAINS,
            RelationType.DEPENDS_ON,
            RelationType.USES,
            RelationType.SUPPORTS,
            RelationType.REFERS_TO
        ]
    
    def extract_relationships(self, text: str, entities: List[Tuple[str, str, float]]) -> List[Tuple[str, str, str, float]]:
        """
        Extract relationships between entities in text.
        
        Args:
            text: Text to analyze
            entities: List of (entity_text, entity_type, confidence) tuples
            
        Returns:
            List of (source_entity, relation_type, target_entity, confidence) tuples
        """
        if not text or not entities:
            return []
            
        relationships = []
        
        # Extract entity texts for faster lookup
        entity_texts = [entity[0] for entity in entities]
        entity_types = {entity[0]: entity[1] for entity in entities}
        
        # Rule-based extraction
        if self.use_custom_rules:
            relationships.extend(self._extract_relationships_rules(text, entity_texts, entity_types))
            
        # Dependency-based extraction
        relationships.extend(self._extract_relationships_dependencies(text, entity_texts, entity_types))
        
        # Transformer-based extraction
        if self.use_transformers and self.relation_classifier:
            transformers_relationships = self._extract_relationships_transformers(text, entity_texts, entity_types)
            
            # Combine with existing relationships
            seen = set((s, r, t) for s, r, t, _ in relationships)
            for source, relation, target, confidence in transformers_relationships:
                if (source, relation, target) not in seen:
                    relationships.append((source, relation, target, confidence))
                    seen.add((source, relation, target))
        
        return relationships
    
    def _extract_relationships_rules(self, text: str, entities: List[str], entity_types: Dict[str, str]) -> List[Tuple[str, str, str, float]]:
        """Extract relationships using pattern matching."""
        relationships = []
        
        # Apply patterns for each relation type
        for relation_type, patterns in self.relation_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    if len(match.groups()) >= 2:
                        source = match.group(1)
                        target = match.group(2)
                        
                        # Check if either source or target is in our entities
                        source_match = next((e for e in entities if e.lower() in source.lower() or source.lower() in e.lower()), None)
                        target_match = next((e for e in entities if e.lower() in target.lower() or target.lower() in e.lower()), None)
                        
                        if source_match and target_match:
                            relationships.append((source_match, relation_type, target_match, 0.7))
        
        return relationships
    
    def _extract_relationships_dependencies(self, text: str, entities: List[str], entity_types: Dict[str, str]) -> List[Tuple[str, str, str, float]]:
        """Extract relationships using dependency parsing."""
        relationships = []
        
        try:
            # Process with spaCy
            doc = self.nlp(text)
            
            # Find entities in the dependency graph
            entity_spans = []
            for entity in entities:
                # Find all occurrences of entity in text
                for match in re.finditer(re.escape(entity), text, re.IGNORECASE):
                    start, end = match.span()
                    # Find tokens that overlap with this span
                    entity_tokens = [token for token in doc if token.idx >= start and token.idx < end]
                    if entity_tokens:
                        entity_spans.append((entity, entity_tokens))
            
            # Check for relationships between entity pairs
            for i, (source_entity, source_tokens) in enumerate(entity_spans):
                for j, (target_entity, target_tokens) in enumerate(entity_spans):
                    if i == j:
                        continue
                        
                    # Find the shortest dependency path between entities
                    relation = self._find_dependency_relation(doc, source_tokens, target_tokens)
                    if relation:
                        relationships.append((source_entity, relation, target_entity, 0.6))
            
            return relationships
            
        except Exception as e:
            logger.warning(f"Error extracting relationships with dependencies: {str(e)}")
            return []
    
    def _find_dependency_relation(self, doc, source_tokens, target_tokens) -> Optional[str]:
        """Find relationship from dependency path between tokens."""
        source_roots = set()
        for token in source_tokens:
            # Get the root of this token
            current = token
            while current.dep_ != "ROOT" and current.head != current:
                current = current.head
            source_roots.add(current)
            
        target_roots = set()
        for token in target_tokens:
            # Get the root of this token
            current = token
            while current.dep_ != "ROOT" and current.head != current:
                current = current.head
            target_roots.add(current)
            
        # Look for verbs connecting the entities
        connecting_verbs = []
        for token in doc:
            if token.pos_ == "VERB":
                # Check if this verb connects source and target
                source_connected = any(token in root.subtree for root in source_roots)
                target_connected = any(token in root.subtree for root in target_roots)
                
                if source_connected and target_connected:
                    connecting_verbs.append(token)
        
        if connecting_verbs:
            # Map common verbs to relation types
            verb_mappings = {
                "work": RelationType.WORKS_FOR,
                "employ": RelationType.WORKS_FOR,
                "hire": RelationType.WORKS_FOR,
                "create": RelationType.CREATED_BY,
                "develop": RelationType.CREATED_BY,
                "build": RelationType.CREATED_BY,
                "assign": RelationType.ASSIGNED_TO,
                "delegate": RelationType.ASSIGNED_TO,
                "include": RelationType.PART_OF,
                "contain": RelationType.PART_OF,
                "locate": RelationType.LOCATED_IN,
                "base": RelationType.LOCATED_IN,
                "situate": RelationType.LOCATED_IN,
                "report": RelationType.REPORTS_TO,
                "manage": RelationType.MANAGES,
                "supervise": RelationType.MANAGES,
                "direct": RelationType.MANAGES,
                "depend": RelationType.DEPENDS_ON,
                "rely": RelationType.DEPENDS_ON,
                "require": RelationType.DEPENDS_ON,
                "use": RelationType.USES,
                "utilize": RelationType.USES,
                "support": RelationType.SUPPORTS,
                "refer": RelationType.REFERS_TO,
                "mention": RelationType.MENTIONS
            }
            
            for verb in connecting_verbs:
                # Check if verb or lemma maps to a relation type
                relation = verb_mappings.get(verb.text.lower()) or verb_mappings.get(verb.lemma_.lower())
                if relation:
                    return relation
            
            # Default to most common verb as relation
            return RelationType.RELATED_TO
        
        return None
    
    def _extract_relationships_transformers(self, text: str, entities: List[str], entity_types: Dict[str, str]) -> List[Tuple[str, str, str, float]]:
        """Extract relationships using transformer-based zero-shot classification."""
        relationships = []
        
        # Create entity pairs
        entity_pairs = []
        for i, source in enumerate(entities):
            for j, target in enumerate(entities):
                if i != j:
                    # Check for source-target occurrence in text
                    source_pos = [m.start() for m in re.finditer(re.escape(source), text)]
                    target_pos = [m.start() for m in re.finditer(re.escape(target), text)]
                    
                    if not source_pos or not target_pos:
                        continue
                        
                    # Find closest occurrence pair
                    min_distance = float('inf')
                    closest_pair = None
                    
                    for s_pos in source_pos:
                        for t_pos in target_pos:
                            distance = abs(s_pos - t_pos)
                            if distance < min_distance:
                                min_distance = distance
                                closest_pair = (s_pos, t_pos)
                    
                    if closest_pair and min_distance < 100:  # Only consider if within 100 chars
                        s_pos, t_pos = closest_pair
                        
                        # Extract surrounding context
                        context_start = max(0, min(s_pos, t_pos) - 50)
                        context_end = min(len(text), max(s_pos + len(source), t_pos + len(target)) + 50)
                        context = text[context_start:context_end]
                        
                        entity_pairs.append((source, target, context))
        
        # For each entity pair, classify the relationship
        for source, target, context in entity_pairs:
            # Create a hypothesis
            hypothesis = f"{source} [RELATION] {target}"
            
            # Classify with zero-shot
            try:
                result = self.relation_classifier(
                    context,
                    self.relation_candidates,
                    hypothesis_template="The relation between {}"
                )
                
                # Get top relation
                relation = result["labels"][0]
                confidence = result["scores"][0]
                
                relationships.append((source, relation, target, confidence))
                
            except Exception as e:
                logger.warning(f"Error classifying relationship with transformers: {str(e)}")
        
        return relationships

class KnowledgeGraph:
    """Advanced knowledge graph for entity relationships."""
    
    def __init__(self, graph_path: Optional[str] = None):
        """
        Initialize knowledge graph.
        
        Args:
            graph_path: Path to save/load the knowledge graph
        """
        self.graph_path = graph_path
        self.entities = {}
        self.relationships = {}
        self.nx_graph = nx.DiGraph()
        
        # Initialize extractors
        self.entity_extractor = EntityExtractor()
        self.relationship_extractor = RelationshipExtractor()
        
        # Document to entities mapping
        self.document_entities = defaultdict(set)
        
        # Load existing graph if path provided
        if graph_path and os.path.exists(graph_path):
            self.load()
    
    def get_entity_id(self, entity_name: str, entity_type: str) -> str:
        """Generate a stable entity ID."""
        normalized_name = entity_name.lower().strip()
        return hashlib.md5(f"{normalized_name}_{entity_type}".encode()).hexdigest()
    
    def get_relationship_id(self, source_id: str, target_id: str, relation_type: str) -> str:
        """Generate a stable relationship ID."""
        return hashlib.md5(f"{source_id}_{relation_type}_{target_id}".encode()).hexdigest()
    
    def add_entity(self, name: str, entity_type: str, 
                  metadata: Optional[Dict[str, Any]] = None,
                  aliases: Optional[List[str]] = None,
                  source_document: Optional[str] = None) -> Entity:
        """
        Add an entity to the knowledge graph.
        
        Args:
            name: Entity name
            entity_type: Type of entity
            metadata: Additional metadata
            aliases: Alternative names
            source_document: Source document ID
            
        Returns:
            The added or updated Entity
        """
        entity_id = self.get_entity_id(name, entity_type)
        
        if entity_id in self.entities:
            # Update existing entity
            entity = self.entities[entity_id]
            
            if metadata:
                entity.update_metadata(metadata)
                
            if aliases:
                for alias in aliases:
                    entity.add_alias(alias)
                    
            if source_document:
                entity.add_source_document(source_document)
                
                # Update document to entity mapping
                self.document_entities[source_document].add(entity_id)
        else:
            # Create new entity
            entity = Entity(
                id=entity_id,
                name=name,
                entity_type=entity_type,
                metadata=metadata,
                aliases=aliases,
                source_documents=[source_document] if source_document else []
            )
            
            self.entities[entity_id] = entity
            
            # Update document to entity mapping
            if source_document:
                self.document_entities[source_document].add(entity_id)
                
            # Add to NetworkX graph
            self.nx_graph.add_node(
                entity_id,
                name=name,
                entity_type=entity_type,
                metadata=metadata or {}
            )
        
        return entity
    
    def add_relationship(self, source_entity: Union[str, Entity], 
                        relation_type: str,
                        target_entity: Union[str, Entity],
                        metadata: Optional[Dict[str, Any]] = None,
                        confidence: float = 1.0,
                        source_document: Optional[str] = None) -> Relationship:
        """
        Add a relationship between entities.
        
        Args:
            source_entity: Source entity or entity ID
            relation_type: Type of relationship
            target_entity: Target entity or entity ID
            metadata: Additional metadata
            confidence: Confidence score (0-1)
            source_document: Source document ID
            
        Returns:
            The added or updated Relationship
        """
        # Get entity IDs
        source_id = source_entity.id if isinstance(source_entity, Entity) else source_entity
        target_id = target_entity.id if isinstance(target_entity, Entity) else target_entity
        
        # Ensure both entities exist
        if source_id not in self.entities or target_id not in self.entities:
            logger.warning(f"Cannot add relationship: entities not found ({source_id}, {target_id})")
            return None
            
        relationship_id = self.get_relationship_id(source_id, target_id, relation_type)
        
        if relationship_id in self.relationships:
            # Update existing relationship
            relationship = self.relationships[relationship_id]
            
            # Update confidence with new evidence
            relationship.update_confidence(confidence)
            
            if metadata:
                relationship.metadata.update(metadata)
                
            if source_document:
                relationship.add_source_document(source_document)
        else:
            # Create new relationship
            relationship = Relationship(
                id=relationship_id,
                source_id=source_id,
                target_id=target_id,
                relation_type=relation_type,
                metadata=metadata,
                confidence=confidence,
                source_documents=[source_document] if source_document else []
            )
            
            self.relationships[relationship_id] = relationship
            
            # Add edge to NetworkX graph
            self.nx_graph.add_edge(
                source_id,
                target_id,
                relation_type=relation_type,
                confidence=confidence,
                metadata=metadata or {}
            )
        
        return relationship
    
    def process_text(self, text: str, document_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Process text to extract entities and relationships.
        
        Args:
            text: Text to process
            document_id: Document identifier
            
        Returns:
            Dictionary with extracted entities and relationships
        """
        if not text:
            return {"entities": [], "relationships": []}
            
        # Extract entities
        extracted_entities = self.entity_extractor.extract_entities(text)
        
        # Add entities to graph
        added_entities = []
        for entity_text, entity_type, confidence in extracted_entities:
            entity = self.add_entity(
                name=entity_text,
                entity_type=entity_type,
                metadata={"confidence": confidence},
                source_document=document_id
            )
            added_entities.append((entity, entity_text, entity_type, confidence))
        
        # Extract relationships
        extracted_relationships = self.relationship_extractor.extract_relationships(
            text, 
            [(entity_text, entity_type, confidence) for _, entity_text, entity_type, confidence in added_entities]
        )
        
        # Add relationships to graph
        added_relationships = []
        for source_text, relation_type, target_text, confidence in extracted_relationships:
            # Find corresponding entities
            source_entity = next((entity for entity, text, _, _ in added_entities if text == source_text), None)
            target_entity = next((entity for entity, text, _, _ in added_entities if text == target_text), None)
            
            if source_entity and target_entity:
                relationship = self.add_relationship(
                    source_entity=source_entity.id,
                    relation_type=relation_type,
                    target_entity=target_entity.id,
                    confidence=confidence,
                    source_document=document_id
                )
                
                if relationship:
                    added_relationships.append((relationship, source_text, relation_type, target_text, confidence))
        
        return {
            "entities": [(entity.id, entity_text, entity_type, confidence) for entity, entity_text, entity_type, confidence in added_entities],
            "relationships": [(rel.id, source_text, relation_type, target_text, confidence) for rel, source_text, relation_type, target_text, confidence in added_relationships]
        }
    
    def get_entity(self, entity_id: str) -> Optional[Entity]:
        """Get entity by ID."""
        return self.entities.get(entity_id)
    
    def get_relationship(self, relationship_id: str) -> Optional[Relationship]:
        """Get relationship by ID."""
        return self.relationships.get(relationship_id)
    
    def find_entities(self, query: str, entity_type: Optional[str] = None, 
                     threshold: float = 0.6) -> List[Entity]:
        """
        Find entities by name or similarity.
        
        Args:
            query: Search query
            entity_type: Optional entity type filter
            threshold: Similarity threshold
            
        Returns:
            List of matching entities
        """
        matches = []
        
        for entity in self.entities.values():
            # Filter by entity type if specified
            if entity_type and entity.entity_type != entity_type:
                continue
                
            # Exact match check
            if entity.matches(query):
                matches.append((entity, 1.0))
                continue
                
            # Similarity check
            similarity = entity.similarity(query)
            if similarity >= threshold:
                matches.append((entity, similarity))
        
        # Sort by similarity
        matches.sort(key=lambda x: x[1], reverse=True)
        
        return [entity for entity, _ in matches]
    
    def get_entity_relationships(self, entity_id: str) -> Dict[str, List[Relationship]]:
        """
        Get all relationships for an entity.
        
        Args:
            entity_id: Entity ID
            
        Returns:
            Dictionary with incoming and outgoing relationships
        """
        if entity_id not in self.entities:
            return {"incoming": [], "outgoing": []}
            
        outgoing = []
        incoming = []
        
        # Check all relationships
        for relationship in self.relationships.values():
            if relationship.source_id == entity_id:
                outgoing.append(relationship)
            elif relationship.target_id == entity_id:
                incoming.append(relationship)
        
        return {
            "incoming": incoming,
            "outgoing": outgoing
        }
    
    def get_entity_neighbors(self, entity_id: str, relation_types: Optional[List[str]] = None,
                            max_hops: int = 1) -> List[Dict[str, Any]]:
        """
        Get neighbor entities within N hops.
        
        Args:
            entity_id: Central entity ID
            relation_types: Optional filter for relation types
            max_hops: Maximum number of hops
            
        Returns:
            List of neighbor entities with relationship info
        """
        if entity_id not in self.entities:
            return []
            
        neighbors = []
        visited = {entity_id}  # Skip central entity
        queue = [(entity_id, 0, None, None)]  # (id, hops, relationship, direction)
        
        while queue:
            current_id, hops, relationship, direction = queue.pop(0)
            
            # If past first entity and within hop limit
            if hops > 0 and hops <= max_hops:
                entity = self.entities[current_id]
                neighbors.append({
                    "entity": entity,
                    "hops": hops,
                    "relationship": relationship,
                    "direction": direction
                })
            
            # If at max hop limit, don't explore further
            if hops >= max_hops:
                continue
                
            # Explore neighbors
            relationships = self.get_entity_relationships(current_id)
            
            # Add outgoing neighbors
            for rel in relationships["outgoing"]:
                # Filter by relation type if specified
                if relation_types and rel.relation_type not in relation_types:
                    continue
                    
                if rel.target_id not in visited:
                    visited.add(rel.target_id)
                    queue.append((rel.target_id, hops + 1, rel, "outgoing"))
            
            # Add incoming neighbors
            for rel in relationships["incoming"]:
                # Filter by relation type if specified
                if relation_types and rel.relation_type not in relation_types:
                    continue
                    
                if rel.source_id not in visited:
                    visited.add(rel.source_id)
                    queue.append((rel.source_id, hops + 1, rel, "incoming"))
        
        return neighbors
    
    def get_shortest_path(self, source_id: str, target_id: str) -> Optional[List[Dict[str, Any]]]:
        """
        Find shortest path between two entities.
        
        Args:
            source_id: Source entity ID
            target_id: Target entity ID
            
        Returns:
            List of path elements (entity and relationship) or None if no path
        """
        if source_id not in self.entities or target_id not in self.entities:
            return None
            
        try:
            # Use NetworkX to find shortest path
            path_nodes = nx.shortest_path(self.nx_graph, source_id, target_id)
            
            # Convert to path with entities and relationships
            result = []
            
            for i in range(len(path_nodes)):
                # Add entity
                entity_id = path_nodes[i]
                entity = self.entities[entity_id]
                result.append({"type": "entity", "entity": entity})
                
                # Add relationship if not at end
                if i < len(path_nodes) - 1:
                    next_id = path_nodes[i + 1]
                    
                    # Get relationship
                    for rel in self.relationships.values():
                        if rel.source_id == entity_id and rel.target_id == next_id:
                            result.append({"type": "relationship", "relationship": rel})
                            break
            
            return result
            
        except (nx.NetworkXNoPath, nx.NodeNotFound):
            return None
    
    def get_document_entities(self, document_id: str) -> List[Entity]:
        """
        Get all entities for a document.
        
        Args:
            document_id: Document ID
            
        Returns:
            List of entities
        """
        entity_ids = self.document_entities.get(document_id, set())
        return [self.entities[entity_id] for entity_id in entity_ids if entity_id in self.entities]
    
    def get_knowledge_card(self, entity_id: str) -> Dict[str, Any]:
        """
        Generate a knowledge card for an entity.
        
        Args:
            entity_id: Entity ID
            
        Returns:
            Dictionary with entity information
        """
        if entity_id not in self.entities:
            return {"error": "Entity not found"}
            
        entity = self.entities[entity_id]
        
        # Get relationships
        relationships = self.get_entity_relationships(entity_id)
        
        # Format relationship data
        outgoing_formatted = []
        for rel in relationships["outgoing"]:
            if rel.target_id in self.entities:
                target = self.entities[rel.target_id]
                outgoing_formatted.append({
                    "relation": rel.relation_type,
                    "entity": {
                        "id": target.id,
                        "name": target.name,
                        "type": target.entity_type
                    },
                    "confidence": rel.confidence
                })
                
        incoming_formatted = []
        for rel in relationships["incoming"]:
            if rel.source_id in self.entities:
                source = self.entities[rel.source_id]
                incoming_formatted.append({
                    "relation": rel.relation_type,
                    "entity": {
                        "id": source.id,
                        "name": source.name,
                        "type": source.entity_type
                    },
                    "confidence": rel.confidence
                })
        
        # Get documents where entity appears
        documents = entity.source_documents
        
        # Create knowledge card
        knowledge_card = {
            "entity": {
                "id": entity.id,
                "name": entity.name,
                "type": entity.entity_type,
                "aliases": entity.aliases,
                "metadata": entity.metadata
            },
            "relationships": {
                "outgoing": outgoing_formatted,
                "incoming": incoming_formatted
            },
            "documents": documents,
            "updated_at": entity.last_updated.isoformat()
        }
        
        return knowledge_card
    
    def search_entities_by_type(self, entity_type: str, limit: int = 100) -> List[Entity]:
        """
        Search for entities by type.
        
        Args:
            entity_type: Entity type to search for
            limit: Maximum number of results
            
        Returns:
            List of matching entities
        """
        results = [entity for entity in self.entities.values() 
                  if entity.entity_type == entity_type]
                  
        # Sort by recency
        results.sort(key=lambda x: x.last_updated, reverse=True)
        
        return results[:limit]
    
    def search_entities_by_document(self, document_id: str, 
                                   entity_type: Optional[str] = None) -> List[Entity]:
        """
        Search for entities mentioned in a document.
        
        Args:
            document_id: Document ID
            entity_type: Optional entity type filter
            
        Returns:
            List of matching entities
        """
        entity_ids = self.document_entities.get(document_id, set())
        
        results = []
        for entity_id in entity_ids:
            if entity_id in self.entities:
                entity = self.entities[entity_id]
                if not entity_type or entity.entity_type == entity_type:
                    results.append(entity)
                    
        return results
    
    def get_related_documents(self, entity_id: str, max_distance: int = 2) -> List[str]:
        """
        Find documents related to an entity by graph distance.
        
        Args:
            entity_id: Entity ID
            max_distance: Maximum graph distance
            
        Returns:
            List of document IDs
        """
        if entity_id not in self.entities:
            return []
            
        # Start with direct document mentions
        documents = set(self.entities[entity_id].source_documents)
        
        # Get neighboring entities up to max_distance
        neighbors = self.get_entity_neighbors(entity_id, max_hops=max_distance)
        
        # Add documents from neighbors
        for neighbor in neighbors:
            entity = neighbor["entity"]
            documents.update(entity.source_documents)
            
        return list(documents)
    
    def run_graph_query(self, query_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run a structured query against the knowledge graph.
        
        Args:
            query_type: Type of query (entity, path, neighborhood, etc.)
            params: Query parameters
            
        Returns:
            Query results
        """
        if query_type == "entity":
            # Get entity by ID or name
            if "id" in params:
                entity = self.get_entity(params["id"])
                if entity:
                    return self.get_knowledge_card(entity.id)
                return {"error": "Entity not found"}
                
            elif "name" in params:
                entities = self.find_entities(
                    params["name"],
                    entity_type=params.get("type"),
                    threshold=params.get("threshold", 0.6)
                )
                
                if entities:
                    return self.get_knowledge_card(entities[0].id)
                return {"error": "Entity not found"}
                
        elif query_type == "path":
            # Find path between entities
            source_id = params.get("source_id")
            target_id = params.get("target_id")
            
            if not source_id or not target_id:
                return {"error": "Source and target IDs required"}
                
            path = self.get_shortest_path(source_id, target_id)
            if path:
                return {"path": [{"type": item["type"], "id": item[item["type"]].id} for item in path]}
            return {"error": "No path found"}
            
        elif query_type == "neighborhood":
            # Get entity neighborhood
            entity_id = params.get("entity_id")
            if not entity_id:
                return {"error": "Entity ID required"}
                
            neighbors = self.get_entity_neighbors(
                entity_id,
                relation_types=params.get("relation_types"),
                max_hops=params.get("max_hops", 1)
            )
            
            return {
                "entity_id": entity_id,
                "neighbors": [
                    {
                        "entity_id": n["entity"].id,
                        "entity_name": n["entity"].name,
                        "entity_type": n["entity"].entity_type,
                        "hops": n["hops"],
                        "relationship": n["relationship"].relation_type if n["relationship"] else None,
                        "direction": n["direction"]
                    }
                    for n in neighbors
                ]
            }
            
        elif query_type == "document_entities":
            # Get entities for a document
            document_id = params.get("document_id")
            if not document_id:
                return {"error": "Document ID required"}
                
            entities = self.get_document_entities(document_id)
            return {
                "document_id": document_id,
                "entities": [
                    {
                        "id": entity.id,
                        "name": entity.name,
                        "type": entity.entity_type
                    }
                    for entity in entities
                ]
            }
            
        elif query_type == "related_documents":
            # Get documents related to an entity
            entity_id = params.get("entity_id")
            if not entity_id:
                return {"error": "Entity ID required"}
                
            documents = self.get_related_documents(
                entity_id,
                max_distance=params.get("max_distance", 2)
            )
            
            return {
                "entity_id": entity_id,
                "documents": documents
            }
            
        else:
            return {"error": f"Unknown query type: {query_type}"}
    
    def save(self, path: Optional[str] = None) -> None:
        """
        Save knowledge graph to disk.
        
        Args:
            path: Path to save to (defaults to initialized path)
        """
        save_path = path or self.graph_path
        if not save_path:
            logger.warning("No path specified for saving knowledge graph")
            return
            
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        try:
            data = {
                "entities": {entity_id: entity.to_dict() for entity_id, entity in self.entities.items()},
                "relationships": {rel_id: rel.to_dict() for rel_id, rel in self.relationships.items()},
                "document_entities": {doc_id: list(entity_ids) for doc_id, entity_ids in self.document_entities.items()}
            }
            
            with open(save_path, "wb") as f:
                pickle.dump(data, f)
                
            logger.info(f"Saved knowledge graph to {save_path} with {len(self.entities)} entities and {len(self.relationships)} relationships")
            
        except Exception as e:
            logger.error(f"Error saving knowledge graph: {str(e)}")
    
    def load(self, path: Optional[str] = None) -> None:
        """
        Load knowledge graph from disk.
        
        Args:
            path: Path to load from (defaults to initialized path)
        """
        load_path = path or self.graph_path
        if not load_path:
            logger.warning("No path specified for loading knowledge graph")
            return
            
        if not os.path.exists(load_path):
            logger.warning(f"Knowledge graph file not found: {load_path}")
            return
            
        try:
            with open(load_path, "rb") as f:
                data = pickle.load(f)
                
            # Load entities
            self.entities = {}
            for entity_id, entity_data in data["entities"].items():
                self.entities[entity_id] = Entity.from_dict(entity_data)
                
            # Load relationships
            self.relationships = {}
            for rel_id, rel_data in data["relationships"].items():
                self.relationships[rel_id] = Relationship.from_dict(rel_data)
                
            # Load document to entities mapping
            self.document_entities = defaultdict(set)
            for doc_id, entity_ids in data["document_entities"].items():
                self.document_entities[doc_id] = set(entity_ids)
                
            # Rebuild NetworkX graph
            self.nx_graph = nx.DiGraph()
            
            # Add nodes
            for entity_id, entity in self.entities.items():
                self.nx_graph.add_node(
                    entity_id,
                    name=entity.name,
                    entity_type=entity.entity_type,
                    metadata=entity.metadata
                )
                
            # Add edges
            for rel_id, rel in self.relationships.items():
                self.nx_graph.add_edge(
                    rel.source_id,
                    rel.target_id,
                    relation_type=rel.relation_type,
                    confidence=rel.confidence,
                    metadata=rel.metadata
                )
                
            logger.info(f"Loaded knowledge graph from {load_path} with {len(self.entities)} entities and {len(self.relationships)} relationships")
            
        except Exception as e:
            logger.error(f"Error loading knowledge graph: {str(e)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the knowledge graph.
        
        Returns:
            Dictionary with statistics
        """
        # Count entities by type
        entity_types = {}
        for entity in self.entities.values():
            if entity.entity_type not in entity_types:
                entity_types[entity.entity_type] = 0
            entity_types[entity.entity_type] += 1
            
        # Count relationships by type
        relation_types = {}
        for rel in self.relationships.values():
            if rel.relation_type not in relation_types:
                relation_types[rel.relation_type] = 0
            relation_types[rel.relation_type] += 1
            
        # Get graph metrics
        try:
            # Skip if graph is empty
            if self.nx_graph.number_of_nodes() == 0:
                centrality = {}
            else:
                # Calculate degree centrality (most connected nodes)
                centrality = nx.degree_centrality(self.nx_graph)
                
            # Get top 10 central entities
            top_central = sorted(
                [(entity_id, score) for entity_id, score in centrality.items()],
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            central_entities = [
                {
                    "id": entity_id,
                    "name": self.entities[entity_id].name,
                    "type": self.entities[entity_id].entity_type,
                    "centrality": score
                }
                for entity_id, score in top_central
                if entity_id in self.entities
            ]
            
        except Exception as e:
            logger.warning(f"Error calculating graph metrics: {str(e)}")
            central_entities = []
        
        return {
            "entity_count": len(self.entities),
            "relationship_count": len(self.relationships),
            "document_count": len(self.document_entities),
            "entity_types": entity_types,
            "relation_types": relation_types,
            "central_entities": central_entities
        }










Source Controller for Configurable Knowledge Source Selection

"""
Source Controller for managing knowledge source selection and query routing.
Enables users to select which knowledge sources to query for each request.
"""
import logging
import re
import json
from enum import Enum
from typing import Dict, Any, List, Optional, Tuple, Union, Set

from connectors.confluence import ConfluenceConnector
from connectors.remedy import RemedyConnector
from processors.document_processor import Document, DocumentContent, ContentType
from config.config import config

logger = logging.getLogger(__name__)

class KnowledgeSource(Enum):
    """Knowledge source types"""
    CONFLUENCE = "confluence"
    REMEDY = "remedy"
    ALL = "all"  # Special value for querying all sources

class SourcePriority(Enum):
    """Priority levels for knowledge sources"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class QueryIntent(Enum):
    """Query intent types to guide source selection"""
    DOCUMENT = "document"  # Looking for documentation
    TICKET = "ticket"      # Looking for ticket information
    GENERAL = "general"    # General query without specific source intent
    TECHNICAL = "technical"  # Technical information
    PROCESS = "process"    # Process or workflow information
    STATUS = "status"      # Status information (tickets, projects)
    PEOPLE = "people"      # Information about people or teams

class SourceController:
    """
    Controls access to knowledge sources based on user preferences and query analysis.
    """
    
    def __init__(self):
        """Initialize the source controller."""
        # Initialize connectors
        self.connectors = {}
        self._initialize_connectors()
        
        # Default configuration
        self.default_sources = [KnowledgeSource.CONFLUENCE, KnowledgeSource.REMEDY]
        self.default_priorities = {
            KnowledgeSource.CONFLUENCE: SourcePriority.MEDIUM,
            KnowledgeSource.REMEDY: SourcePriority.MEDIUM
        }
        
        # Intent-based routing configuration
        self.intent_routing = {
            QueryIntent.DOCUMENT: {
                KnowledgeSource.CONFLUENCE: SourcePriority.HIGH,
                KnowledgeSource.REMEDY: SourcePriority.LOW
            },
            QueryIntent.TICKET: {
                KnowledgeSource.CONFLUENCE: SourcePriority.LOW,
                KnowledgeSource.REMEDY: SourcePriority.HIGH
            },
            QueryIntent.TECHNICAL: {
                KnowledgeSource.CONFLUENCE: SourcePriority.HIGH,
                KnowledgeSource.REMEDY: SourcePriority.MEDIUM
            },
            QueryIntent.PROCESS: {
                KnowledgeSource.CONFLUENCE: SourcePriority.HIGH,
                KnowledgeSource.REMEDY: SourcePriority.MEDIUM
            },
            QueryIntent.STATUS: {
                KnowledgeSource.CONFLUENCE: SourcePriority.LOW,
                KnowledgeSource.REMEDY: SourcePriority.HIGH
            },
            QueryIntent.PEOPLE: {
                KnowledgeSource.CONFLUENCE: SourcePriority.MEDIUM,
                KnowledgeSource.REMEDY: SourcePriority.MEDIUM
            },
            QueryIntent.GENERAL: self.default_priorities
        }
        
        # Regex patterns for query matching
        self.source_patterns = {
            KnowledgeSource.CONFLUENCE: [
                r'\b(?:confluence|wiki|documentation|docs|guide|manual|kb)\b',
                r'\b(?:how\s+to|what\s+is|explain)\b',
                r'\b(?:information|details)\s+(?:about|on)\b'
            ],
            KnowledgeSource.REMEDY: [
                r'\b(?:remedy|ticket|incident|change|request|task|problem)\b',
                r'\b(?:INC|CHG|REQ|TASK)\d{5,}\b',  # Ticket IDs
                r'\b(?:status|update|priority|assigned)\b'
            ]
        }
        
        # Intent detection patterns
        self.intent_patterns = {
            QueryIntent.DOCUMENT: [
                r'\b(?:documentation|guide|manual|wiki|article|page|how\s+to)\b',
                r'\b(?:procedure|steps|instructions|tutorial)\b',
                r'\b(?:confluence)\b'
            ],
            QueryIntent.TICKET: [
                r'\b(?:ticket|incident|change|problem|task|request)\b',
                r'\b(?:INC|CHG|REQ|TASK)\d{5,}\b',  # Ticket IDs
                r'\b(?:status|update|priority|assigned|resolver)\b',
                r'\b(?:remedy)\b'
            ],
            QueryIntent.TECHNICAL: [
                r'\b(?:system|server|database|network|application|software|code|api|service)\b',
                r'\b(?:error|exception|bug|issue|failed|crash)\b',
                r'\b(?:configuration|setup|install|deploy|implement)\b'
            ],
            QueryIntent.PROCESS: [
                r'\b(?:process|workflow|procedure|policy|standard|guideline)\b',
                r'\b(?:approval|sign-off|review|validate|authorize)\b',
                r'\b(?:steps|stages|phases)\b'
            ],
            QueryIntent.STATUS: [
                r'\b(?:status|progress|update|current state|tracking)\b',
                r'\b(?:complete|pending|open|closed|resolved|active)\b',
                r'\b(?:deadline|ETA|timeline|schedule|due date)\b'
            ],
            QueryIntent.PEOPLE: [
                r'\b(?:team|group|department|person|employee|manager|lead)\b',
                r'\b(?:responsible|owner|assigned|contact)\b',
                r'\b(?:who\s+is|who\s+are|who\s+should)\b'
            ]
        }
        
        # Source context keywords for disambiguation
        self.source_context = {
            KnowledgeSource.CONFLUENCE: [
                "documentation", "article", "page", "wiki", "guide", 
                "information", "knowledge base", "procedures", "standards",
                "technical documentation", "guidelines", "user manual"
            ],
            KnowledgeSource.REMEDY: [
                "ticket", "incident", "change request", "problem ticket", 
                "service request", "task", "assignment", "record",
                "ticket status", "support case", "open requests", "worklog"
            ]
        }
    
    def _initialize_connectors(self):
        """Initialize the knowledge source connectors."""
        # Confluence
        confluence_config = config.get("confluence", {})
        if confluence_config.get("url"):
            try:
                self.connectors[KnowledgeSource.CONFLUENCE] = ConfluenceConnector(
                    base_url=confluence_config.get("url"),
                    username=confluence_config.get("username"),
                    password=confluence_config.get("password"),
                    api_token=confluence_config.get("api_token"),
                    space_key=confluence_config.get("space_key")
                )
                logger.info("Initialized Confluence connector")
            except Exception as e:
                logger.error(f"Failed to initialize Confluence connector: {str(e)}")
        
        # Remedy
        remedy_config = config.get("remedy", {})
        if remedy_config.get("url"):
            try:
                self.connectors[KnowledgeSource.REMEDY] = RemedyConnector(
                    base_url=remedy_config.get("url"),
                    username=remedy_config.get("username"),
                    password=remedy_config.get("password"),
                    api_token=remedy_config.get("api_token"),
                    server_name=remedy_config.get("server_name")
                )
                logger.info("Initialized Remedy connector")
            except Exception as e:
                logger.error(f"Failed to initialize Remedy connector: {str(e)}")
    
    def get_available_sources(self) -> List[KnowledgeSource]:
        """
        Get list of available knowledge sources.
        
        Returns:
            List of available knowledge sources
        """
        return list(self.connectors.keys()) + [KnowledgeSource.ALL]
    
    def check_source_availability(self) -> Dict[KnowledgeSource, bool]:
        """
        Check availability of each knowledge source.
        
        Returns:
            Dictionary mapping sources to availability status
        """
        results = {}
        
        for source, connector in self.connectors.items():
            try:
                results[source] = connector.check_connection()
            except Exception as e:
                logger.error(f"Error checking {source.value} availability: {str(e)}")
                results[source] = False
        
        # ALL is available if at least one source is available
        results[KnowledgeSource.ALL] = any(results.values())
        
        return results
    
    def detect_query_intent(self, query: str) -> QueryIntent:
        """
        Detect the intent of a query to guide source selection.
        
        Args:
            query: The query text
            
        Returns:
            Detected query intent
        """
        # Normalize query
        query = query.lower()
        
        # Match against intent patterns
        intent_matches = {}
        for intent, patterns in self.intent_patterns.items():
            matches = 0
            for pattern in patterns:
                if re.search(pattern, query):
                    matches += 1
            intent_matches[intent] = matches
        
        # Get intent with most matches (or GENERAL if tie or no matches)
        max_matches = max(intent_matches.values()) if intent_matches else 0
        if max_matches > 0:
            max_intents = [intent for intent, matches in intent_matches.items() 
                          if matches == max_matches]
            if len(max_intents) == 1:
                return max_intents[0]
        
        # Default to general
        return QueryIntent.GENERAL
    
    def detect_source_from_query(self, query: str) -> Optional[KnowledgeSource]:
        """
        Detect if a query explicitly mentions a knowledge source.
        
        Args:
            query: The query text
            
        Returns:
            Detected knowledge source or None if no clear source is mentioned
        """
        # Special case for "all sources"
        if "all sources" in query.lower() or "both sources" in query.lower():
            return KnowledgeSource.ALL
            
        # Normalize query
        query = query.lower()
        
        # Check for explicit source indicators like "in confluence" or "from remedy"
        if re.search(r'\b(?:in|from|on|using|with|through|via)\s+confluence\b', query):
            return KnowledgeSource.CONFLUENCE
        
        if re.search(r'\b(?:in|from|on|using|with|through|via)\s+remedy\b', query):
            return KnowledgeSource.REMEDY
        
        # Check for source-specific patterns
        source_matches = {}
        for source, patterns in self.source_patterns.items():
            matches = 0
            for pattern in patterns:
                if re.search(pattern, query):
                    matches += 1
            source_matches[source] = matches
        
        # Get source with most matches
        max_matches = max(source_matches.values()) if source_matches else 0
        if max_matches > 0:
            max_sources = [source for source, matches in source_matches.items() 
                          if matches == max_matches]
            if len(max_sources) == 1:
                return max_sources[0]
        
        # No clear source detected
        return None
    
    def get_sources_for_query(self, query: str, 
                            user_preference: Optional[KnowledgeSource] = None) -> List[Dict[str, Any]]:
        """
        Determine which sources to query based on query analysis and user preference.
        
        Args:
            query: The query text
            user_preference: Optional user-specified source preference
            
        Returns:
            List of dictionaries with source and priority
        """
        sources_to_query = []
        
        # Check source availability
        available_sources = self.check_source_availability()
        
        # Handle user preference if specified
        if user_preference:
            if user_preference == KnowledgeSource.ALL:
                # Query all available sources
                for source in self.connectors.keys():
                    if available_sources.get(source, False):
                        sources_to_query.append({
                            "source": source,
                            "priority": self.default_priorities.get(source, SourcePriority.MEDIUM),
                            "reason": "User selected all sources"
                        })
            elif user_preference in self.connectors and available_sources.get(user_preference, False):
                # Query only the user-specified source
                sources_to_query.append({
                    "source": user_preference,
                    "priority": SourcePriority.HIGH,
                    "reason": "User specified this source"
                })
                
            # Return if we have sources based on user preference
            if sources_to_query:
                return sources_to_query
        
        # Try to detect source from query
        detected_source = self.detect_source_from_query(query)
        if detected_source:
            if detected_source == KnowledgeSource.ALL:
                # Query all available sources
                for source in self.connectors.keys():
                    if available_sources.get(source, False):
                        sources_to_query.append({
                            "source": source,
                            "priority": self.default_priorities.get(source, SourcePriority.MEDIUM),
                            "reason": "Query mentioned all sources"
                        })
                return sources_to_query
            elif detected_source in self.connectors and available_sources.get(detected_source, False):
                # Query the detected source
                sources_to_query.append({
                    "source": detected_source,
                    "priority": SourcePriority.HIGH,
                    "reason": "Query specifically mentioned this source"
                })
                return sources_to_query
        
        # If no clear source detected, use intent-based routing
        intent = self.detect_query_intent(query)
        intent_priorities = self.intent_routing.get(intent, self.default_priorities)
        
        for source, priority in intent_priorities.items():
            if source in self.connectors and available_sources.get(source, False):
                sources_to_query.append({
                    "source": source,
                    "priority": priority,
                    "reason": f"Based on {intent.value} intent"
                })
        
        # If still no sources, fallback to all available sources
        if not sources_to_query:
            for source in self.connectors.keys():
                if available_sources.get(source, False):
                    sources_to_query.append({
                        "source": source,
                        "priority": self.default_priorities.get(source, SourcePriority.MEDIUM),
                        "reason": "Fallback to all available sources"
                    })
        
        return sources_to_query
    
    def query_knowledge_sources(self, query: str, 
                              user_preference: Optional[KnowledgeSource] = None,
                              max_results_per_source: int = 10) -> Dict[KnowledgeSource, List[Dict[str, Any]]]:
        """
        Query the appropriate knowledge sources based on query and preferences.
        
        Args:
            query: The query text
            user_preference: Optional user-specified source preference
            max_results_per_source: Maximum results to return per source
            
        Returns:
            Dictionary mapping sources to their query results
        """
        results = {}
        
        # Determine which sources to query
        sources_to_query = self.get_sources_for_query(query, user_preference)
        
        # Query each source
        for source_info in sources_to_query:
            source = source_info["source"]
            priority = source_info["priority"]
            connector = self.connectors.get(source)
            
            if not connector:
                logger.warning(f"No connector available for {source.value}")
                continue
                
            try:
                # Adjust results count based on priority
                if priority == SourcePriority.HIGH:
                    result_count = max_results_per_source
                elif priority == SourcePriority.MEDIUM:
                    result_count = max(5, max_results_per_source // 2)
                else:  # LOW
                    result_count = max(3, max_results_per_source // 3)
                
                # Query the source
                if source == KnowledgeSource.CONFLUENCE:
                    source_results = self._query_confluence(connector, query, result_count)
                elif source == KnowledgeSource.REMEDY:
                    source_results = self._query_remedy(connector, query, result_count)
                else:
                    logger.warning(f"Unsupported source type: {source.value}")
                    continue
                
                # Store results with metadata
                results[source] = {
                    "items": source_results,
                    "priority": priority.value,
                    "count": len(source_results),
                    "reason": source_info["reason"]
                }
                
            except Exception as e:
                logger.error(f"Error querying {source.value}: {str(e)}")
                results[source] = {
                    "items": [],
                    "priority": priority.value,
                    "count": 0,
                    "error": str(e),
                    "reason": source_info["reason"]
                }
        
        return results
    
    def _query_confluence(self, connector: ConfluenceConnector, 
                         query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        Query Confluence for relevant content.
        
        Args:
            connector: Confluence connector
            query: Query text
            max_results: Maximum number of results
            
        Returns:
            List of relevant content items
        """
        # Search Confluence content
        search_results = connector.search_content(query, limit=max_results)
        
        # Process each result
        processed_results = []
        for content in search_results:
            try:
                # Parse the content
                parsed_content = connector.parse_content(content)
                
                # Extract key information
                item = {
                    "id": parsed_content["metadata"]["id"],
                    "title": parsed_content["metadata"]["title"],
                    "type": "confluence_page",
                    "url": parsed_content["metadata"]["url"],
                    "text": parsed_content["text"][:1000] + "..." if len(parsed_content["text"]) > 1000 else parsed_content["text"],
                    "metadata": parsed_content["metadata"],
                    "created_at": parsed_content["metadata"].get("created_at"),
                    "updated_at": parsed_content["metadata"].get("updated_at")
                }
                
                # Add table information if available
                if parsed_content["tables"]:
                    item["tables"] = parsed_content["tables"]
                
                processed_results.append(item)
                
            except Exception as e:
                logger.warning(f"Error processing Confluence content: {str(e)}")
        
        return processed_results
    
    def _query_remedy(self, connector: RemedyConnector, 
                     query: str, max_results: int) -> List[Dict[str, Any]]:
        """
        Query Remedy for relevant tickets.
        
        Args:
            connector: Remedy connector
            query: Query text
            max_results: Maximum number of results
            
        Returns:
            List of relevant ticket items
        """
        # Extract any ticket IDs from the query
        ticket_ids = re.findall(r'\b(?:INC|CHG|REQ|TASK)\d{5,}\b', query, re.IGNORECASE)
        
        results = []
        
        # If specific ticket IDs were mentioned
        if ticket_ids:
            for ticket_id in ticket_ids[:5]:  # Limit to 5 explicit tickets
                # Determine ticket type (incident, change, etc.)
                ticket_type = "incident"  # Default
                if ticket_id.upper().startswith("CHG"):
                    ticket_type = "change"
                elif ticket_id.upper().startswith("REQ"):
                    ticket_type = "service_request"
                elif ticket_id.upper().startswith("TASK"):
                    ticket_type = "work_order"
                
                # Get the ticket with details
                ticket = connector.get_ticket_with_details(ticket_type, ticket_id)
                
                if ticket:
                    results.append({
                        "id": ticket.get("id"),
                        "title": ticket.get("title"),
                        "type": f"remedy_{ticket_type}",
                        "status": ticket.get("status"),
                        "priority": ticket.get("priority"),
                        "assigned_to": ticket.get("assigned_to"),
                        "owner_group": ticket.get("owner_group"),
                        "description": ticket.get("description"),
                        "created_at": ticket.get("created_at"),
                        "updated_at": ticket.get("updated_at")
                    })
        
        # Perform keyword search if we need more results
        if len(results) < max_results:
            keywords = re.findall(r'\b[a-zA-Z]{3,}\b', query)
            
            if keywords:
                # Search for tickets matching keywords
                search_results = connector.search_tickets_by_keyword(
                    "incident",  # Default to incidents
                    keywords=keywords,
                    limit=max_results - len(results)
                )
                
                for ticket in search_results:
                    results.append({
                        "id": ticket.get("id"),
                        "title": ticket.get("title"),
                        "type": "remedy_incident",
                        "status": ticket.get("status"),
                        "priority": ticket.get("priority"),
                        "assigned_to": ticket.get("assigned_to"),
                        "owner_group": ticket.get("owner_group"),
                        "description": ticket.get("description"),
                        "created_at": ticket.get("created_at"),
                        "updated_at": ticket.get("updated_at")
                    })
        
        return results
    
    def combine_results(self, source_results: Dict[KnowledgeSource, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Combine and rank results from multiple sources.
        
        Args:
            source_results: Dictionary mapping sources to their results
            
        Returns:
            Combined and ranked list of results
        """
        all_results = []
        
        # Process each source's results
        for source, results_data in source_results.items():
            items = results_data.get("items", [])
            priority = results_data.get("priority")
            
            # Convert priority to numeric score for sorting
            priority_score = {
                "high": 3,
                "medium": 2,
                "low": 1
            }.get(priority, 2)
            
            # Add each item with source and priority info
            for item in items:
                all_results.append({
                    "item": item,
                    "source": source,
                    "priority_score": priority_score,
                    "source_rank": items.index(item) + 1  # 1-based rank within source
                })
        
        # Rank the combined results
        ranked_results = self._rank_combined_results(all_results)
        
        # Return just the items with source info
        return [
            {
                **result["item"],
                "source": result["source"].value,
                "rank": index + 1
            }
            for index, result in enumerate(ranked_results)
        ]
    
    def _rank_combined_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Rank combined results using a balanced algorithm.
        
        Args:
            results: List of results with metadata
            
        Returns:
            Ranked list of results
        """
        if not results:
            return []
            
        # Calculate a score for each result
        for result in results:
            # Base score from source priority
            score = result["priority_score"] * 10
            
            # Adjust for source rank (higher ranks get lower scores)
            score -= min(result["source_rank"] - 1, 9)  # Cap at -9 to avoid negative scores
            
            # Boost certain item types
            item = result["item"]
            item_type = item.get("type", "")
            
            # Boost exact ticket matches
            if "remedy" in item_type and re.search(r'\b(?:INC|CHG|REQ|TASK)\d{5,}\b', item.get("id", ""), re.IGNORECASE):
                score += 5
                
            # Boost items with metadata showing recency
            updated_at = item.get("updated_at")
            if updated_at:
                # Simple recency check - in a real system use date comparison
                try:
                    if "202" in str(updated_at):  # Updated in recent years
                        score += 2
                except:
                    pass
                    
            # Add the score to the result
            result["score"] = score
        
        # Sort by score (descending)
        ranked_results = sorted(results, key=lambda x: x["score"], reverse=True)
        
        # Interleave results from different sources for diversity
        if len(set(r["source"] for r in results)) > 1:
            final_results = []
            source_queues = {}
            
            # Group by source
            for result in ranked_results:
                source = result["source"]
                if source not in source_queues:
                    source_queues[source] = []
                source_queues[source].append(result)
            
            # Interleave based on priority
            while any(source_queues.values()):
                # Process sources in priority order
                for priority in [3, 2, 1]:  # HIGH, MEDIUM, LOW
                    for source, queue in list(source_queues.items()):
                        if queue and queue[0]["priority_score"] == priority:
                            final_results.append(queue.pop(0))
                            if not queue:
                                del source_queues[source]
            
            return final_results
        else:
            # Only one source, return as is
            return ranked_results
    
    def generate_source_context_message(self, query: str, 
                                      selected_source: Optional[KnowledgeSource] = None) -> str:
        """
        Generate a context message about sources being queried.
        
        Args:
            query: The query text
            selected_source: User-selected source
            
        Returns:
            Context message about sources
        """
        sources_to_query = self.get_sources_for_query(query, selected_source)
        
        if not sources_to_query:
            return "No knowledge sources are currently available."
            
        if len(sources_to_query) == 1:
            source = sources_to_query[0]["source"]
            reason = sources_to_query[0]["reason"]
            
            # Get source-specific context keywords
            context_keywords = ", ".join(self.source_context.get(source, [])[:3])
            
            if context_keywords:
                return f"Querying {source.value} for information. This source contains {context_keywords} and similar content. {reason}."
            else:
                return f"Querying {source.value} for information. {reason}."
                
        else:
            source_names = [s["source"].value for s in sources_to_query]
            return f"Querying multiple sources: {', '.join(source_names)}. Will combine relevant information from all sources."
    
    def format_results_for_user(self, combined_results: List[Dict[str, Any]], 
                              query: str, max_items: int = 5) -> str:
        """
        Format results in a user-friendly message.
        
        Args:
            combined_results: Combined and ranked results
            query: Original query
            max_items: Maximum items to include
            
        Returns:
            Formatted message
        """
        if not combined_results:
            return "I couldn't find any relevant information for your query."
            
        message = f"Here's what I found for '{query}':\n\n"
        
        # Group by source for clearer presentation
        sources = {}
        for item in combined_results[:max_items]:
            source = item["source"]
            if source not in sources:
                sources[source] = []
            sources[source].append(item)
        
        # Add results from each source
        for source, items in sources.items():
            message += f"From {source}:\n"
            
            for item in items:
                if "remedy" in item.get("type", ""):
                    # Format Remedy tickets
                    message += f"- Ticket {item.get('id')}: {item.get('title')} "
                    message += f"(Status: {item.get('status')}, Priority: {item.get('priority')})\n"
                else:
                    # Format Confluence pages
                    message += f"- {item.get('title')} "
                    if item.get("url"):
                        message += f"[Link: {item.get('url')}]\n"
                    else:
                        message += "\n"
            
            message += "\n"
            
        return message
    
    def convert_to_documents(self, source_results: Dict[KnowledgeSource, Dict[str, Any]]) -> List[Document]:
        """
        Convert source results to Document objects for further processing.
        
        Args:
            source_results: Dictionary mapping sources to their results
            
        Returns:
            List of Document objects
        """
        documents = []
        
        # Process each source's results
        for source, results_data in source_results.items():
            items = results_data.get("items", [])
            
            for item in items:
                doc_id = f"{source.value}_{item.get('id')}"
                doc_type = item.get("type", "unknown")
                
                # Create document
                document = Document(
                    document_id=doc_id,
                    title=item.get("title", ""),
                    metadata={
                        "source": source.value,
                        "type": doc_type,
                        "url": item.get("url"),
                        "created_at": item.get("created_at"),
                        "updated_at": item.get("updated_at")
                    }
                )
                
                # Add text content
                if "text" in item:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=item["text"],
                            source_document=doc_id
                        )
                    )
                elif "description" in item:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=item["description"],
                            source_document=doc_id
                        )
                    )
                
                # Add tables if available
                if "tables" in item:
                    for i, table in enumerate(item["tables"]):
                        document.add_content(
                            DocumentContent(
                                content_type=ContentType.TABLE,
                                content=table.get("rows", []),  # Simplified for now
                                metadata={
                                    "headers": table.get("headers", []),
                                    "title": table.get("title", f"Table {i+1}")
                                },
                                source_document=doc_id
                            )
                        )
                
                # Add additional metadata as text
                if "remedy" in doc_type:
                    # Create a summary for Remedy tickets
                    metadata_text = []
                    for key in ["status", "priority", "assigned_to", "owner_group"]:
                        if key in item:
                            metadata_text.append(f"{key.replace('_', ' ').title()}: {item[key]}")
                    
                    if metadata_text:
                        document.add_content(
                            DocumentContent(
                                content_type=ContentType.TEXT,
                                content="\n".join(metadata_text),
                                metadata={"type": "metadata_summary"},
                                source_document=doc_id
                            )
                        )
                
                documents.append(document)
        
        return documents











Advanced OCR Engine with Layout Analysis

"""
Advanced OCR engine with layout analysis and image preprocessing.
Handles complex document layouts, tables in images, and multiple languages.
"""
import os
import logging
import tempfile
import math
from typing import Dict, Any, List, Optional, Tuple, Union
import json
from pathlib import Path
import re
import base64
import io

import numpy as np
from PIL import Image, ImageEnhance, ImageFilter

# Optional imports with fallbacks
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logging.warning("OpenCV not installed. Advanced image processing will be limited.")

try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False
    logging.warning("Pytesseract not installed. Basic OCR will be unavailable.")

try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    logging.warning("EasyOCR not installed. Advanced OCR will be unavailable.")

logger = logging.getLogger(__name__)

class OCRPreprocessor:
    """
    Image preprocessor that enhances images for better OCR results.
    Applies various image processing techniques based on image characteristics.
    """
    
    def __init__(self, use_advanced: bool = True):
        """
        Initialize OCR preprocessor.
        
        Args:
            use_advanced: Whether to use advanced OpenCV techniques if available
        """
        self.use_advanced = use_advanced and CV2_AVAILABLE
        
    def preprocess(self, image: Union[str, Image.Image, np.ndarray]) -> Image.Image:
        """
        Preprocess image for better OCR results.
        
        Args:
            image: Image file path, PIL image, or numpy array
            
        Returns:
            Preprocessed PIL Image
        """
        # Load image if path provided
        if isinstance(image, str):
            pil_image = Image.open(image)
        elif isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        elif isinstance(image, Image.Image):
            pil_image = image
        else:
            raise ValueError("Unsupported image type")
            
        # Make a copy to avoid modifying original
        pil_image = pil_image.copy()
        
        # Convert to RGB if needed
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
            
        # Basic PIL-based enhancements
        pil_image = self._basic_enhance(pil_image)
        
        # Apply advanced CV2 preprocessing if available
        if self.use_advanced:
            # Convert to OpenCV format
            cv_image = np.array(pil_image)
            cv_image = cv_image[:, :, ::-1].copy()  # RGB to BGR
            
            # Apply advanced preprocessing
            processed_cv_image = self._advanced_enhance(cv_image)
            
            # Convert back to PIL
            processed_pil_image = Image.fromarray(cv2.cvtColor(processed_cv_image, cv2.COLOR_BGR2RGB))
            
            # Check which one is better (based on contrast and clarity)
            if self._image_quality_score(processed_pil_image) > self._image_quality_score(pil_image):
                return processed_pil_image
                
        return pil_image
    
    def _basic_enhance(self, image: Image.Image) -> Image.Image:
        """Apply basic enhancements using PIL."""
        # Resize if too small or too large
        orig_width, orig_height = image.size
        if orig_width < 1000 or orig_height < 1000:
            scale_factor = 1000 / min(orig_width, orig_height)
            new_width = int(orig_width * scale_factor)
            new_height = int(orig_height * scale_factor)
            image = image.resize((new_width, new_height), Image.LANCZOS)
        elif orig_width > 3000 or orig_height > 3000:
            scale_factor = 3000 / max(orig_width, orig_height)
            new_width = int(orig_width * scale_factor)
            new_height = int(orig_height * scale_factor)
            image = image.resize((new_width, new_height), Image.LANCZOS)
            
        # Check if the image is dark
        brightness = self._calculate_brightness(image)
        if brightness < 120:  # Dark image
            image = ImageEnhance.Brightness(image).enhance(1.5)
            image = ImageEnhance.Contrast(image).enhance(1.2)
        else:
            # Enhance contrast slightly for normal images
            image = ImageEnhance.Contrast(image).enhance(1.1)
            
        # Sharpen for better details
        image = image.filter(ImageFilter.SHARPEN)
        
        return image
    
    def _advanced_enhance(self, image: np.ndarray) -> np.ndarray:
        """Apply advanced enhancements using OpenCV."""
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Check if the image needs denoising
        noise_level = np.std(gray)
        if noise_level > 20:  # Noisy image
            denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
        else:
            denoised = image
            
        # Detect if image is skewed
        lines = cv2.HoughLinesP(cv2.Canny(gray, 50, 150, apertureSize=3), 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)
        
        if lines is not None and len(lines) > 5:
            angles = []
            for line in lines:
                x1, y1, x2, y2 = line[0]
                if x2 - x1 != 0:  # avoid division by zero
                    angle = math.atan2(y2 - y1, x2 - x1) * 180.0 / np.pi
                    # Only consider mostly horizontal lines
                    if abs(angle) < 20:
                        angles.append(angle)
                        
            if angles:
                # Get median angle to avoid outliers
                median_angle = np.median(angles)
                
                # Only deskew if the angle is significant
                if abs(median_angle) > 0.5:
                    # Get image dimensions
                    (h, w) = gray.shape
                    center = (w // 2, h // 2)
                    
                    # Deskew
                    M = cv2.getRotationMatrix2D(center, median_angle, 1.0)
                    denoised = cv2.warpAffine(denoised, M, (w, h), 
                                             flags=cv2.INTER_CUBIC, 
                                             borderMode=cv2.BORDER_REPLICATE)
        
        # Apply adaptive thresholding for better text detection
        gray_denoised = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)
        binary = cv2.adaptiveThreshold(gray_denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                      cv2.THRESH_BINARY, 11, 5)
        
        # Return both the denoised color image and the binary image
        # Choose based on image content
        text_pixels = np.sum(binary == 0)  # Count black pixels (text)
        if text_pixels > (gray.shape[0] * gray.shape[1] * 0.01):
            # If there's significant text, use binary image
            return cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
        else:
            # Otherwise use the denoised color image
            return denoised
    
    def _calculate_brightness(self, image: Image.Image) -> float:
        """Calculate average brightness of an image."""
        # Convert to grayscale
        gray = image.convert('L')
        # Calculate average pixel value
        stat = ImageStat.Stat(gray)
        return stat.mean[0]
    
    def _image_quality_score(self, image: Image.Image) -> float:
        """Calculate a quality score for OCR suitability."""
        # Convert to grayscale
        gray = image.convert('L')
        
        # Calculate contrast
        stat = ImageStat.Stat(gray)
        contrast = stat.stddev[0]
        
        # Calculate edge density (sharpness)
        edges = gray.filter(ImageFilter.FIND_EDGES)
        edge_stat = ImageStat.Stat(edges)
        edge_density = edge_stat.mean[0]
        
        # Combined score
        return contrast * 0.6 + edge_density * 0.4

class LayoutAnalyzer:
    """
    Analyzes document layout to identify paragraphs, tables, headers, etc.
    """
    
    def __init__(self):
        """Initialize layout analyzer."""
        self.min_table_rows = 2
        self.min_table_cols = 2
        
    def analyze_layout(self, image: Union[str, Image.Image, np.ndarray]) -> Dict[str, Any]:
        """
        Analyze document layout.
        
        Args:
            image: Image file path, PIL image, or numpy array
            
        Returns:
            Dictionary with layout information
        """
        if not CV2_AVAILABLE:
            return {"error": "OpenCV not available for layout analysis"}
            
        # Load image if needed
        if isinstance(image, str):
            cv_image = cv2.imread(image)
        elif isinstance(image, Image.Image):
            cv_image = np.array(image)
            cv_image = cv_image[:, :, ::-1].copy()  # RGB to BGR
        elif isinstance(image, np.ndarray):
            cv_image = image.copy()
        else:
            return {"error": "Unsupported image type"}
            
        # Convert to grayscale
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # Regions of interest
        layout_info = {
            "paragraphs": [],
            "headers": [],
            "tables": [],
            "images": [],
            "lists": []
        }
        
        # Detect paragraphs and text blocks
        layout_info["paragraphs"] = self._detect_paragraphs(gray, cv_image)
        
        # Detect tables
        layout_info["tables"] = self._detect_tables(gray, cv_image)
        
        # Detect headers
        layout_info["headers"] = self._detect_headers(gray, cv_image, layout_info["paragraphs"])
        
        # Detect bullet lists
        layout_info["lists"] = self._detect_lists(gray, cv_image)
        
        return layout_info
    
    def _detect_paragraphs(self, gray: np.ndarray, image: np.ndarray) -> List[Dict[str, Any]]:
        """Detect paragraphs and text blocks."""
        paragraphs = []
        
        # Apply adaptive thresholding
        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                      cv2.THRESH_BINARY_INV, 11, 5)
        
        # Dilate to connect letters into words
        kernel = np.ones((3, 15), np.uint8)  # Horizontal dilation
        dilated = cv2.dilate(binary, kernel, iterations=1)
        
        # Find contours
        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Skip very small regions
            if w < 100 or h < 20:
                continue
                
            # Skip very large regions (likely not a paragraph)
            if w > gray.shape[1] * 0.9 and h > gray.shape[0] * 0.5:
                continue
                
            # Paragraph aspect ratio should be wider than tall
            aspect_ratio = w / float(h)
            if 1.0 < aspect_ratio < 20.0:
                paragraphs.append({
                    "bbox": [x, y, w, h],
                    "aspect_ratio": aspect_ratio
                })
                
        return paragraphs
    
    def _detect_tables(self, gray: np.ndarray, image: np.ndarray) -> List[Dict[str, Any]]:
        """Detect tables in the document."""
        tables = []
        
        # Apply edge detection
        edges = cv2.Canny(gray, 50, 150, apertureSize=3)
        
        # Detect lines
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=20)
        
        if lines is None:
            return tables
            
        # Separate horizontal and vertical lines
        h_lines = []
        v_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line[0]
            
            # Calculate angle
            angle = math.atan2(y2 - y1, x2 - x1) * 180 / np.pi
            
            # Classify as horizontal or vertical
            if abs(angle) < 20 or abs(angle) > 160:
                h_lines.append((x1, y1, x2, y2))
            elif abs(angle) > 70 and abs(angle) < 110:
                v_lines.append((x1, y1, x2, y2))
        
        # Not enough lines for a table
        if len(h_lines) < self.min_table_rows or len(v_lines) < self.min_table_cols:
            return tables
            
        # Find intersections
        intersections = []
        
        for h_line in h_lines:
            h_x1, h_y1, h_x2, h_y2 = h_line
            
            for v_line in v_lines:
                v_x1, v_y1, v_x2, v_y2 = v_line
                
                # Calculate intersection point
                d = (h_x1 - h_x2) * (v_y1 - v_y2) - (h_y1 - h_y2) * (v_x1 - v_x2)
                
                if d != 0:  # Not parallel
                    x = ((v_x1 - v_x2) * (h_x1 * h_y2 - h_y1 * h_x2) - 
                         (h_x1 - h_x2) * (v_x1 * v_y2 - v_y1 * v_x2)) / d
                    y = ((v_y1 - v_y2) * (h_x1 * h_y2 - h_y1 * h_x2) - 
                         (h_y1 - h_y2) * (v_x1 * v_y2 - v_y1 * v_x2)) / d
                    
                    # Check if intersection is within line segments
                    if (min(h_x1, h_x2) <= x <= max(h_x1, h_x2) and 
                        min(h_y1, h_y2) <= y <= max(h_y1, h_y2) and
                        min(v_x1, v_x2) <= x <= max(v_x1, v_x2) and
                        min(v_y1, v_y2) <= y <= max(v_y1, v_y2)):
                        intersections.append((int(x), int(y)))
        
        # If we have enough intersections, we have a table
        if len(intersections) >= (self.min_table_rows * self.min_table_cols):
            # Group intersections into a grid
            x_coords = sorted(list(set(int(point[0]) for point in intersections)))
            y_coords = sorted(list(set(int(point[1]) for point in intersections)))
            
            # Create a bounding box around the table
            if x_coords and y_coords:
                x_min, x_max = min(x_coords), max(x_coords)
                y_min, y_max = min(y_coords), max(y_coords)
                
                tables.append({
                    "bbox": [x_min, y_min, x_max - x_min, y_max - y_min],
                    "rows": len(y_coords) - 1,  # Number of rows
                    "cols": len(x_coords) - 1,  # Number of columns
                    "intersections": intersections
                })
                
        return tables
    
    def _detect_headers(self, gray: np.ndarray, image: np.ndarray, 
                       paragraphs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect headers based on position and size."""
        headers = []
        
        # Sort paragraphs by y-coordinate
        sorted_paragraphs = sorted(paragraphs, key=lambda p: p["bbox"][1])
        
        # Check top paragraphs first
        top_margin = gray.shape[0] * 0.2  # Top 20% of the page
        
        for i, para in enumerate(sorted_paragraphs):
            x, y, w, h = para["bbox"]
            
            # Headers are typically at the top of the page or before other paragraphs
            is_top = y < top_margin
            is_first = i == 0
            has_space_after = False
            
            # Check if there's significant space after this paragraph
            if i < len(sorted_paragraphs) - 1:
                next_y = sorted_paragraphs[i+1]["bbox"][1]
                space_after = next_y - (y + h)
                has_space_after = space_after > h * 1.5  # More than 1.5x its height
            
            # Check for centered paragraph (potential title)
            page_center = gray.shape[1] / 2
            is_centered = abs(x + w/2 - page_center) < page_center * 0.2  # Within 20% of center
            
            # Headers are typically shorter
            is_short = w < gray.shape[1] * 0.7  # Less than 70% of page width
            
            # Combine conditions
            if (is_top or is_first or has_space_after) and (is_centered or is_short):
                headers.append({
                    "bbox": [x, y, w, h],
                    "is_top": is_top,
                    "is_centered": is_centered
                })
                
        return headers
    
    def _detect_lists(self, gray: np.ndarray, image: np.ndarray) -> List[Dict[str, Any]]:
        """Detect bullet points and numbered lists."""
        lists = []
        
        # Apply adaptive thresholding
        binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                      cv2.THRESH_BINARY_INV, 11, 5)
        
        # Look for bullet-like shapes
        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20,
                                  param1=50, param2=30, minRadius=3, maxRadius=10)
        
        bullet_points = []
        if circles is not None:
            circles = np.round(circles[0, :]).astype("int")
            for (x, y, r) in circles:
                bullet_points.append((x, y, r))
        
        # If we found bullet points, check if they're aligned vertically
        if len(bullet_points) >= 3:
            # Group by x-coordinate
            x_coords = [p[0] for p in bullet_points]
            x_mean = sum(x_coords) / len(x_coords)
            aligned_bullets = [p for p in bullet_points if abs(p[0] - x_mean) < 20]
            
            if len(aligned_bullets) >= 3:
                # Sort by y-coordinate
                aligned_bullets.sort(key=lambda p: p[1])
                
                # Check spacing
                consistent_spacing = True
                spacings = []
                
                for i in range(1, len(aligned_bullets)):
                    spacing = aligned_bullets[i][1] - aligned_bullets[i-1][1]
                    spacings.append(spacing)
                
                # Check if spacings are consistent
                if spacings:
                    avg_spacing = sum(spacings) / len(spacings)
                    consistent_spacing = all(abs(s - avg_spacing) < avg_spacing * 0.3 for s in spacings)
                
                if consistent_spacing:
                    # Find the leftmost x-coordinate
                    left_x = min(b[0] for b in aligned_bullets)
                    
                    # Find the top and bottom y-coordinates
                    top_y = min(b[1] for b in aligned_bullets)
                    bottom_y = max(b[1] for b in aligned_bullets)
                    
                    # Estimate the right boundary
                    right_x = gray.shape[1] * 0.8  # Assume list items extend to 80% of page width
                    
                    lists.append({
                        "bbox": [left_x - 20, top_y - 20, int(right_x - left_x + 40), bottom_y - top_y + 40],
                        "type": "bullet",
                        "count": len(aligned_bullets)
                    })
        
        return lists

class OCREngine:
    """
    Advanced OCR engine with support for multiple back-ends and languages.
    """
    
    def __init__(self, engine: str = "auto", languages: List[str] = ["en"], 
                 tessdata_dir: Optional[str] = None):
        """
        Initialize OCR engine.
        
        Args:
            engine: OCR engine to use ('pytesseract', 'easyocr', or 'auto')
            languages: List of language codes
            tessdata_dir: Path to Tesseract data directory
        """
        self.engine_type = engine
        self.languages = languages
        self.tessdata_dir = tessdata_dir
        
        # Set up preprocessor
        self.preprocessor = OCRPreprocessor()
        self.layout_analyzer = LayoutAnalyzer()
        
        # Initialize OCR engines
        self.pytesseract_available = PYTESSERACT_AVAILABLE
        self.easyocr_available = EASYOCR_AVAILABLE
        
        if self.engine_type == "auto":
            # Choose best available engine
            if self.easyocr_available:
                self.engine_type = "easyocr"
            elif self.pytesseract_available:
                self.engine_type = "pytesseract"
            else:
                raise ValueError("No OCR engine available. Please install EasyOCR or pytesseract.")
                
        # Initialize the selected engine
        if self.engine_type == "easyocr":
            if not self.easyocr_available:
                raise ValueError("EasyOCR not available. Please install it or choose a different engine.")
            self.reader = easyocr.Reader(languages)
            
        elif self.engine_type == "pytesseract":
            if not self.pytesseract_available:
                raise ValueError("pytesseract not available. Please install it or choose a different engine.")
            if self.tessdata_dir:
                pytesseract.pytesseract.tesseract_cmd = self.tessdata_dir
                
    def read_text(self, image: Union[str, Image.Image, np.ndarray], 
                  preprocess: bool = True) -> str:
        """
        Extract text from an image.
        
        Args:
            image: Image file path, PIL image, or numpy array
            preprocess: Whether to preprocess the image
            
        Returns:
            Extracted text
        """
        # Preprocess image if requested
        if preprocess:
            processed_image = self.preprocessor.preprocess(image)
        else:
            # Load image if path provided
            if isinstance(image, str):
                processed_image = Image.open(image)
            elif isinstance(image, np.ndarray):
                processed_image = Image.fromarray(image)
            elif isinstance(image, Image.Image):
                processed_image = image
            else:
                raise ValueError("Unsupported image type")
        
        # Extract text using the selected engine
        if self.engine_type == "easyocr":
            return self._read_with_easyocr(processed_image)
        else:  # pytesseract
            return self._read_with_pytesseract(processed_image)
            
    def _read_with_easyocr(self, image: Image.Image) -> str:
        """Extract text using EasyOCR."""
        # Convert to numpy array
        img_array = np.array(image)
        
        # Read text
        result = self.reader.readtext(img_array)
        
        # Combine all text
        text = ' '.join([item[1] for item in result])
        
        return text
    
    def _read_with_pytesseract(self, image: Image.Image) -> str:
        """Extract text using pytesseract."""
        # Set language
        lang = '+'.join(self.languages)
        
        # Get text
        text = pytesseract.image_to_string(image, lang=lang)
        
        return text
    
    def read_document(self, image: Union[str, Image.Image, np.ndarray],
                     with_layout: bool = True) -> Dict[str, Any]:
        """
        Extract text with layout analysis.
        
        Args:
            image: Image file path, PIL image, or numpy array
            with_layout: Whether to perform layout analysis
            
        Returns:
            Dictionary with extracted text and layout information
        """
        # Preprocess image
        processed_image = self.preprocessor.preprocess(image)
        
        result = {
            "full_text": self.read_text(processed_image, preprocess=False),
            "preprocessed": True
        }
        
        # Perform layout analysis if requested
        if with_layout and CV2_AVAILABLE:
            layout_info = self.layout_analyzer.analyze_layout(processed_image)
            
            # Load image as numpy array for cropping
            img_array = np.array(processed_image)
            
            # Extract text from each region
            structured_text = {
                "paragraphs": [],
                "headers": [],
                "tables": [],
                "lists": []
            }
            
            # Process headers
            for i, header in enumerate(layout_info["headers"]):
                x, y, w, h = header["bbox"]
                
                # Crop the region
                region = img_array[y:y+h, x:x+w]
                
                # Extract text
                region_text = self.read_text(region, preprocess=False)
                
                structured_text["headers"].append({
                    "text": region_text,
                    "bbox": header["bbox"],
                    "is_top": header.get("is_top", False),
                    "is_centered": header.get("is_centered", False)
                })
            
            # Process paragraphs
            for i, para in enumerate(layout_info["paragraphs"]):
                x, y, w, h = para["bbox"]
                
                # Skip if this region overlaps with a header
                if any(self._regions_overlap(para["bbox"], header["bbox"]) 
                      for header in layout_info["headers"]):
                    continue
                    
                # Crop the region
                region = img_array[y:y+h, x:x+w]
                
                # Extract text
                region_text = self.read_text(region, preprocess=False)
                
                structured_text["paragraphs"].append({
                    "text": region_text,
                    "bbox": para["bbox"]
                })
            
            # Process tables
            for i, table in enumerate(layout_info["tables"]):
                x, y, w, h = table["bbox"]
                
                # Crop the region
                region = img_array[y:y+h, x:x+w]
                
                # Extract table structure (if advanced OCR is available)
                table_data = self._extract_table_structure(region, table["rows"], table["cols"])
                
                structured_text["tables"].append({
                    "data": table_data,
                    "bbox": table["bbox"],
                    "rows": table["rows"],
                    "cols": table["cols"]
                })
            
            # Process lists
            for i, list_item in enumerate(layout_info["lists"]):
                x, y, w, h = list_item["bbox"]
                
                # Crop the region
                region = img_array[y:y+h, x:x+w]
                
                # Extract text
                region_text = self.read_text(region, preprocess=False)
                
                # Split into lines
                lines = region_text.split('\n')
                items = [line.strip() for line in lines if line.strip()]
                
                structured_text["lists"].append({
                    "items": items,
                    "text": region_text,
                    "bbox": list_item["bbox"],
                    "type": list_item["type"]
                })
            
            # Add structured text to result
            result["structured"] = structured_text
            
        return result
    
    def _regions_overlap(self, bbox1, bbox2):
        """Check if two bounding boxes overlap."""
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        
        # Calculate coordinates of box corners
        left1, right1 = x1, x1 + w1
        top1, bottom1 = y1, y1 + h1
        
        left2, right2 = x2, x2 + w2
        top2, bottom2 = y2, y2 + h2
        
        # Check for overlap
        return not (right1 < left2 or left1 > right2 or bottom1 < top2 or top1 > bottom2)
    
    def _extract_table_structure(self, table_image, rows, cols):
        """Extract structured data from a table image."""
        # Convert to PIL Image if numpy array
        if isinstance(table_image, np.ndarray):
            table_pil = Image.fromarray(table_image)
        else:
            table_pil = table_image
            
        # Get text using standard OCR
        table_text = self.read_text(table_pil, preprocess=False)
        
        # Basic approach: split by lines and estimate cell boundaries
        lines = table_text.split('\n')
        lines = [line.strip() for line in lines if line.strip()]
        
        # Create an empty table structure
        table_data = []
        
        # If we have at least some lines, try to structure them
        if lines:
            # Try to extrapolate columns based on spaces
            if len(lines) >= rows:
                for i in range(min(rows, len(lines))):
                    # Split by large spaces
                    row_data = re.split(r'\s{2,}', lines[i])
                    
                    # If we got too many or too few columns, adjust
                    if len(row_data) < cols:
                        # Pad with empty strings
                        row_data.extend([''] * (cols - len(row_data)))
                    elif len(row_data) > cols:
                        # Combine extra columns
                        row_data = row_data[:cols-1] + [' '.join(row_data[cols-1:])]
                        
                    table_data.append(row_data)
            else:
                # Not enough lines, just add what we have
                table_data = [lines]
        
        return table_data
    
    def recognize_image(self, image: Union[str, Image.Image, np.ndarray]) -> Dict[str, Any]:
        """
        Full image recognition with OCR and layout analysis.
        
        Args:
            image: Image file path, PIL image, or numpy array
            
        Returns:
            Dictionary with all extracted information
        """
        # Process with layout analysis
        ocr_result = self.read_document(image, with_layout=True)
        
        # Additional metadata
        if isinstance(image, str):
            file_name = os.path.basename(image)
            file_size = os.path.getsize(image)
            ocr_result["file_info"] = {"name": file_name, "size": file_size}
            
        # Add processing timestamp
        import datetime
        ocr_result["timestamp"] = datetime.datetime.now().isoformat()
        
        return ocr_result

class OCRDocumentProcessor:
    """
    Process documents with OCR for full-text and structured extraction.
    """
    
    def __init__(self, output_dir: Optional[str] = None, languages: List[str] = ["en"]):
        """
        Initialize OCR document processor.
        
        Args:
            output_dir: Directory to save processed results
            languages: List of languages to support
        """
        self.output_dir = output_dir
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        # Initialize OCR engine
        try:
            self.ocr_engine = OCREngine(engine="auto", languages=languages)
        except ValueError as e:
            logger.error(f"Error initializing OCR engine: {str(e)}")
            self.ocr_engine = None
            
    def process_file(self, file_path: str, save_result: bool = True) -> Dict[str, Any]:
        """
        Process a document file with OCR.
        
        Args:
            file_path: Path to image or PDF file
            save_result: Whether to save result to output_dir
            
        Returns:
            Dictionary with OCR results
        """
        if not self.ocr_engine:
            return {"error": "OCR engine not available"}
            
        if not os.path.exists(file_path):
            return {"error": f"File not found: {file_path}"}
            
        # Get file extension
        _, ext = os.path.splitext(file_path)
        ext = ext.lower()
        
        # Process based on file type
        if ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif']:
            result = self._process_image(file_path)
        elif ext == '.pdf':
            result = self._process_pdf(file_path)
        else:
            return {"error": f"Unsupported file type: {ext}"}
            
        # Save result if requested
        if save_result and self.output_dir and result and not "error" in result:
            output_path = os.path.join(
                self.output_dir, 
                f"{os.path.splitext(os.path.basename(file_path))[0]}_ocr.json"
            )
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            result["saved_to"] = output_path
            
        return result
    
    def _process_image(self, image_path: str) -> Dict[str, Any]:
        """Process a single image file."""
        try:
            return self.ocr_engine.recognize_image(image_path)
        except Exception as e:
            logger.error(f"Error processing image {image_path}: {str(e)}")
            return {"error": str(e)}
    
    def _process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """Process a PDF document."""
        try:
            # Check for PDF processing libraries
            if not any([CV2_AVAILABLE, PYTESSERACT_AVAILABLE, EASYOCR_AVAILABLE]):
                return {"error": "No PDF processing libraries available"}
                
            # Use PyMuPDF if available
            try:
                import fitz
                return self._process_pdf_pymupdf(pdf_path)
            except ImportError:
                pass
                
            # Use pdf2image if available
            try:
                import pdf2image
                return self._process_pdf_pdf2image(pdf_path)
            except ImportError:
                return {"error": "No PDF extraction libraries available"}
                
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_path}: {str(e)}")
            return {"error": str(e)}
    
    def _process_pdf_pymupdf(self, pdf_path: str) -> Dict[str, Any]:
        """Process PDF using PyMuPDF (higher quality)."""
        import fitz
        
        doc = fitz.open(pdf_path)
        result = {
            "pdf_info": {
                "title": doc.metadata.get("title", ""),
                "author": doc.metadata.get("author", ""),
                "subject": doc.metadata.get("subject", ""),
                "keywords": doc.metadata.get("keywords", ""),
                "page_count": len(doc)
            },
            "pages": {}
        }
        
        for page_num, page in enumerate(doc):
            # Render page to image
            pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # Process with OCR
            page_result = self.ocr_engine.recognize_image(img)
            
            # Store result
            result["pages"][str(page_num + 1)] = page_result
            
        return result
    
    def _process_pdf_pdf2image(self, pdf_path: str) -> Dict[str, Any]:
        """Process PDF using pdf2image."""
        import pdf2image
        
        # Convert PDF to images
        images = pdf2image.convert_from_path(pdf_path)
        
        result = {
            "pdf_info": {
                "page_count": len(images)
            },
            "pages": {}
        }
        
        for page_num, img in enumerate(images):
            # Process with OCR
            page_result = self.ocr_engine.recognize_image(img)
            
            # Store result
            result["pages"][str(page_num + 1)] = page_result
            
        return result
    
    def extract_from_base64(self, base64_string: str, file_type: str = "png") -> Dict[str, Any]:
        """
        Extract text from a base64-encoded image.
        
        Args:
            base64_string: Base64-encoded image data
            file_type: Image file type (extension)
            
        Returns:
            OCR result dictionary
        """
        if not self.ocr_engine:
            return {"error": "OCR engine not available"}
            
        try:
            # Decode base64
            image_data = base64.b64decode(base64_string)
            
            # Create PIL Image
            img = Image.open(io.BytesIO(image_data))
            
            # Process with OCR
            return self.ocr_engine.recognize_image(img)
            
        except Exception as e:
            logger.error(f"Error processing base64 image: {str(e)}")
            return {"error": str(e)}








Advanced Query Analysis with Intent Detection

"""
Advanced query analysis with intent detection, entity extraction, 
query expansion, and source routing.
"""
import logging
import re
import string
import json
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from collections import Counter, defaultdict
import numpy as np

logger = logging.getLogger(__name__)

class QueryClassifier:
    """
    Classifies queries by type, intent, and complexity.
    """
    
    def __init__(self):
        """Initialize query classifier."""
        # Define intent patterns
        self.intent_patterns = {
            "definition": [
                r'what (?:is|are) (?:a |an |the )?([^\?]+)',
                r'define (?:a |an |the )?([^\?]+)',
                r'meaning of (?:a |an |the )?([^\?]+)',
                r'definition of (?:a |an |the )?([^\?]+)'
            ],
            "procedure": [
                r'how (?:do|can|to) (?:i|we|you)? ([^\?]+)',
                r'steps (?:to|for) ([^\?]+)',
                r'process (?:of|for) ([^\?]+)',
                r'guide (?:to|for) ([^\?]+)',
                r'instructions (?:for|to) ([^\?]+)'
            ],
            "comparison": [
                r'(?:compare|comparison of|difference between) ([^\?]+)',
                r'(?:vs|versus) ([^\?]+)',
                r'what (?:is|are) the (?:differences?|similarities?) between ([^\?]+)',
                r'how (?:does|do) ([^\?]+) (?:compare to|differ from) ([^\?]+)'
            ],
            "status": [
                r'status of ([^\?]+)',
                r'what is the (?:current )?status of ([^\?]+)',
                r'where (?:is|are) ([^\?]+) (?:at|now)',
                r'how (?:is|are) ([^\?]+) (?:doing|progressing|coming along)'
            ],
            "troubleshooting": [
                r'(?:troubleshoot|fix|solve|resolve) ([^\?]+)',
                r'why (?:is|are|does|do) ([^\?]+) (?:not|n\'t) ([^\?]+)',
                r'(?:issue|problem|error|bug) with ([^\?]+)',
                r'how to (?:fix|solve|resolve|troubleshoot) ([^\?]+)'
            ],
            "location": [
                r'where (?:is|are|can i find) ([^\?]+)',
                r'location of ([^\?]+)',
                r'find (?:a|an|the)? ([^\?]+)'
            ],
            "time": [
                r'when (?:is|are|will|should|did|does) ([^\?]+)',
                r'time (?:for|of) ([^\?]+)',
                r'schedule (?:for|of) ([^\?]+)'
            ],
            "reason": [
                r'why (?:is|are|does|do|should|would) ([^\?]+)',
                r'reason (?:for|why) ([^\?]+)',
                r'explain why ([^\?]+)'
            ],
            "recommendation": [
                r'(?:recommend|suggest) (?:a|an|some|the best) ([^\?]+)',
                r'what (?:is|are) the best ([^\?]+)',
                r'which ([^\?]+) should i ([^\?]+)'
            ],
            "factual": [
                r'who (?:is|are|was|were) ([^\?]+)',
                r'tell me about ([^\?]+)'
            ]
        }
        
        # Define source-specific keywords
        self.source_keywords = {
            "confluence": [
                "documentation", "guide", "manual", "document", "tutorial", "article", 
                "page", "space", "wiki", "knowledge base", "kb", "learn", "content",
                "information", "docs", "reference"
            ],
            "remedy": [
                "ticket", "incident", "problem", "change", "issue", "status", "request",
                "service", "support", "help desk", "resolution", "update", "assignment",
                "work order", "case", "assigned", "raised", "created", "opened", "closed",
                "task", "worklog", "sla", "priority", "severity"
            ]
        }
        
        # Define query types
        self.query_types = {
            "question": r'\?$',
            "command": r'!$',
            "statement": r'\.$'
        }

    def classify_intent(self, query: str) -> Dict[str, Any]:
        """
        Classify the intent of a query.
        
        Args:
            query: The query to classify
            
        Returns:
            Dictionary with intent classification
        """
        # Normalize query
        normalized_query = query.lower().strip()
        
        # Determine query type
        query_type = "statement"  # Default
        for qtype, pattern in self.query_types.items():
            if re.search(pattern, normalized_query):
                query_type = qtype
                break
        
        # Determine intent
        intent_scores = {}
        primary_intent = "unknown"
        highest_score = 0
        
        for intent, patterns in self.intent_patterns.items():
            # Check each pattern
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, normalized_query)
                if matches:
                    # Add score for each match
                    score += len(matches) * 0.25
                    
                    # Bonus for exact pattern match
                    if pattern.replace(r'([^\?]+)', '').strip() in normalized_query:
                        score += 0.5
            
            intent_scores[intent] = score
            
            if score > highest_score:
                highest_score = score
                primary_intent = intent
        
        # If no clear intent matched, try keyword-based approach
        if highest_score == 0:
            # Keywords indicating different intents
            intent_keywords = {
                "definition": ["what is", "define", "meaning", "definition"],
                "procedure": ["how to", "steps", "process", "guide", "instructions"],
                "comparison": ["compare", "versus", "vs", "differences", "similarities"],
                "status": ["status", "progress", "update"],
                "troubleshooting": ["fix", "solve", "resolve", "issue", "problem", "error"],
                "recommendation": ["recommend", "suggest", "best", "should i"],
                "factual": ["who", "what", "where", "when", "tell me about"]
            }
            
            for intent, keywords in intent_keywords.items():
                for keyword in keywords:
                    if keyword in normalized_query:
                        intent_scores[intent] = intent_scores.get(intent, 0) + 0.2
                        
                        if intent_scores[intent] > highest_score:
                            highest_score = intent_scores[intent]
                            primary_intent = intent
        
        # If still no clear intent, use default based on query type
        if primary_intent == "unknown":
            if query_type == "question":
                if normalized_query.startswith("what"):
                    primary_intent = "definition"
                elif normalized_query.startswith("how"):
                    primary_intent = "procedure"
                elif normalized_query.startswith("why"):
                    primary_intent = "reason"
                else:
                    primary_intent = "factual"
            elif query_type == "command":
                primary_intent = "procedure"
        
        # Check if specific sources are mentioned
        source_indicators = {}
        for source, keywords in self.source_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in normalized_query:
                    score += 1
            source_indicators[source] = score / len(keywords) if score > 0 else 0
        
        # Determine the target source based on intent and keywords
        target_source = "both"  # Default to both
        
        # If we have strong indicators for a particular source
        if max(source_indicators.values(), default=0) > 0.1:
            max_source = max(source_indicators.items(), key=lambda x: x[1])[0]
            target_source = max_source
        
        # Certain intents have natural source affinities
        intent_source_affinities = {
            "definition": "confluence",
            "procedure": "confluence",
            "factual": "confluence",
            "status": "remedy", 
            "troubleshooting": "remedy"
        }
        
        # If we have a source affinity for this intent and no strong keyword indicators
        if primary_intent in intent_source_affinities and max(source_indicators.values(), default=0) < 0.2:
            target_source = intent_source_affinities[primary_intent]
        
        # Determine complexity
        words = normalized_query.split()
        complexity = "simple" if len(words) < 8 else "complex"
        
        # Check for specific source requests
        explicit_source_request = None
        if re.search(r'\bin confluence\b', query.lower()):
            explicit_source_request = "confluence"
        elif re.search(r'\bin remedy\b', query.lower()):
            explicit_source_request = "remedy"
        elif re.search(r'\bin both\b', query.lower()):
            explicit_source_request = "both"
            
        if explicit_source_request:
            target_source = explicit_source_request
            
        return {
            "query": query,
            "normalized_query": normalized_query,
            "query_type": query_type,
            "intent": primary_intent,
            "intent_scores": intent_scores,
            "complexity": complexity,
            "source_indicators": source_indicators,
            "target_source": target_source,
            "explicit_source_request": explicit_source_request
        }

class EntityExtractor:
    """
    Extracts entities from queries for enhanced retrieval.
    """
    
    def __init__(self):
        """Initialize entity extractor."""
        # Ticket pattern (for Remedy)
        self.ticket_pattern = r'(?:INC|PRB|CRQ|SRQ|WO)[0-9]{6,10}'
        
        # Common entity types
        self.entity_patterns = {
            "ticket": [
                self.ticket_pattern,
                r'ticket\s+(?:number\s+)?([A-Z0-9\-]+)'
            ],
            "person": [
                r'(?:assigned to|by|from|of)\s+([A-Z][a-z]+\s+[A-Z][a-z]+)'
            ],
            "date": [
                r'(?:on|before|after|since|from|until)\s+(\d{1,2}[/-]\d{1,2}(?:[/-]\d{2,4})?)',
                r'(?:on|before|after|since|from|until)\s+([A-Z][a-z]{2,8}\s+\d{1,2}(?:st|nd|rd|th)?(?:,\s+\d{4})?)'
            ],
            "team": [
                r'(?:team|group|department)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
            ],
            "status": [
                r'status\s+(?:is|of|as)\s+([a-z]+)'
            ],
            "priority": [
                r'priority\s+(?:is|of|as)\s+([a-z0-9]+)'
            ]
        }
        
        # Keywords that indicate specific entities
        self.keyword_indicators = {
            "person": ["assigned", "owner", "created by", "updated by", "assignee", "submitter"],
            "team": ["team", "group", "department", "support group", "assignment group"],
            "status": ["status", "state", "progress"],
            "priority": ["priority", "urgency", "severity", "impact"],
            "date": ["created", "updated", "modified", "submitted", "resolved", "closed", "date"]
        }

    def extract_entities(self, query: str) -> Dict[str, List[str]]:
        """
        Extract entities from a query.
        
        Args:
            query: The query to extract entities from
            
        Returns:
            Dictionary with entity types and values
        """
        # Normalize query
        normalized_query = query.lower()
        
        entities = defaultdict(list)
        
        # First check for specific patterns
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, query, re.IGNORECASE)
                for match in matches:
                    if len(match.groups()) > 0:
                        # Get the captured group
                        entity_value = match.group(1)
                    else:
                        # Get the full match
                        entity_value = match.group(0)
                        
                    entities[entity_type].append(entity_value)
        
        # Check for keyword indicators
        for entity_type, keywords in self.keyword_indicators.items():
            for keyword in keywords:
                if keyword in normalized_query:
                    # Try to extract the value that follows the keyword
                    match = re.search(rf'{keyword}\s+(?:is|of|as|by|to)?\s+([A-Za-z0-9\s]+)', normalized_query)
                    if match:
                        entity_value = match.group(1).strip()
                        if entity_value and entity_value not in entities[entity_type]:
                            entities[entity_type].append(entity_value)
        
        # Special case for ticket numbers
        ticket_matches = re.findall(self.ticket_pattern, query)
        for ticket in ticket_matches:
            if ticket not in entities["ticket"]:
                entities["ticket"].append(ticket)
        
        return dict(entities)

class QueryExpander:
    """
    Expands queries with synonyms and related terms.
    """
    
    def __init__(self, synonyms_file: Optional[str] = None):
        """
        Initialize query expander.
        
        Args:
            synonyms_file: Path to synonyms JSON file
        """
        # Default domain-specific synonyms
        self.synonyms = {
            "ticket": ["incident", "case", "issue", "problem", "request"],
            "document": ["article", "page", "wiki", "doc", "knowledge"],
            "guide": ["tutorial", "howto", "instructions", "procedure", "steps"],
            "status": ["state", "progress", "condition", "situation"],
            "fix": ["resolve", "solve", "troubleshoot", "repair", "correct"]
        }
        
        # Load additional synonyms if provided
        if synonyms_file:
            try:
                with open(synonyms_file, 'r') as f:
                    additional_synonyms = json.load(f)
                    self.synonyms.update(additional_synonyms)
            except Exception as e:
                logger.error(f"Error loading synonyms file: {str(e)}")
    
    def expand_query(self, query: str, max_expansions: int = 3) -> str:
        """
        Expand a query with synonyms and related terms.
        
        Args:
            query: The query to expand
            max_expansions: Maximum number of expansion terms to add
            
        Returns:
            Expanded query
        """
        # Normalize and tokenize query
        tokens = query.lower().split()
        
        # Find expansion candidates
        expansion_terms = []
        
        for token in tokens:
            # Strip punctuation
            clean_token = token.strip(string.punctuation)
            
            # Skip short tokens
            if len(clean_token) < 3:
                continue
                
            # Look for synonyms
            for term, synonyms in self.synonyms.items():
                if clean_token == term or clean_token in synonyms:
                    # Add synonyms not already in the query
                    for synonym in synonyms:
                        if synonym != clean_token and synonym not in tokens:
                            expansion_terms.append(synonym)
        
        # Limit number of expansion terms
        if len(expansion_terms) > max_expansions:
            # Prioritize shorter, more common terms
            expansion_terms.sort(key=lambda x: len(x))
            expansion_terms = expansion_terms[:max_expansions]
            
        # Create expanded query
        if expansion_terms:
            expanded_query = f"{query} {' '.join(expansion_terms)}"
        else:
            expanded_query = query
            
        return expanded_query

class QueryReformulator:
    """
    Reformulates queries for better retrieval from different sources.
    """
    
    def __init__(self):
        """Initialize query reformulator."""
        # Source-specific transformation templates
        self.confluence_templates = {
            "definition": ["what is {entity}", "define {entity}", "{entity} definition"],
            "procedure": ["how to {verb} {entity}", "steps to {verb} {entity}", "process for {verb}ing {entity}"],
            "comparison": ["compare {entity1} and {entity2}", "difference between {entity1} and {entity2}"],
            "factual": ["information about {entity}", "{entity} details", "{entity} documentation"]
        }
        
        self.remedy_templates = {
            "status": ["status of {ticket}", "{ticket} current state", "{entity} progress"],
            "troubleshooting": ["{entity} issue resolution", "fix {entity} problem", "troubleshoot {entity}"],
            "assignment": ["who is assigned to {ticket}", "{entity} owner", "{entity} assignment"],
            "time": ["when was {ticket} {action}", "{entity} timeline", "{entity} SLA"]
        }
        
        # Verb transformation
        self.verb_forms = {}  # e.g., "configure" -> ["configure", "configuring", "configuration"]
    
    def reformulate_for_confluence(self, query: str, 
                                 intent: str, 
                                 entities: Dict[str, List[str]]) -> str:
        """
        Reformulate query for Confluence search.
        
        Args:
            query: Original query
            intent: Query intent
            entities: Extracted entities
            
        Returns:
            Reformulated query
        """
        # If no specific intent or entities, return original query
        if intent == "unknown" or not entities:
            return query
            
        # Get relevant template
        templates = self.confluence_templates.get(intent)
        if not templates:
            return query
            
        # Extract main entity
        main_entity = None
        for entity_type, values in entities.items():
            if values and entity_type not in ["ticket", "date", "status"]:
                main_entity = values[0]
                break
                
        if not main_entity:
            # Try to extract from query
            words = query.lower().split()
            stop_words = ["what", "how", "why", "when", "where", "which", "who", "is", "are", "do", "does"]
            content_words = [w for w in words if w not in stop_words and len(w) > 3]
            
            if content_words:
                main_entity = content_words[0]
            else:
                return query
                
        # Simple verb extraction
        verb = None
        verb_match = re.search(r'(?:how to|steps to|process for) (\w+)', query.lower())
        if verb_match:
            verb = verb_match.group(1)
            
        # Select a template
        template = templates[0]  # Default to first template
        
        # Fill in template
        reformulated = template.format(
            entity=main_entity,
            verb=verb if verb else "use",
            entity1=main_entity,
            entity2=entities.get("entity2", [""])[0] if "entity2" in entities else ""
        )
        
        return reformulated
    
    def reformulate_for_remedy(self, query: str, 
                             intent: str, 
                             entities: Dict[str, List[str]]) -> str:
        """
        Reformulate query for Remedy search.
        
        Args:
            query: Original query
            intent: Query intent
            entities: Extracted entities
            
        Returns:
            Reformulated query
        """
        # If no specific intent or entities, return original query
        if intent == "unknown" or not entities:
            return query
            
        # If we have a ticket number, prioritize it
        if "ticket" in entities and entities["ticket"]:
            ticket = entities["ticket"][0]
            return f"ticket:{ticket}"
            
        # Get relevant template
        templates = self.remedy_templates.get(intent)
        if not templates:
            return query
            
        # Extract main entity
        main_entity = None
        for entity_type, values in entities.items():
            if values:
                main_entity = values[0]
                break
                
        if not main_entity:
            return query
            
        # Extract action for time-related queries
        action = "created"
        if "updated" in query.lower():
            action = "updated"
        elif "closed" in query.lower():
            action = "closed"
        elif "resolved" in query.lower():
            action = "resolved"
            
        # Select a template
        template = templates[0]  # Default to first template
        
        # Fill in template
        reformulated = template.format(
            ticket=entities.get("ticket", [""])[0] if "ticket" in entities else "",
            entity=main_entity,
            action=action
        )
        
        return reformulated

class QueryAnalyzer:
    """
    Main query analysis engine that combines all components.
    """
    
    def __init__(self, synonyms_file: Optional[str] = None):
        """
        Initialize query analyzer.
        
        Args:
            synonyms_file: Path to synonyms JSON file
        """
        self.classifier = QueryClassifier()
        self.entity_extractor = EntityExtractor()
        self.query_expander = QueryExpander(synonyms_file)
        self.reformulator = QueryReformulator()
    
    def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Perform comprehensive query analysis.
        
        Args:
            query: The query to analyze
            
        Returns:
            Dictionary with all query analysis results
        """
        # Basic cleanup
        query = query.strip()
        if not query:
            return {"error": "Empty query"}
            
        # Classify intent
        classification = self.classifier.classify_intent(query)
        
        # Extract entities
        entities = self.entity_extractor.extract_entities(query)
        
        # Expand query
        expanded_query = self.query_expander.expand_query(query)
        
        # Reformulate for different sources
        confluence_query = self.reformulator.reformulate_for_confluence(
            query, classification.get("intent"), entities
        )
        
        remedy_query = self.reformulator.reformulate_for_remedy(
            query, classification.get("intent"), entities
        )
        
        # Compile final result
        result = {
            "original_query": query,
            "expanded_query": expanded_query,
            "classification": classification,
            "entities": entities,
            "source_specific_queries": {
                "confluence": confluence_query,
                "remedy": remedy_query
            }
        }
        
        return result
    
    def route_query(self, query: str) -> Dict[str, Any]:
        """
        Analyze query and determine which sources to query.
        
        Args:
            query: The query to analyze
            
        Returns:
            Dictionary with routing information
        """
        # Analyze query
        analysis = self.analyze_query(query)
        
        # Get target source from classification
        target_source = analysis["classification"]["target_source"]
        
        # Check for explicit source request
        explicit_request = analysis["classification"]["explicit_source_request"]
        if explicit_request:
            target_source = explicit_request
        
        # Set source-specific options
        if target_source == "both":
            sources_to_query = ["confluence", "remedy"]
            primary_source = None
        else:
            sources_to_query = [target_source]
            primary_source = target_source
            
        # Special case: if we have a specific ticket number, always query Remedy first
        if "ticket" in analysis["entities"] and analysis["entities"]["ticket"]:
            if "remedy" not in sources_to_query:
                sources_to_query.append("remedy")
            primary_source = "remedy"
        
        # Determine the best query for each source
        source_queries = {}
        for source in sources_to_query:
            if source == "confluence":
                source_queries[source] = analysis["source_specific_queries"]["confluence"]
            else:  # remedy
                source_queries[source] = analysis["source_specific_queries"]["remedy"]
        
        return {
            "analysis": analysis,
            "sources_to_query": sources_to_query,
            "primary_source": primary_source,
            "source_queries": source_queries
        }













Advanced Table Processing Engine


"""
Advanced table processing engine for extracting, analyzing, and formatting
tabular data from various sources.
"""
import re
import io
import json
import logging
import os
from typing import Dict, Any, List, Optional, Tuple, Union, Set, Generator
import base64
from collections import defaultdict

import numpy as np
import pandas as pd
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class TableDetector:
    """
    Detects tables in various formats (HTML, markdown, plain text).
    """
    
    def __init__(self):
        """Initialize table detector."""
        # HTML table patterns
        self.html_table_pattern = re.compile(r'<table.*?>.*?</table>', re.DOTALL)
        
        # Markdown table patterns
        self.markdown_table_header = re.compile(r'\|(.+)\|\s*\n\|([\s-:]*\|)+')
        
        # Plain text table patterns
        self.aligned_columns_pattern = re.compile(r'(\s*\S+\s+){2,}\S+\s*')
        
    def detect_tables(self, content: str, content_type: str = "auto") -> List[Dict[str, Any]]:
        """
        Detect tables in content.
        
        Args:
            content: The content to search for tables
            content_type: Type of content ('html', 'markdown', 'text', or 'auto')
            
        Returns:
            List of dictionaries with table info (type, position, content)
        """
        if not content:
            return []
            
        # Auto-detect content type if not specified
        if content_type == "auto":
            if '<table' in content:
                content_type = "html"
            elif '|---|' in content or '|---:' in content:
                content_type = "markdown"
            else:
                content_type = "text"
                
        # Detect tables based on content type
        if content_type == "html":
            return self._detect_html_tables(content)
        elif content_type == "markdown":
            return self._detect_markdown_tables(content)
        else:  # text
            return self._detect_text_tables(content)
    
    def _detect_html_tables(self, content: str) -> List[Dict[str, Any]]:
        """Detect HTML tables."""
        tables = []
        
        # Find all table tags
        matches = self.html_table_pattern.finditer(content)
        
        for i, match in enumerate(matches):
            table_html = match.group(0)
            start_pos, end_pos = match.span()
            
            tables.append({
                "type": "html",
                "index": i,
                "start": start_pos,
                "end": end_pos,
                "content": table_html
            })
            
        return tables
    
    def _detect_markdown_tables(self, content: str) -> List[Dict[str, Any]]:
        """Detect Markdown tables."""
        tables = []
        
        # Find markdown tables by looking for header pattern
        matches = self.markdown_table_header.finditer(content)
        
        for i, match in enumerate(matches):
            start_pos = match.start()
            
            # Extract the full table
            table_lines = []
            lines = content[start_pos:].split('\n')
            
            # Add header and separator lines
            table_lines.append(lines[0])
            table_lines.append(lines[1])
            
            # Find data rows
            j = 2
            while j < len(lines) and lines[j].strip().startswith('|') and lines[j].strip().endswith('|'):
                table_lines.append(lines[j])
                j += 1
                
            # Calculate end position
            table_content = '\n'.join(table_lines)
            end_pos = start_pos + len(table_content)
            
            tables.append({
                "type": "markdown",
                "index": i,
                "start": start_pos,
                "end": end_pos,
                "content": table_content
            })
            
        return tables
    
    def _detect_text_tables(self, content: str) -> List[Dict[str, Any]]:
        """Detect plain text tables."""
        tables = []
        
        # Split into lines
        lines = content.split('\n')
        
        # Look for consecutive lines with consistent spacing
        potential_tables = []
        current_table = []
        
        for i, line in enumerate(lines):
            if self._is_table_row(line) and len(line.strip()) > 0:
                current_table.append((i, line))
            else:
                if len(current_table) >= 3:  # Minimum 3 rows for a table
                    potential_tables.append(current_table)
                current_table = []
                
        # Add the last table if it exists
        if len(current_table) >= 3:
            potential_tables.append(current_table)
            
        # Process potential tables
        for i, table_rows in enumerate(potential_tables):
            # Check column alignment
            if self._has_consistent_columns(table_rows):
                start_line = table_rows[0][0]
                end_line = table_rows[-1][0]
                
                # Calculate position in original content
                start_pos = 0
                for j in range(start_line):
                    start_pos += len(lines[j]) + 1
                    
                end_pos = 0
                for j in range(end_line + 1):
                    end_pos += len(lines[j]) + 1
                
                table_content = '\n'.join(line for _, line in table_rows)
                
                tables.append({
                    "type": "text",
                    "index": i,
                    "start": start_pos,
                    "end": end_pos,
                    "content": table_content
                })
                
        return tables
    
    def _is_table_row(self, line: str) -> bool:
        """Check if a line appears to be a row in a text table."""
        # Look for consistent spacing
        return bool(self.aligned_columns_pattern.match(line))
    
    def _has_consistent_columns(self, rows: List[Tuple[int, str]]) -> bool:
        """Check if rows have consistent column alignment."""
        if not rows:
            return False
            
        # Find column positions in the first row
        first_row = rows[0][1]
        column_positions = []
        in_column = False
        
        for i, char in enumerate(first_row):
            if not in_column and char not in ' \t':
                column_positions.append(i)
                in_column = True
            elif in_column and char in ' \t':
                in_column = False
        
        # Check if at least 3 columns
        if len(column_positions) < 2:
            return False
            
        # Check if other rows have similar column positions
        for _, row in rows[1:]:
            row_positions = []
            in_column = False
            
            for i, char in enumerate(row):
                if not in_column and char not in ' \t':
                    row_positions.append(i)
                    in_column = True
                elif in_column and char in ' \t':
                    in_column = False
                    
            # Check similarity of positions
            if len(row_positions) != len(column_positions):
                return False
                
            # Check if columns are aligned (within a tolerance)
            for i in range(len(column_positions)):
                if abs(row_positions[i] - column_positions[i]) > 1:
                    return False
                    
        return True

class TableParser:
    """
    Parses tables from various formats into structured data.
    """
    
    def __init__(self):
        """Initialize table parser."""
        pass
    
    def parse_table(self, table_info: Dict[str, Any]) -> pd.DataFrame:
        """
        Parse a table into a pandas DataFrame.
        
        Args:
            table_info: Table information from TableDetector
            
        Returns:
            DataFrame with parsed table
        """
        table_type = table_info.get("type")
        content = table_info.get("content")
        
        if not content:
            return pd.DataFrame()
            
        if table_type == "html":
            return self._parse_html_table(content)
        elif table_type == "markdown":
            return self._parse_markdown_table(content)
        else:  # text
            return self._parse_text_table(content)
    
    def _parse_html_table(self, html: str) -> pd.DataFrame:
        """Parse HTML table."""
        try:
            # Read HTML using pandas
            tables = pd.read_html(html)
            
            if tables:
                df = tables[0]
                
                # Clean up column names
                df.columns = [str(col).strip() for col in df.columns]
                
                return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Error parsing HTML table: {str(e)}")
            
            # Try using BeautifulSoup as fallback
            try:
                soup = BeautifulSoup(html, 'html.parser')
                table = soup.find('table')
                
                if not table:
                    return pd.DataFrame()
                
                # Extract headers
                headers = []
                th_tags = table.find_all('th')
                if th_tags:
                    headers = [th.get_text().strip() for th in th_tags]
                
                # Extract rows
                rows = []
                tr_tags = table.find_all('tr')
                
                for tr in tr_tags:
                    # Skip header row
                    if tr.find('th') and not tr.find('td'):
                        continue
                        
                    cells = tr.find_all('td')
                    if cells:
                        row = [cell.get_text().strip() for cell in cells]
                        rows.append(row)
                
                if not rows:
                    return pd.DataFrame()
                    
                # Create DataFrame
                if headers and len(headers) == len(rows[0]):
                    df = pd.DataFrame(rows, columns=headers)
                else:
                    df = pd.DataFrame(rows)
                    
                return df
                
            except Exception as nested_e:
                logger.error(f"Error in BeautifulSoup fallback: {str(nested_e)}")
                return pd.DataFrame()
    
    def _parse_markdown_table(self, markdown: str) -> pd.DataFrame:
        """Parse Markdown table."""
        try:
            lines = markdown.strip().split('\n')
            
            if len(lines) < 3:  # Need at least header, separator, and one data row
                return pd.DataFrame()
                
            # Extract headers
            header_line = lines[0]
            headers = [col.strip() for col in header_line.split('|')[1:-1]]
            
            # Extract data rows
            rows = []
            for line in lines[2:]:  # Skip header and separator rows
                if not line.strip():
                    continue
                    
                values = [cell.strip() for cell in line.split('|')[1:-1]]
                
                # Ensure row has correct number of columns
                if len(values) == len(headers):
                    rows.append(values)
                else:
                    # Pad or truncate to match header length
                    if len(values) < len(headers):
                        values.extend([''] * (len(headers) - len(values)))
                    else:
                        values = values[:len(headers)]
                    rows.append(values)
            
            # Create DataFrame
            df = pd.DataFrame(rows, columns=headers)
            return df
            
        except Exception as e:
            logger.error(f"Error parsing Markdown table: {str(e)}")
            return pd.DataFrame()
    
    def _parse_text_table(self, text: str) -> pd.DataFrame:
        """Parse plain text table."""
        try:
            lines = text.strip().split('\n')
            
            if len(lines) < 2:  # Need at least header and one data row
                return pd.DataFrame()
                
            # Find column boundaries
            col_positions = self._find_column_positions(lines)
            
            if not col_positions:
                return pd.DataFrame()
                
            # Extract headers and data
            headers = self._extract_row_values(lines[0], col_positions)
            
            rows = []
            for i in range(1, len(lines)):
                if not lines[i].strip():
                    continue
                    
                values = self._extract_row_values(lines[i], col_positions)
                rows.append(values)
            
            # Create DataFrame
            df = pd.DataFrame(rows, columns=headers)
            return df
            
        except Exception as e:
            logger.error(f"Error parsing text table: {str(e)}")
            return pd.DataFrame()
    
    def _find_column_positions(self, lines: List[str]) -> List[Tuple[int, int]]:
        """
        Find column start and end positions in plain text table.
        
        Returns list of (start, end) tuples for each column.
        """
        if not lines:
            return []
            
        # Find positions where columns start and end
        first_line = lines[0]
        positions = []
        in_column = False
        start = 0
        
        for i, char in enumerate(first_line):
            if not in_column and char not in ' \t':
                start = i
                in_column = True
            elif in_column and char in ' \t':
                positions.append((start, i))
                in_column = False
                
        # Add the last column if it ends at the line end
        if in_column:
            positions.append((start, len(first_line)))
            
        # Verify positions with other lines
        consistent_positions = []
        
        for pos in positions:
            valid = True
            
            # Check if this column has consistent content
            for line in lines[1:]:
                if len(line) <= pos[0]:
                    continue
                    
                # Check if there's actual content in this column
                column_text = line[pos[0]:min(pos[1], len(line))].strip()
                if not column_text:
                    valid = False
                    break
                    
            if valid:
                consistent_positions.append(pos)
                
        return consistent_positions
    
    def _extract_row_values(self, line: str, positions: List[Tuple[int, int]]) -> List[str]:
        """Extract values from a row using column positions."""
        values = []
        
        for start, end in positions:
            if start >= len(line):
                values.append('')
            else:
                value = line[start:min(end, len(line))].strip()
                values.append(value)
                
        return values

class TableAnalyzer:
    """
    Analyzes and extracts insights from tables.
    """
    
    def __init__(self):
        """Initialize table analyzer."""
        pass
    
    def analyze_table(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze a table and extract key insights.
        
        Args:
            df: DataFrame to analyze
            
        Returns:
            Dictionary with analysis results
        """
        if df.empty:
            return {"error": "Empty table"}
            
        result = {
            "dimensions": {
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": list(df.columns)
            },
            "data_types": {},
            "statistics": {},
            "insights": []
        }
        
        # Analyze data types
        dtype_info = {}
        numeric_columns = []
        categorical_columns = []
        date_columns = []
        
        for col in df.columns:
            col_str = str(col)
            if pd.api.types.is_numeric_dtype(df[col]):
                dtype_info[col_str] = "numeric"
                numeric_columns.append(col)
            elif pd.api.types.is_datetime64_dtype(df[col]):
                dtype_info[col_str] = "datetime"
                date_columns.append(col)
            else:
                # Try to detect if it contains dates
                try:
                    # Check if column contains date-like strings
                    date_pattern = r'\d{1,4}[-/]\d{1,2}[-/]\d{1,4}'
                    if df[col].astype(str).str.match(date_pattern).any():
                        pd.to_datetime(df[col], errors='coerce')
                        dtype_info[col_str] = "date"
                        date_columns.append(col)
                    else:
                        dtype_info[col_str] = "categorical"
                        categorical_columns.append(col)
                except:
                    dtype_info[col_str] = "categorical"
                    categorical_columns.append(col)
                    
        result["data_types"] = dtype_info
        
        # Calculate statistics
        stats = {}
        
        # Numeric columns
        for col in numeric_columns:
            col_str = str(col)
            try:
                col_stats = {
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "median": float(df[col].median()),
                    "std": float(df[col].std())
                }
                
                stats[col_str] = col_stats
            except:
                # Skip if errors
                pass
                
        # Categorical columns
        for col in categorical_columns:
            col_str = str(col)
            try:
                value_counts = df[col].value_counts().to_dict()
                
                # Limit to top 10 values
                if len(value_counts) > 10:
                    # Sort by frequency
                    sorted_items = sorted(value_counts.items(), key=lambda x: x[1], reverse=True)
                    value_counts = dict(sorted_items[:10])
                    
                col_stats = {
                    "unique_values": df[col].nunique(),
                    "most_common": value_counts
                }
                
                stats[col_str] = col_stats
            except:
                # Skip if errors
                pass
                
        # Date columns
        for col in date_columns:
            col_str = str(col)
            try:
                # Convert to datetime if needed
                if not pd.api.types.is_datetime64_dtype(df[col]):
                    dates = pd.to_datetime(df[col], errors='coerce')
                else:
                    dates = df[col]
                    
                col_stats = {
                    "min": dates.min().strftime('%Y-%m-%d') if not pd.isna(dates.min()) else None,
                    "max": dates.max().strftime('%Y-%m-%d') if not pd.isna(dates.max()) else None,
                    "range_days": (dates.max() - dates.min()).days if not pd.isna(dates.min()) and not pd.isna(dates.max()) else None
                }
                
                stats[col_str] = col_stats
            except:
                # Skip if errors
                pass
                
        result["statistics"] = stats
        
        # Extract insights
        insights = []
        
        # Check if table has summary statistics
        if numeric_columns and categorical_columns:
            insights.append("Table contains both numerical and categorical data, useful for analysis.")
        
        # Check for missing values
        missing_values = df.isnull().sum().sum()
        if missing_values > 0:
            missing_pct = (missing_values / (df.shape[0] * df.shape[1])) * 100
            insights.append(f"Table contains {missing_values} missing values ({missing_pct:.1f}% of all cells).")
        
        # Check for date ranges
        if date_columns:
            for col in date_columns:
                col_str = str(col)
                if col_str in stats and stats[col_str].get("range_days") is not None:
                    range_days = stats[col_str]["range_days"]
                    if range_days > 365:
                        insights.append(f"Date column '{col_str}' spans more than a year ({range_days / 365:.1f} years).")
        
        # Check for outliers in numeric columns
        for col in numeric_columns:
            col_str = str(col)
            if col_str in stats:
                q1 = float(df[col].quantile(0.25))
                q3 = float(df[col].quantile(0.75))
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                if len(outliers) > 0:
                    outlier_pct = (len(outliers) / len(df)) * 100
                    if outlier_pct > 5:
                        insights.append(f"Column '{col_str}' has {len(outliers)} outliers ({outlier_pct:.1f}% of values).")
        
        # Check for skewed distributions
        for col in numeric_columns:
            col_str = str(col)
            try:
                from scipy import stats as sp_stats
                skewness = float(sp_stats.skew(df[col].dropna()))
                
                if abs(skewness) > 1:
                    direction = "right" if skewness > 0 else "left"
                    insights.append(f"Column '{col_str}' has a {direction}-skewed distribution (skewness: {skewness:.2f}).")
            except:
                # Skip if scipy not available
                pass
        
        result["insights"] = insights
        
        return result
    
    def generate_summary(self, df: pd.DataFrame, analysis: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate a human-readable summary of a table.
        
        Args:
            df: DataFrame to summarize
            analysis: Pre-computed analysis (optional)
            
        Returns:
            Summary text
        """
        if df.empty:
            return "Empty table."
            
        if not analysis:
            analysis = self.analyze_table(df)
            
        # Build summary
        lines = []
        
        # Basic dimensions
        rows = analysis["dimensions"]["rows"]
        cols = analysis["dimensions"]["columns"]
        lines.append(f"Table with {rows} rows and {cols} columns.")
        
        # Column types
        numeric_cols = [col for col, type in analysis["data_types"].items() if type == "numeric"]
        date_cols = [col for col, type in analysis["data_types"].items() if type in ["datetime", "date"]]
        categorical_cols = [col for col, type in analysis["data_types"].items() if type == "categorical"]
        
        if numeric_cols:
            lines.append(f"Numeric columns: {', '.join(numeric_cols)}")
        if date_cols:
            lines.append(f"Date columns: {', '.join(date_cols)}")
        if categorical_cols:
            lines.append(f"Categorical columns: {', '.join(categorical_cols[:5])}" + 
                       (f" and {len(categorical_cols) - 5} more" if len(categorical_cols) > 5 else ""))
        
        # Key statistics
        if numeric_cols:
            stats_lines = []
            for col in numeric_cols[:3]:  # Limit to first 3 numeric columns
                if col in analysis["statistics"]:
                    stats = analysis["statistics"][col]
                    stats_line = f" {col}: Range {stats['min']:.2f} to {stats['max']:.2f}, Average {stats['mean']:.2f}"
                    stats_lines.append(stats_line)
                    
            if stats_lines:
                lines.append("Key statistics:")
                lines.extend(stats_lines)
        
        # Most common categories
        if categorical_cols:
            cat_lines = []
            for col in categorical_cols[:2]:  # Limit to first 2 categorical columns
                if col in analysis["statistics"]:
                    stats = analysis["statistics"][col]
                    if "most_common" in stats and stats["most_common"]:
                        top_values = list(stats["most_common"].items())[:3]  # Top 3 values
                        values_text = ", ".join([f"{val} ({count})" for val, count in top_values])
                        cat_line = f" {col}: Top values are {values_text}"
                        cat_lines.append(cat_line)
                        
            if cat_lines:
                lines.append("Most common values:")
                lines.extend(cat_lines)
        
        # Date ranges
        if date_cols:
            date_lines = []
            for col in date_cols:
                if col in analysis["statistics"]:
                    stats = analysis["statistics"][col]
                    if "min" in stats and "max" in stats and stats["min"] and stats["max"]:
                        date_line = f" {col}: From {stats['min']} to {stats['max']}"
                        date_lines.append(date_line)
                        
            if date_lines:
                lines.append("Date ranges:")
                lines.extend(date_lines)
        
        # Add insights
        if analysis["insights"]:
            lines.append("\nKey insights:")
            for insight in analysis["insights"][:3]:  # Limit to top 3 insights
                lines.append(f" {insight}")
        
        return "\n".join(lines)
    
    def extract_key_information(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Extract key information from a table for use in responses.
        
        Args:
            df: DataFrame to analyze
            
        Returns:
            Dictionary with key extracted information
        """
        if df.empty:
            return {}
            
        # Get table analysis
        analysis = self.analyze_table(df)
        
        # Extract key information
        info = {
            "size": f"{analysis['dimensions']['rows']} rows by {analysis['dimensions']['columns']} columns",
            "columns": analysis["dimensions"]["column_names"],
            "statistics": {},
            "key_values": {},
            "summary": self.generate_summary(df, analysis)
        }
        
        # Extract key statistics for numeric columns
        for col, data_type in analysis["data_types"].items():
            if data_type == "numeric" and col in analysis["statistics"]:
                stats = analysis["statistics"][col]
                info["statistics"][col] = {
                    "min": stats["min"],
                    "max": stats["max"],
                    "mean": stats["mean"],
                    "median": stats["median"]
                }
        
        # Extract most common values for categorical columns
        for col, data_type in analysis["data_types"].items():
            if data_type == "categorical" and col in analysis["statistics"]:
                stats = analysis["statistics"][col]
                if "most_common" in stats:
                    info["key_values"][col] = stats["most_common"]
        
        # Add sample data
        try:
            # Get first few rows as records
            sample_rows = df.head(5).to_dict(orient='records')
            info["sample_data"] = sample_rows
        except:
            pass
            
        return info

class TableFormatter:
    """
    Formats tables for display in different output formats.
    """
    
    def __init__(self):
        """Initialize table formatter."""
        pass
    
    def format_as_markdown(self, df: pd.DataFrame, max_rows: int = 20, 
                          max_cols: Optional[int] = None) -> str:
        """
        Format table as markdown.
        
        Args:
            df: DataFrame to format
            max_rows: Maximum number of rows to include
            max_cols: Maximum number of columns to include
            
        Returns:
            Markdown-formatted table
        """
        if df.empty:
            return "Empty table"
            
        # Limit rows and columns
        if len(df) > max_rows:
            df = df.head(max_rows)
            
        if max_cols and len(df.columns) > max_cols:
            df = df.iloc[:, :max_cols]
            
        # Format column headers
        headers = [str(col) for col in df.columns]
        header_row = "| " + " | ".join(headers) + " |"
        
        # Format separator row
        separator_row = "| " + " | ".join(["---"] * len(headers)) + " |"
        
        # Format data rows
        data_rows = []
        for _, row in df.iterrows():
            formatted_values = []
            for val in row:
                # Format different types appropriately
                if pd.isna(val):
                    formatted_values.append("")
                elif isinstance(val, (int, float)):
                    if isinstance(val, int):
                        formatted_values.append(str(val))
                    else:
                        # Limit decimal places for floats
                        formatted_values.append(f"{val:.4g}")
                else:
                    # Escape pipe characters in string values
                    formatted_values.append(str(val).replace("|", "\\|"))
                    
            data_row = "| " + " | ".join(formatted_values) + " |"
            data_rows.append(data_row)
            
        # Combine all rows
        markdown_table = "\n".join([header_row, separator_row] + data_rows)
        
        # Add note if table was truncated
        if len(df) < len(df):
            markdown_table += f"\n\n*Table truncated to {max_rows} rows*"
            
        if max_cols and len(df.columns) < len(df.columns):
            markdown_table += f"\n\n*Table truncated to {max_cols} columns*"
            
        return markdown_table
    
    def format_as_html(self, df: pd.DataFrame, max_rows: int = 50,
                      max_cols: Optional[int] = None, 
                      classes: str = "table table-striped") -> str:
        """
        Format table as HTML.
        
        Args:
            df: DataFrame to format
            max_rows: Maximum number of rows to include
            max_cols: Maximum number of columns to include
            classes: CSS classes for the table
            
        Returns:
            HTML-formatted table
        """
        if df.empty:
            return "<p>Empty table</p>"
            
        # Limit rows and columns
        truncated_rows = False
        truncated_cols = False
        
        if len(df) > max_rows:
            df = df.head(max_rows)
            truncated_rows = True
            
        if max_cols and len(df.columns) > max_cols:
            df = df.iloc[:, :max_cols]
            truncated_cols = True
            
        # Convert to HTML
        html_table = df.to_html(index=False, classes=classes, escape=True)
        
        # Add note if table was truncated
        notes = []
        if truncated_rows:
            notes.append(f"Table truncated to {max_rows} rows")
            
        if truncated_cols:
            notes.append(f"Table truncated to {max_cols} columns")
            
        if notes:
            notes_html = f"<p><em>{'<br>'.join(notes)}</em></p>"
            html_table += notes_html
            
        return html_table
    
    def format_as_ascii(self, df: pd.DataFrame, max_rows: int = 20,
                       max_cols: Optional[int] = None) -> str:
        """
        Format table as ASCII art.
        
        Args:
            df: DataFrame to format
            max_rows: Maximum number of rows to include
            max_cols: Maximum number of columns to include
            
        Returns:
            ASCII art formatted table
        """
        if df.empty:
            return "Empty table"
            
        # Limit rows and columns
        truncated_rows = False
        truncated_cols = False
        
        if len(df) > max_rows:
            df = df.head(max_rows)
            truncated_rows = True
            
        if max_cols and len(df.columns) > max_cols:
            df = df.iloc[:, :max_cols]
            truncated_cols = True
            
        # Get string representation
        result = df.to_string(index=False)
        
        # Add note if table was truncated
        notes = []
        if truncated_rows:
            notes.append(f"Table truncated to {max_rows} rows")
            
        if truncated_cols:
            notes.append(f"Table truncated to {max_cols} columns")
            
        if notes:
            result += "\n\n" + "\n".join(notes)
            
        return result
    
    def format_as_csv(self, df: pd.DataFrame) -> str:
        """
        Format table as CSV.
        
        Args:
            df: DataFrame to format
            
        Returns:
            CSV-formatted table
        """
        if df.empty:
            return ""
            
        # Use pandas to convert to CSV
        csv_data = df.to_csv(index=False)
        return csv_data

class TableProcessor:
    """
    Main class for detecting, parsing, analyzing, and formatting tables.
    """
    
    def __init__(self):
        """Initialize table processor."""
        self.detector = TableDetector()
        self.parser = TableParser()
        self.analyzer = TableAnalyzer()
        self.formatter = TableFormatter()
    
    def process_content(self, content: str, content_type: str = "auto") -> List[Dict[str, Any]]:
        """
        Process content to extract and analyze tables.
        
        Args:
            content: The content to process
            content_type: Type of content ('html', 'markdown', 'text', or 'auto')
            
        Returns:
            List of dictionaries with processed tables
        """
        # Detect tables
        table_infos = self.detector.detect_tables(content, content_type)
        
        if not table_infos:
            return []
            
        processed_tables = []
        
        for table_info in table_infos:
            # Parse table
            df = self.parser.parse_table(table_info)
            
            if df.empty:
                continue
                
            # Analyze table
            analysis = self.analyzer.analyze_table(df)
            
            # Generate summary
            summary = self.analyzer.generate_summary(df, analysis)
            
            # Get key information
            key_info = self.analyzer.extract_key_information(df)
            
            processed_tables.append({
                "type": table_info["type"],
                "index": table_info["index"],
                "dataframe": df,
                "analysis": analysis,
                "summary": summary,
                "key_info": key_info,
                "markdown": self.formatter.format_as_markdown(df),
                "html": self.formatter.format_as_html(df)
            })
            
        return processed_tables
    
    def process_dataframe(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Process a pandas DataFrame.
        
        Args:
            df: DataFrame to process
            
        Returns:
            Dictionary with processed table
        """
        if df.empty:
            return {"error": "Empty DataFrame"}
            
        # Analyze table
        analysis = self.analyzer.analyze_table(df)
        
        # Generate summary
        summary = self.analyzer.generate_summary(df, analysis)
        
        # Get key information
        key_info = self.analyzer.extract_key_information(df)
        
        return {
            "dataframe": df,
            "analysis": analysis,
            "summary": summary,
            "key_info": key_info,
            "markdown": self.formatter.format_as_markdown(df),
            "html": self.formatter.format_as_html(df)
        }
    
    def parse_table_from_html(self, html: str) -> Optional[pd.DataFrame]:
        """
        Parse a table from HTML.
        
        Args:
            html: HTML content
            
        Returns:
            DataFrame with parsed table or None if no table found
        """
        # Detect tables
        table_infos = self.detector.detect_tables(html, "html")
        
        if not table_infos:
            return None
            
        # Parse the first table
        df = self.parser.parse_table(table_infos[0])
        return df
    
    def search_table(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """
        Search a table for relevant information.
        
        Args:
            df: DataFrame to search
            query: Search query
            
        Returns:
            Dictionary with search results
        """
        if df.empty:
            return {"error": "Empty table"}
            
        # Convert query to lowercase for case-insensitive matching
        query_lower = query.lower()
        
        # Split query into words
        query_words = query_lower.split()
        
        # Initialize results
        matching_cells = []
        matching_rows = []
        
        # Search for exact matches in column names
        matching_columns = [col for col in df.columns 
                           if str(col).lower() == query_lower or
                           query_lower in str(col).lower()]
        
        # Search each cell
        for i, row in df.iterrows():
            row_matches = {}
            
            for col in df.columns:
                cell_value = str(row[col])
                
                # Check for exact match
                if query_lower == cell_value.lower():
                    row_matches[str(col)] = {
                        "value": cell_value,
                        "match_type": "exact"
                    }
                # Check for partial match
                elif query_lower in cell_value.lower():
                    row_matches[str(col)] = {
                        "value": cell_value,
                        "match_type": "partial"
                    }
                # Check for word match
                elif any(word in cell_value.lower().split() for word in query_words):
                    row_matches[str(col)] = {
                        "value": cell_value,
                        "match_type": "word"
                    }
            
            if row_matches:
                matching_cells.extend([
                    {
                        "row": i,
                        "column": col,
                        "value": info["value"],
                        "match_type": info["match_type"]
                    }
                    for col, info in row_matches.items()
                ])
                
                matching_rows.append({
                    "row_index": i,
                    "row_data": row.to_dict(),
                    "matching_columns": list(row_matches.keys())
                })
        
        # Create filtered dataframe with matching rows
        filtered_df = None
        if matching_rows:
            filtered_df = df.iloc[[row["row_index"] for row in matching_rows]]
        
        result = {
            "query": query,
            "matching_columns": matching_columns,
            "matching_cells": matching_cells,
            "matching_rows": matching_rows,
            "match_count": len(matching_cells),
            "row_count": len(matching_rows),
            "filtered_dataframe": filtered_df
        }
        
        # Format results if we have matches
        if filtered_df is not None and not filtered_df.empty:
            result["markdown"] = self.formatter.format_as_markdown(filtered_df)
            result["html"] = self.formatter.format_as_html(filtered_df)
            
        return result
    
    def answer_question(self, df: pd.DataFrame, question: str) -> Dict[str, Any]:
        """
        Answer a question about a table.
        
        Args:
            df: DataFrame to query
            question: Question about the table
            
        Returns:
            Dictionary with answer and supporting data
        """
        if df.empty:
            return {"error": "Empty table", "answer": "The table is empty."}
            
        # Get table analysis
        analysis = self.analyzer.analyze_table(df)
        
        # Initialize result
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        # Normalize question
        question_lower = question.lower()
        
        # Check for specific question types
        if "how many" in question_lower:
            # Count question
            result = self._answer_count_question(df, question, analysis)
        elif "what is the highest" in question_lower or "what is the maximum" in question_lower:
            # Maximum value question
            result = self._answer_max_question(df, question, analysis)
        elif "what is the lowest" in question_lower or "what is the minimum" in question_lower:
            # Minimum value question
            result = self._answer_min_question(df, question, analysis)
        elif "what is the average" in question_lower or "what is the mean" in question_lower:
            # Average question
            result = self._answer_average_question(df, question, analysis)
        elif "list" in question_lower or "show" in question_lower:
            # List question
            result = self._answer_list_question(df, question, analysis)
        else:
            # General search
            search_result = self.search_table(df, question)
            
            if search_result["match_count"] > 0:
                # Prepare answer based on search results
                if search_result["match_count"] == 1:
                    # Single match
                    match = search_result["matching_cells"][0]
                    result["answer"] = f"I found '{match['value']}' in the '{match['column']}' column."
                else:
                    # Multiple matches
                    result["answer"] = f"I found {search_result['match_count']} matches for '{question}' in the table."
                    
                result["confidence"] = 0.7
                result["supporting_data"] = search_result["filtered_dataframe"]
            else:
                # No direct matches, provide table summary
                result["answer"] = f"I couldn't find a direct answer to your question. Here's a summary of the table: {self.analyzer.generate_summary(df, analysis)}"
                result["confidence"] = 0.3
                
        return result
    
    def _answer_count_question(self, df: pd.DataFrame, question: str, 
                              analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Answer a count-related question."""
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        question_lower = question.lower()
        
        # Check for "how many rows" type questions
        if "how many rows" in question_lower or "how many records" in question_lower:
            row_count = len(df)
            result["answer"] = f"There are {row_count} rows in the table."
            result["confidence"] = 0.9
            return result
            
        # Check for "how many columns" type questions
        if "how many columns" in question_lower or "how many fields" in question_lower:
            col_count = len(df.columns)
            result["answer"] = f"There are {col_count} columns in the table."
            result["confidence"] = 0.9
            return result
            
        # Look for specific value counts
        for col in df.columns:
            col_str = str(col).lower()
            
            # Check if the column is mentioned in the question
            if col_str in question_lower:
                # Check for specific value
                value_match = re.search(rf"how many (?:rows|records) (?:have|where) {col_str} (?:is|=|equals|contains) ['\"](.*?)['\"]", question_lower)
                
                if value_match:
                    target_value = value_match.group(1)
                    count = len(df[df[col].astype(str).str.lower() == target_value.lower()])
                    
                    result["answer"] = f"There are {count} rows where {col} equals '{target_value}'."
                    result["confidence"] = 0.85
                    
                    # Add supporting data
                    if count > 0:
                        filtered_df = df[df[col].astype(str).str.lower() == target_value.lower()]
                        result["supporting_data"] = filtered_df
                        
                    return result
                
                # General count for a column
                if "how many" in question_lower and col_str in question_lower:
                    # If categorical, count unique values
                    if analysis["data_types"].get(str(col)) == "categorical":
                        unique_count = df[col].nunique()
                        result["answer"] = f"There are {unique_count} unique values in the '{col}' column."
                        result["confidence"] = 0.8
                        return result
        
        # Default count response
        result["answer"] = f"There are {len(df)} rows and {len(df.columns)} columns in the table."
        result["confidence"] = 0.6
        return result
    
    def _answer_max_question(self, df: pd.DataFrame, question: str, 
                            analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Answer a maximum value question."""
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        question_lower = question.lower()
        
        # Look for column name in question
        for col in df.columns:
            col_str = str(col).lower()
            
            # Check if the column is mentioned in the question
            if col_str in question_lower and analysis["data_types"].get(str(col)) == "numeric":
                max_value = float(df[col].max())
                max_row = df[df[col] == max_value].iloc[0]
                
                result["answer"] = f"The highest value in the '{col}' column is {max_value}."
                result["confidence"] = 0.9
                result["supporting_data"] = pd.DataFrame([max_row])
                return result
                
        # Check for generic maximum question
        if "highest" in question_lower or "maximum" in question_lower:
            # Look for any numeric column
            numeric_cols = [col for col, dtype in analysis["data_types"].items() if dtype == "numeric"]
            
            if numeric_cols:
                # Just use the first numeric column
                col = numeric_cols[0]
                max_value = float(df[col].max())
                
                result["answer"] = f"The highest value in the '{col}' column is {max_value}."
                result["confidence"] = 0.7
                return result
                
        result["answer"] = "I couldn't identify which column you're asking about for the maximum value."
        result["confidence"] = 0.4
        return result
    
    def _answer_min_question(self, df: pd.DataFrame, question: str, 
                            analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Answer a minimum value question."""
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        question_lower = question.lower()
        
        # Look for column name in question
        for col in df.columns:
            col_str = str(col).lower()
            
            # Check if the column is mentioned in the question
            if col_str in question_lower and analysis["data_types"].get(str(col)) == "numeric":
                min_value = float(df[col].min())
                min_row = df[df[col] == min_value].iloc[0]
                
                result["answer"] = f"The lowest value in the '{col}' column is {min_value}."
                result["confidence"] = 0.9
                result["supporting_data"] = pd.DataFrame([min_row])
                return result
                
        # Check for generic minimum question
        if "lowest" in question_lower or "minimum" in question_lower:
            # Look for any numeric column
            numeric_cols = [col for col, dtype in analysis["data_types"].items() if dtype == "numeric"]
            
            if numeric_cols:
                # Just use the first numeric column
                col = numeric_cols[0]
                min_value = float(df[col].min())
                
                result["answer"] = f"The lowest value in the '{col}' column is {min_value}."
                result["confidence"] = 0.7
                return result
                
        result["answer"] = "I couldn't identify which column you're asking about for the minimum value."
        result["confidence"] = 0.4
        return result
    
    def _answer_average_question(self, df: pd.DataFrame, question: str, 
                               analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Answer an average value question."""
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        question_lower = question.lower()
        
        # Look for column name in question
        for col in df.columns:
            col_str = str(col).lower()
            
            # Check if the column is mentioned in the question
            if col_str in question_lower and analysis["data_types"].get(str(col)) == "numeric":
                avg_value = float(df[col].mean())
                
                result["answer"] = f"The average value in the '{col}' column is {avg_value:.2f}."
                result["confidence"] = 0.9
                return result
                
        # Check for generic average question
        if "average" in question_lower or "mean" in question_lower:
            # Look for any numeric column
            numeric_cols = [col for col, dtype in analysis["data_types"].items() if dtype == "numeric"]
            
            if numeric_cols:
                # Just use the first numeric column
                col = numeric_cols[0]
                avg_value = float(df[col].mean())
                
                result["answer"] = f"The average value in the '{col}' column is {avg_value:.2f}."
                result["confidence"] = 0.7
                return result
                
        result["answer"] = "I couldn't identify which column you're asking about for the average value."
        result["confidence"] = 0.4
        return result
    
    def _answer_list_question(self, df: pd.DataFrame, question: str, 
                             analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Answer a list or show question."""
        result = {
            "question": question,
            "answer": "",
            "confidence": 0.0,
            "supporting_data": None
        }
        
        question_lower = question.lower()
        
        # Look for column name in the question
        target_col = None
        for col in df.columns:
            col_str = str(col).lower()
            
            if col_str in question_lower:
                target_col = col
                break
                
        if target_col:
            # Check if we're asked to list unique values
            if "unique" in question_lower:
                unique_values = df[target_col].unique()
                
                if len(unique_values) <= 20:  # Limit to reasonable number
                    values_list = ", ".join(str(val) for val in unique_values)
                    result["answer"] = f"Unique values in '{target_col}' column: {values_list}"
                else:
                    result["answer"] = f"There are {len(unique_values)} unique values in '{target_col}' column. Here are the first 10: " + \
                                      ", ".join(str(val) for val in unique_values[:10])
                    
                result["confidence"] = 0.85
                result["supporting_data"] = pd.DataFrame({target_col: unique_values})
                return result
                
            # Check if we're asked for top/most frequent values
            if "top" in question_lower or "most" in question_lower or "frequent" in question_lower:
                value_counts = df[target_col].value_counts().head(10)
                
                values_list = ", ".join(f"{idx}: {count}" for idx, count in value_counts.items())
                result["answer"] = f"Most frequent values in '{target_col}' column: {values_list}"
                result["confidence"] = 0.85
                result["supporting_data"] = value_counts.reset_index()
                return result
                
            # Default to showing column values
            values = df[target_col].head(10).tolist()
            values_list = ", ".join(str(val) for val in values)
            
            result["answer"] = f"Here are the first 10 values from the '{target_col}' column: {values_list}"
            result["confidence"] = 0.8
            result["supporting_data"] = df[[target_col]].head(10)
            return result
            
        # Generic list request, show sample rows
        sample = df.head(5)
        result["answer"] = f"Here's a sample of {len(sample)} rows from the table."
        result["confidence"] = 0.7
        result["supporting_data"] = sample
        return result









RAG System Orchestrator


"""
Main RAG system that orchestrates all components and provides a unified interface.
This system integrates Confluence and Remedy data sources with advanced processing.
"""
import logging
import time
import os
import json
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import threading
from queue import Queue
from pathlib import Path

from config.config import config
from connectors.confluence_connector import ConfluenceConnector
from connectors.remedy_connector import RemedyConnector
from processors.document_processor import DocumentProcessor, Document, ContentType, DocumentContent
from retrieval.hybrid_search import HybridSearchEngine
from retrieval.knowledge_graph import KnowledgeGraph
from nlp.query_analysis import QueryAnalyzer
from nlp.embeddings import EmbeddingModel
from nlp.answer_generation import AnswerGenerator
from utils.cache import CacheManager
from utils.metrics import MetricsTracker
from utils.logger import setup_logger

# Setup logging
logger = setup_logger('rag_system', level=logging.INFO)

class SourceType:
    """Constants for source types."""
    CONFLUENCE = "confluence"
    REMEDY = "remedy"
    BOTH = "both"

class RAGSystem:
    """
    Main RAG system that orchestrates all components.
    """
    
    def __init__(self, data_dir: str = "data"):
        """
        Initialize the RAG system.
        
        Args:
            data_dir: Directory for storing data
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Initializing RAG system...")
        
        # Initialize connectors
        self._init_connectors()
        
        # Initialize processors
        self._init_processors()
        
        # Initialize embedding model
        self.embedding_model = EmbeddingModel()
        
        # Initialize vector search
        self._init_search_engine()
        
        # Initialize knowledge graph
        self.knowledge_graph = KnowledgeGraph()
        
        # Initialize NLP components
        self.query_analyzer = QueryAnalyzer()
        self.answer_generator = AnswerGenerator(
            self.search_engine,
            self.knowledge_graph,
            self.embedding_model
        )
        
        # Initialize cache and metrics
        self.cache_manager = CacheManager(os.path.join(str(self.data_dir), "cache"))
        self.metrics_tracker = MetricsTracker(os.path.join(str(self.data_dir), "metrics"))
        
        # Initialize content index
        self.confluence_content = {}  # doc_id -> Document
        self.remedy_content = {}      # doc_id -> Document
        
        # Status tracking
        self.system_ready = False
        self.indexing_status = {"status": "not_started", "progress": 0, "total": 0}
        
        # For parallel processing
        self.processing_queue = Queue()
        self.processing_workers = []
        self.max_workers = config.get('app', 'max_workers', 4)
        
        logger.info("RAG system initialized")
    
    def _init_connectors(self):
        """Initialize and test data source connectors."""
        # Initialize Confluence connector
        confluence_config = config.get('confluence')
        self.confluence_connector = ConfluenceConnector(
            base_url=confluence_config.get('url'),
            username=confluence_config.get('username'),
            password=confluence_config.get('password'),
            api_token=confluence_config.get('api_token'),
            space_key=confluence_config.get('space_key')
        )
        
        # Initialize Remedy connector
        remedy_config = config.get('remedy')
        self.remedy_connector = RemedyConnector(
            base_url=remedy_config.get('url'),
            username=remedy_config.get('username'),
            password=remedy_config.get('password'),
            server_name=remedy_config.get('server_name')
        )
        
        # Test connections
        logger.info("Testing Confluence connection...")
        self.confluence_available = self.confluence_connector.check_connection()
        logger.info(f"Confluence connection: {'OK' if self.confluence_available else 'FAILED'}")
        
        logger.info("Testing Remedy connection...")
        self.remedy_available = self.remedy_connector.check_connection()
        logger.info(f"Remedy connection: {'OK' if self.remedy_available else 'FAILED'}")
    
    def _init_processors(self):
        """Initialize document processing components."""
        self.document_processor = DocumentProcessor(
            cache_dir=os.path.join(str(self.data_dir), "processor_cache")
        )
    
    def _init_search_engine(self):
        """Initialize the hybrid search engine."""
        self.search_engine = HybridSearchEngine(
            vector_db_path=os.path.join(str(self.data_dir), "vector_db"),
            embedding_model=self.embedding_model
        )
    
    def index_confluence_content(self, space_key: Optional[str] = None, 
                                max_pages: int = 1000) -> int:
        """
        Index Confluence content.
        
        Args:
            space_key: Space key to index (uses configured default if None)
            max_pages: Maximum number of pages to index
            
        Returns:
            Number of documents indexed
        """
        if not self.confluence_available:
            logger.error("Confluence connector is not available")
            return 0
            
        # Use configured space key if not provided
        if not space_key:
            space_key = config.get('confluence', 'space_key')
            
        if not space_key:
            logger.error("No Confluence space key provided")
            return 0
            
        # Update indexing status
        self.indexing_status = {"status": "indexing_confluence", "progress": 0, "total": max_pages}
        
        logger.info(f"Indexing Confluence space: {space_key}")
        
        # Create a callback function to track progress
        def page_callback(page):
            self.indexing_status["progress"] += 1
        
        # Crawl the space
        pages = self.confluence_connector.crawl_space(
            space_key=space_key,
            max_pages=max_pages,
            include_attachments=True,
            callback=page_callback
        )
        
        if not pages:
            logger.warning(f"No pages found in Confluence space {space_key}")
            return 0
            
        # Update indexing status
        self.indexing_status["total"] = len(pages)
        self.indexing_status["progress"] = 0
        
        # Process each page into a Document
        indexed_count = 0
        for i, page in enumerate(pages):
            try:
                # Create a document ID
                doc_id = f"confluence_{page['metadata']['id']}"
                
                # Create a Document
                document = Document(
                    document_id=doc_id,
                    title=page['metadata']['title'],
                    source_path=page['metadata']['url'],
                    metadata=page['metadata']
                )
                
                # Add text content
                if 'text' in page and page['text']:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=page['text'],
                            source_document=doc_id,
                            metadata={"type": "main_content"}
                        )
                    )
                
                # Add table content
                for table in page.get('tables', []):
                    try:
                        # Convert table data to DataFrame
                        import pandas as pd
                        df = pd.DataFrame(table.get('rows', []), columns=table.get('headers', []))
                        
                        document.add_content(
                            DocumentContent(
                                content_type=ContentType.TABLE,
                                content=df,
                                metadata={"title": table.get('title', 'Table')},
                                source_document=doc_id
                            )
                        )
                    except Exception as e:
                        logger.warning(f"Error processing table in {doc_id}: {str(e)}")
                
                # Add code blocks
                for code_block in page.get('code_blocks', []):
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.CODE,
                            content=code_block.get('content', ''),
                            metadata={
                                "language": code_block.get('language'),
                                "title": code_block.get('title')
                            },
                            source_document=doc_id
                        )
                    )
                
                # Add lists
                for list_item in page.get('lists', []):
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.LIST,
                            content=list_item.get('items', []),
                            metadata={
                                "type": list_item.get('type', 'unordered'),
                                "title": list_item.get('title')
                            },
                            source_document=doc_id
                        )
                    )
                
                # Store document
                self.confluence_content[doc_id] = document
                
                # Add to search index
                self._index_document(document)
                
                # Add to knowledge graph
                self._add_to_knowledge_graph(document)
                
                indexed_count += 1
                
                # Update indexing status
                self.indexing_status["progress"] += 1
                
            except Exception as e:
                logger.error(f"Error processing Confluence page {i}: {str(e)}")
        
        logger.info(f"Indexed {indexed_count} Confluence pages")
        
        # Update indexing status
        self.indexing_status["status"] = "indexing_confluence_complete"
        
        return indexed_count
    
    def index_remedy_tickets(self, form_type: str = "incident", 
                           max_tickets: int = 1000,
                           status: Optional[List[str]] = None) -> int:
        """
        Index Remedy tickets.
        
        Args:
            form_type: Type of form to index (incident, change, etc.)
            max_tickets: Maximum number of tickets to index
            status: List of statuses to filter by (None for all)
            
        Returns:
            Number of tickets indexed
        """
        if not self.remedy_available:
            logger.error("Remedy connector is not available")
            return 0
            
        # Update indexing status
        self.indexing_status = {"status": "indexing_remedy", "progress": 0, "total": max_tickets}
        
        logger.info(f"Indexing Remedy {form_type} tickets")
        
        # Get tickets
        if status:
            tickets = self.remedy_connector.get_tickets_by_status(form_type, status, limit=max_tickets)
        else:
            tickets = self.remedy_connector.search_tickets_advanced(form_type, limit=max_tickets)
        
        if not tickets:
            logger.warning(f"No {form_type} tickets found in Remedy")
            return 0
            
        # Update indexing status
        self.indexing_status["total"] = len(tickets)
        
        # Process tickets
        indexed_count = 0
        for i, ticket in enumerate(tickets):
            try:
                # Get full ticket details
                ticket_details = self.remedy_connector.get_ticket_with_details(
                    form_type, 
                    ticket.get('id'),
                    include_history=True
                )
                
                if not ticket_details:
                    continue
                    
                # Create a document ID
                doc_id = f"remedy_{form_type}_{ticket_details.get('id')}"
                
                # Create a Document
                document = Document(
                    document_id=doc_id,
                    title=ticket_details.get('title', f"Ticket {ticket_details.get('id')}"),
                    metadata={
                        "id": ticket_details.get('id'),
                        "type": form_type,
                        "status": ticket_details.get('status'),
                        "priority": ticket_details.get('priority'),
                        "created_at": ticket_details.get('created_at'),
                        "assigned_to": ticket_details.get('assigned_to'),
                        "owner_group": ticket_details.get('owner_group')
                    }
                )
                
                # Add description as text content
                description = ticket_details.get('description')
                if description:
                    document.add_content(
                        DocumentContent(
                            content_type=ContentType.TEXT,
                            content=description,
                            source_document=doc_id,
                            metadata={"type": "description"}
                        )
                    )
                
                # Add history entries
                for j, history in enumerate(ticket_details.get('history', [])):
                    # Only add non-empty entries with meaningful text
                    if history.get('description') and len(history.get('description', '')) > 20:
                        document.add_content(
                            DocumentContent(
                                content_type=ContentType.TEXT,
                                content=history.get('description'),
                                source_document=doc_id,
                                metadata={
                                    "type": "history",
                                    "index": j,
                                    "created_at": history.get('created_at'),
                                    "created_by": history.get('created_by')
                                }
                            )
                        )
                
                # Store document
                self.remedy_content[doc_id] = document
                
                # Add to search index
                self._index_document(document)
                
                # Add to knowledge graph
                self._add_to_knowledge_graph(document)
                
                indexed_count += 1
                
                # Update indexing status
                self.indexing_status["progress"] += 1
                
            except Exception as e:
                logger.error(f"Error processing Remedy ticket {i}: {str(e)}")
        
        logger.info(f"Indexed {indexed_count} Remedy tickets")
        
        # Update indexing status
        self.indexing_status["status"] = "indexing_remedy_complete"
        
        return indexed_count
    
    def _index_document(self, document: Document) -> None:
        """
        Index a document in the search engine.
        
        Args:
            document: Document to index
        """
        try:
            # Add to search index
            self.search_engine.add_document(document)
        except Exception as e:
            logger.error(f"Error indexing document {document.document_id}: {str(e)}")
    
    def _add_to_knowledge_graph(self, document: Document) -> None:
        """
        Add a document to the knowledge graph.
        
        Args:
            document: Document to add
        """
        try:
            # Add document nodes and relationships
            self.knowledge_graph.add_document(document)
        except Exception as e:
            logger.error(f"Error adding document to knowledge graph {document.document_id}: {str(e)}")
    
    def start_indexing_async(self) -> None:
        """Start asynchronous indexing of all content."""
        def indexing_task():
            try:
                # Index Confluence content
                if self.confluence_available:
                    self.index_confluence_content()
                
                # Index Remedy tickets
                if self.remedy_available:
                    # Index incidents
                    self.index_remedy_tickets(form_type="incident")
                    # Index changes
                    self.index_remedy_tickets(form_type="change")
                
                # Mark system as ready
                self.system_ready = True
                self.indexing_status = {"status": "complete", "progress": 100, "total": 100}
                
                logger.info("Indexing complete - system ready")
                
            except Exception as e:
                logger.error(f"Error during indexing: {str(e)}")
                self.indexing_status = {"status": "failed", "error": str(e)}
        
        # Start indexing in a background thread
        indexing_thread = threading.Thread(target=indexing_task)
        indexing_thread.daemon = True
        indexing_thread.start()
        
        logger.info("Started asynchronous indexing")
    
    def start_worker_threads(self) -> None:
        """Start worker threads for parallel processing."""
        def worker():
            while True:
                task_func, args, kwargs, result_queue = self.processing_queue.get()
                try:
                    result = task_func(*args, **kwargs)
                    if result_queue:
                        result_queue.put(result)
                except Exception as e:
                    logger.error(f"Error in worker thread: {str(e)}")
                    if result_queue:
                        result_queue.put({"error": str(e)})
                finally:
                    self.processing_queue.task_done()
        
        # Start worker threads
        for _ in range(self.max_workers):
            thread = threading.Thread(target=worker)
            thread.daemon = True
            thread.start()
            self.processing_workers.append(thread)
            
        logger.info(f"Started {self.max_workers} worker threads")
    
    def get_indexing_status(self) -> Dict[str, Any]:
        """Get current indexing status."""
        return self.indexing_status
    
    def is_ready(self) -> bool:
        """Check if the system is ready to process queries."""
        return self.system_ready
    
    def process_query(self, query: str, source_type: str = SourceType.BOTH) -> Dict[str, Any]:
        """
        Process a user query and generate a response.
        
        Args:
            query: User query
            source_type: Which source to query (confluence, remedy, both)
            
        Returns:
            Dictionary with query response
        """
        # Start timing
        start_time = time.time()
        
        # Check if system is ready
        if not self.system_ready:
            return {
                "success": False,
                "error": "System is not ready yet. Please wait for indexing to complete.",
                "indexing_status": self.indexing_status
            }
            
        logger.info(f"Processing query: '{query}' (source: {source_type})")
        
        # Check cache
        cache_key = f"{query}_{source_type}"
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"Returning cached result for query: '{query}'")
            # Update metrics
            self.metrics_tracker.record_query(
                query, 
                source_type,
                cached_result.get("response_text", ""),
                cached_result.get("context_docs", []),
                cached_result.get("confidence", 0.0),
                time.time() - start_time,
                is_cached=True
            )
            return cached_result
        
        try:
            # 1. Analyze query to understand intent, entities, and complexity
            query_analysis = self.query_analyzer.analyze_query(query)
            
            # 2. Filter sources based on source_type
            source_filter = None
            if source_type == SourceType.CONFLUENCE:
                source_filter = lambda doc_id: doc_id.startswith("confluence_")
            elif source_type == SourceType.REMEDY:
                source_filter = lambda doc_id: doc_id.startswith("remedy_")
            
            # 3. Retrieve relevant documents
            search_results = self.search_engine.search(
                query=query,
                top_k=query_analysis.get("top_k", 10),
                source_filter=source_filter
            )
            
            # 4. Get related entities and facts from knowledge graph
            entities = query_analysis.get("entities", [])
            kg_results = []
            if entities:
                for entity, _ in entities:
                    facts = self.knowledge_graph.get_entity_facts(entity)
                    for fact in facts:
                        kg_results.append({
                            "subject": fact[0],
                            "relation": fact[1],
                            "object": fact[2]
                        })
            
            # Separate results by source for presentation
            confluence_results = [r for r in search_results if r["document_id"].startswith("confluence_")]
            remedy_results = [r for r in search_results if r["document_id"].startswith("remedy_")]
            
            # 5. Generate answer for each source type
            confluence_answer = None
            remedy_answer = None
            combined_answer = None
            
            # Generate source-specific answers
            if source_type in [SourceType.CONFLUENCE, SourceType.BOTH] and confluence_results:
                confluence_answer = self.answer_generator.generate_answer(
                    query, 
                    query_analysis,
                    confluence_results,
                    kg_results
                )
                
            if source_type in [SourceType.REMEDY, SourceType.BOTH] and remedy_results:
                remedy_answer = self.answer_generator.generate_answer(
                    query, 
                    query_analysis,
                    remedy_results,
                    kg_results
                )
                
            # For combined sources, create a unified answer
            if source_type == SourceType.BOTH and confluence_results and remedy_results:
                combined_answer = self.answer_generator.generate_combined_answer(
                    query,
                    query_analysis,
                    confluence_answer,
                    remedy_answer
                )
            
            # 6. Format the final response
            if combined_answer:
                response_text = combined_answer.get("answer_text")
                confidence = combined_answer.get("confidence")
                sources = combined_answer.get("sources")
            elif confluence_answer and remedy_answer:
                # Format as separate sections
                confidence_conf = confluence_answer.get("confidence", 0)
                confidence_remedy = remedy_answer.get("confidence", 0)
                
                response_text = "### Confluence Information\n\n"
                response_text += confluence_answer.get("answer_text", "No relevant information found.")
                
                response_text += "\n\n### Remedy Information\n\n"
                response_text += remedy_answer.get("answer_text", "No relevant information found.")
                
                # Average confidence
                confidence = (confidence_conf + confidence_remedy) / 2
                
                # Combine sources
                sources = (confluence_answer.get("sources", []) + 
                          remedy_answer.get("sources", []))
            elif confluence_answer:
                response_text = confluence_answer.get("answer_text")
                confidence = confluence_answer.get("confidence")
                sources = confluence_answer.get("sources")
            elif remedy_answer:
                response_text = remedy_answer.get("answer_text")
                confidence = remedy_answer.get("confidence")
                sources = remedy_answer.get("sources")
            else:
                response_text = "I couldn't find any relevant information to answer your query."
                confidence = 0.0
                sources = []
            
            # 7. Compile final result
            result = {
                "success": True,
                "query": query,
                "response_text": response_text,
                "confidence": confidence,
                "sources": sources,
                "query_analysis": query_analysis,
                "confluence_results_count": len(confluence_results),
                "remedy_results_count": len(remedy_results),
                "knowledge_graph_facts_count": len(kg_results),
                "processing_time": time.time() - start_time
            }
            
            # Store all document IDs for reference
            context_docs = [r["document_id"] for r in search_results]
            result["context_docs"] = context_docs
            
            # 8. Cache result
            self.cache_manager.set(cache_key, result)
            
            # 9. Update metrics
            self.metrics_tracker.record_query(
                query, 
                source_type,
                response_text,
                context_docs,
                confidence,
                time.time() - start_time,
                is_cached=False
            )
            
            logger.info(f"Processed query in {result['processing_time']:.2f}s with confidence {confidence:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"Error processing query '{query}': {str(e)}")
            error_result = {
                "success": False,
                "query": query,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
            
            # Update metrics even for failed queries
            self.metrics_tracker.record_error(
                query, 
                source_type,
                str(e),
                time.time() - start_time
            )
            
            return error_result
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get system status information."""
        # Gather status information
        status = {
            "system_ready": self.system_ready,
            "indexing_status": self.indexing_status,
            "confluence_available": self.confluence_available,
            "remedy_available": self.remedy_available,
            "confluence_docs_count": len(self.confluence_content),
            "remedy_docs_count": len(self.remedy_content),
            "embedding_model": self.embedding_model.get_model_info(),
            "cache_info": self.cache_manager.get_stats(),
            "uptime": self.metrics_tracker.get_uptime(),
            "query_count": self.metrics_tracker.get_query_count(),
            "avg_response_time": self.metrics_tracker.get_avg_response_time(),
            "top_queries": self.metrics_tracker.get_top_queries(5)
        }
        
        return status
    
    def record_feedback(self, query_id: str, rating: int, comments: Optional[str] = None) -> bool:
        """
        Record user feedback on a response.
        
        Args:
            query_id: ID of the query to rate
            rating: Rating (1-5)
            comments: Optional comments
            
        Returns:
            True if feedback was recorded successfully
        """
        return self.metrics_tracker.record_feedback(query_id, rating, comments)
    
    def get_document_by_id(self, doc_id: str) -> Optional[Document]:
        """
        Get a document by ID.
        
        Args:
            doc_id: Document ID
            
        Returns:
            Document object or None if not found
        """
        if doc_id.startswith("confluence_"):
            return self.confluence_content.get(doc_id)
        elif doc_id.startswith("remedy_"):
            return self.remedy_content.get(doc_id)
        else:
            return None
    
    def clear_cache(self) -> bool:
        """
        Clear the response cache.
        
        Returns:
            True if cache was cleared successfully
        """
        return self.cache_manager.clear()
    
    def shutdown(self) -> None:
        """Shutdown the system and clean up resources."""
        logger.info("Shutting down RAG system...")
        
        # Save the knowledge graph
        try:
            kg_path = os.path.join(str(self.data_dir), "knowledge_graph")
            self.knowledge_graph.save(kg_path)
            logger.info(f"Saved knowledge graph to {kg_path}")
        except Exception as e:
            logger.error(f"Error saving knowledge graph: {str(e)}")
        
        # Save the vector database
        try:
            self.search_engine.save()
            logger.info("Saved search index")
        except Exception as e:
            logger.error(f"Error saving search index: {str(e)}")
        
        # Save metrics
        try:
            self.metrics_tracker.save()
            logger.info("Saved metrics")
        except Exception as e:
            logger.error(f"Error saving metrics: {str(e)}")
        
        logger.info("RAG system shutdown complete")

# Singleton instance
_instance = None

def get_system() -> RAGSystem:
    """Get the RAG system singleton instance."""
    global _instance
    if _instance is None:
        _instance = RAGSystem(data_dir=config.get('app', 'data_dir', 'data'))
    return _instance









Web Application with API Routes


"""
Web application for the RAG system with FastAPI backend.
"""
import os
import json
import logging
import time
from typing import Dict, Any, List, Optional, Union
from fastapi import FastAPI, Request, Response, HTTPException, Depends, Query, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from config.config import config
from rag_system import get_system, SourceType

# Setup logging
logger = logging.getLogger('web_app')
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
logger.addHandler(handler)

# Initialize FastAPI app
app = FastAPI(
    title="Enterprise RAG System",
    description="RAG system integrating Confluence and Remedy data sources with advanced NLP",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins in development
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# Get the directory of the current file
current_dir = os.path.dirname(os.path.abspath(__file__))
static_dir = os.path.join(current_dir, "static")
templates_dir = os.path.join(current_dir, "templates")

# Mount static files directory
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# Initialize templates
templates = Jinja2Templates(directory=templates_dir)

# Define query request model
class QueryRequest(BaseModel):
    query: str
    source_type: str = SourceType.BOTH

# Define feedback request model
class FeedbackRequest(BaseModel):
    query_id: str
    rating: int
    comments: Optional[str] = None

# Get RAG system instance
def get_rag_system():
    return get_system()

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Serve the main index page of the RAG system UI.
    """
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/status")
async def get_status():
    """
    Get the current status of the RAG system.
    """
    system = get_rag_system()
    return system.get_system_status()

@app.get("/indexing-status")
async def get_indexing_status():
    """
    Get the current indexing status.
    """
    system = get_rag_system()
    return system.get_indexing_status()

@app.post("/query")
async def process_query(request: QueryRequest):
    """
    Process a user query and return results.
    """
    system = get_rag_system()
    
    # Check if system is ready
    if not system.is_ready():
        indexing_status = system.get_indexing_status()
        return {
            "success": False,
            "error": "System is still indexing content. Please wait.",
            "indexing_status": indexing_status
        }
    
    # Process the query
    result = system.process_query(request.query, request.source_type)
    
    # Generate a query ID for feedback
    query_id = f"q_{int(time.time())}"
    result["query_id"] = query_id
    
    return result

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    """
    Submit feedback on a query response.
    """
    system = get_rag_system()
    
    success = system.record_feedback(
        request.query_id,
        request.rating,
        request.comments
    )
    
    if not success:
        raise HTTPException(status_code=400, detail="Failed to record feedback")
    
    return {"success": True}

@app.get("/document/{doc_id}")
async def get_document(doc_id: str):
    """
    Get a document by ID.
    """
    system = get_rag_system()
    
    document = system.get_document_by_id(doc_id)
    
    if not document:
        raise HTTPException(status_code=404, detail=f"Document {doc_id} not found")
    
    return document.to_dict()

@app.post("/start-indexing")
async def start_indexing(background_tasks: BackgroundTasks):
    """
    Start the content indexing process.
    """
    system = get_rag_system()
    
    # Add indexing task to background tasks
    background_tasks.add_task(system.start_indexing_async)
    
    return {"status": "Indexing started in the background"}

@app.post("/clear-cache")
async def clear_cache():
    """
    Clear the response cache.
    """
    system = get_rag_system()
    
    success = system.clear_cache()
    
    return {"success": success}

@app.get("/metrics")
async def get_metrics():
    """
    Get system metrics.
    """
    system = get_rag_system()
    
    metrics = {
        "query_count": system.metrics_tracker.get_query_count(),
        "avg_response_time": system.metrics_tracker.get_avg_response_time(),
        "source_distribution": system.metrics_tracker.get_source_distribution(),
        "top_queries": system.metrics_tracker.get_top_queries(10),
        "daily_usage": system.metrics_tracker.get_daily_usage(7),
        "response_time_distribution": system.metrics_tracker.get_response_time_distribution(),
        "feedback_summary": system.metrics_tracker.get_feedback_summary()
    }
    
    return metrics

@app.on_event("startup")
async def startup_event():
    """
    Initialize the system on startup.
    """
    logger.info("Starting RAG system...")
    
    system = get_rag_system()
    
    # Start worker threads
    system.start_worker_threads()
    
    # Start initial indexing if auto_index is enabled
    auto_index = config.get('app', 'auto_index', False)
    if auto_index:
        logger.info("Auto-indexing enabled, starting indexing...")
        system.start_indexing_async()
    
    logger.info("RAG system initialized and ready")

@app.on_event("shutdown")
async def shutdown_event():
    """
    Clean up resources on shutdown.
    """
    logger.info("Shutting down RAG system...")
    
    system = get_rag_system()
    system.shutdown()

def start():
    """Start the web application."""
    host = config.get('app', 'host', '0.0.0.0')
    port = config.get('app', 'port', 8000)
    
    uvicorn.run(
        "web.app:app",
        host=host,
        port=port,
        reload=config.get('app', 'debug', False)
    )

if __name__ == "__main__":
    start()













Modern Web Frontend (HTML/CSS/JS)

"""
HTML template for the RAG system frontend.
This file should be placed in the templates directory.
"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enterprise RAG System</title>
    
    <!-- Tailwind CSS via CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    
    <!-- Highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
    
    <!-- Marked for Markdown rendering -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    
    <!-- Chart.js for visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <style>
        /* Custom styles */
        .typing-animation::after {
            content: '|';
            animation: blink 1s infinite;
        }
        
        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }
        
        /* Source tag styling */
        .source-tag {
            @apply inline-flex items-center px-2 py-1 rounded-md text-xs font-medium mr-2 mb-2;
        }
        
        .source-confluence {
            @apply bg-blue-100 text-blue-800;
        }
        
        .source-remedy {
            @apply bg-green-100 text-green-800;
        }
        
        /* Loading animation */
        .loading-dots::after {
            content: '';
            animation: dots 1.5s infinite;
        }
        
        @keyframes dots {
            0%, 20% { content: '.'; }
            40% { content: '..'; }
            60%, 100% { content: '...'; }
        }
        
        /* Fade-in animation */
        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        /* Markdown rendered content styling */
        .markdown-content h1 {
            @apply text-2xl font-bold mt-6 mb-4;
        }
        
        .markdown-content h2 {
            @apply text-xl font-bold mt-5 mb-3;
        }
        
        .markdown-content h3 {
            @apply text-lg font-bold mt-4 mb-2;
        }
        
        .markdown-content p {
            @apply my-2;
        }
        
        .markdown-content ul {
            @apply list-disc pl-5 my-2;
        }
        
        .markdown-content ol {
            @apply list-decimal pl-5 my-2;
        }
        
        .markdown-content blockquote {
            @apply border-l-4 border-gray-300 pl-4 italic my-3;
        }
        
        .markdown-content code:not(pre code) {
            @apply bg-gray-100 rounded px-1 py-0.5 text-sm;
        }
        
        .markdown-content pre {
            @apply bg-gray-100 rounded p-3 my-3 overflow-x-auto;
        }
        
        .markdown-content table {
            @apply min-w-full border border-gray-300 my-3;
        }
        
        .markdown-content th {
            @apply bg-gray-100 border border-gray-300 px-3 py-2 text-left;
        }
        
        .markdown-content td {
            @apply border border-gray-300 px-3 py-2;
        }
    </style>
</head>
<body class="bg-gray-50 min-h-screen">
    <div class="flex flex-col min-h-screen">
        <!-- Header -->
        <header class="bg-white shadow-sm">
            <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-4 flex justify-between items-center">
                <div class="flex items-center">
                    <h1 class="text-xl font-bold text-gray-900">Enterprise RAG System</h1>
                    <span id="system-status-badge" class="ml-3 px-2 py-1 text-xs rounded-full bg-yellow-100 text-yellow-800">
                        Initializing...
                    </span>
                </div>
                <div class="flex items-center">
                    <button id="metrics-button" class="mr-4 text-gray-600 hover:text-gray-900">
                        <i class="fas fa-chart-line mr-1"></i> Metrics
                    </button>
                    <button id="settings-button" class="text-gray-600 hover:text-gray-900">
                        <i class="fas fa-cog mr-1"></i> Settings
                    </button>
                </div>
            </div>
        </header>
        
        <main class="flex-grow max-w-7xl w-full mx-auto px-4 sm:px-6 lg:px-8 py-6">
            <!-- System status alert (appears when system is indexing) -->
            <div id="indexing-alert" class="mb-6 bg-blue-50 border-l-4 border-blue-500 p-4 hidden">
                <div class="flex items-center">
                    <div class="flex-shrink-0">
                        <i class="fas fa-sync-alt fa-spin text-blue-500"></i>
                    </div>
                    <div class="ml-3">
                        <p class="text-sm text-blue-700">
                            <span id="indexing-message">Indexing content...</span>
                            <span id="indexing-progress" class="ml-2 font-medium"></span>
                        </p>
                    </div>
                </div>
            </div>
            
            <!-- Main query interface -->
            <div class="bg-white shadow-sm rounded-lg p-6 mb-6">
                <h2 class="text-lg font-medium mb-4">Ask a question</h2>
                
                <!-- Source selector -->
                <div class="mb-4">
                    <label class="block text-sm font-medium text-gray-700 mb-1">
                        Data Source
                    </label>
                    <div class="flex space-x-4">
                        <label class="inline-flex items-center">
                            <input type="radio" name="source-type" value="both" class="form-radio h-4 w-4 text-indigo-600" checked>
                            <span class="ml-2">Both</span>
                        </label>
                        <label class="inline-flex items-center">
                            <input type="radio" name="source-type" value="confluence" class="form-radio h-4 w-4 text-indigo-600">
                            <span class="ml-2">Confluence Only</span>
                        </label>
                        <label class="inline-flex items-center">
                            <input type="radio" name="source-type" value="remedy" class="form-radio h-4 w-4 text-indigo-600">
                            <span class="ml-2">Remedy Only</span>
                        </label>
                    </div>
                </div>
                
                <!-- Query input -->
                <div class="relative mt-2">
                    <textarea id="query-input" rows="3" class="block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-4" placeholder="Type your question here..."></textarea>
                    <button id="submit-query" class="absolute bottom-3 right-3 inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500">
                        <i class="fas fa-paper-plane mr-2"></i>
                        Ask
                    </button>
                </div>
                
                <!-- Example queries -->
                <div class="mt-2">
                    <p class="text-sm text-gray-500">Example queries:</p>
                    <div class="flex flex-wrap mt-1">
                        <button class="example-query text-xs bg-gray-100 hover:bg-gray-200 rounded px-2 py-1 mr-2 mb-2">What are the top issues reported this month?</button>
                        <button class="example-query text-xs bg-gray-100 hover:bg-gray-200 rounded px-2 py-1 mr-2 mb-2">Explain our incident escalation process</button>
                        <button class="example-query text-xs bg-gray-100 hover:bg-gray-200 rounded px-2 py-1 mr-2 mb-2">Who owns the customer support knowledge base?</button>
                    </div>
                </div>
            </div>
            
            <!-- Response area -->
            <div id="response-container" class="hidden mb-6">
                <div class="bg-white shadow-sm rounded-lg p-6">
                    <div class="flex items-center justify-between mb-4">
                        <h2 class="text-lg font-medium">Response</h2>
                        <div class="flex items-center">
                            <div id="confidence-container" class="mr-4">
                                <span class="text-sm text-gray-500">Confidence:</span>
                                <span id="confidence-value" class="ml-1 font-medium">-</span>
                            </div>
                            <div id="time-container">
                                <span class="text-sm text-gray-500">Time:</span>
                                <span id="time-value" class="ml-1 font-medium">-</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Loading indicator -->
                    <div id="response-loading" class="py-8 text-center">
                        <div class="inline-block animate-spin rounded-full h-8 w-8 border-t-2 border-b-2 border-indigo-600"></div>
                        <p class="mt-2 text-gray-600">Processing your question<span class="loading-dots"></span></p>
                    </div>
                    
                    <!-- Response content -->
                    <div id="response-content" class="hidden">
                        <div id="query-display" class="bg-gray-50 rounded-md p-3 mb-4">
                            <p class="text-sm text-gray-600 mb-1">Your question:</p>
                            <p id="query-text" class="font-medium"></p>
                        </div>
                        
                        <div class="markdown-content" id="answer-text"></div>
                        
                        <!-- Source citations -->
                        <div id="sources-container" class="mt-4 pt-4 border-t border-gray-200">
                            <p class="text-sm text-gray-500 mb-2">Sources:</p>
                            <div id="sources-list" class="flex flex-wrap"></div>
                        </div>
                        
                        <!-- Feedback buttons -->
                        <div class="mt-6 pt-4 border-t border-gray-200">
                            <p class="text-sm text-gray-500 mb-2">Was this response helpful?</p>
                            <div class="flex items-center">
                                <button id="feedback-positive" class="mr-2 inline-flex items-center px-3 py-2 border border-transparent text-sm font-medium rounded-md text-green-700 bg-green-100 hover:bg-green-200">
                                    <i class="fas fa-thumbs-up mr-2"></i> Yes
                                </button>
                                <button id="feedback-negative" class="mr-2 inline-flex items-center px-3 py-2 border border-transparent text-sm font-medium rounded-md text-red-700 bg-red-100 hover:bg-red-200">
                                    <i class="fas fa-thumbs-down mr-2"></i> No
                                </button>
                                <button id="feedback-comment" class="inline-flex items-center px-3 py-2 border border-gray-300 text-sm font-medium rounded-md text-gray-700 bg-white hover:bg-gray-50">
                                    <i class="fas fa-comment mr-2"></i> Add comment
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Conversation history -->
            <div id="history-container" class="hidden mb-6">
                <h2 class="text-lg font-medium mb-3">Conversation History</h2>
                <div id="history-list" class="space-y-4"></div>
            </div>
        </main>
        
        <!-- Footer -->
        <footer class="bg-white border-t border-gray-200 py-4">
            <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                <p class="text-center text-sm text-gray-500">
                    Enterprise RAG System &copy; 2025 | Powered by advanced NLP
                </p>
            </div>
        </footer>
    </div>
    
    <!-- Modals -->
    
    <!-- Feedback modal -->
    <div id="feedback-modal" class="fixed inset-0 bg-gray-500 bg-opacity-75 flex items-center justify-center hidden z-50">
        <div class="bg-white rounded-lg max-w-md w-full p-6">
            <h3 class="text-lg font-medium mb-4">Provide Feedback</h3>
            <p class="text-sm text-gray-600 mb-4">Your feedback helps us improve our system. Please share your thoughts on the response.</p>
            
            <textarea id="feedback-text" rows="4" class="block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm mb-4" placeholder="What could be improved?"></textarea>
            
            <div class="flex justify-end">
                <button id="feedback-cancel" class="mr-2 inline-flex items-center px-4 py-2 border border-gray-300 text-sm font-medium rounded-md text-gray-700 bg-white hover:bg-gray-50">
                    Cancel
                </button>
                <button id="feedback-submit" class="inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-indigo-600 hover:bg-indigo-700">
                    Submit Feedback
                </button>
            </div>
        </div>
    </div>
    
    <!-- Settings modal -->
    <div id="settings-modal" class="fixed inset-0 bg-gray-500 bg-opacity-75 flex items-center justify-center hidden z-50">
        <div class="bg-white rounded-lg max-w-md w-full p-6">
            <div class="flex justify-between items-center mb-4">
                <h3 class="text-lg font-medium">Settings</h3>
                <button id="settings-close" class="text-gray-400 hover:text-gray-500">
                    <i class="fas fa-times"></i>
                </button>
            </div>
            
            <div class="space-y-4">
                <div>
                    <label class="block text-sm font-medium text-gray-700 mb-1">
                        Indexing Controls
                    </label>
                    <button id="start-indexing" class="inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-indigo-600 hover:bg-indigo-700">
                        <i class="fas fa-sync-alt mr-2"></i> Start Indexing
                    </button>
                </div>
                
                <div>
                    <label class="block text-sm font-medium text-gray-700 mb-1">
                        Cache Controls
                    </label>
                    <button id="clear-cache" class="inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-indigo-600 hover:bg-indigo-700">
                        <i class="fas fa-trash-alt mr-2"></i> Clear Cache
                    </button>
                </div>
                
                <div>
                    <label class="block text-sm font-medium text-gray-700 mb-1">
                        System Information
                    </label>
                    <div class="bg-gray-50 rounded-md p-3">
                        <div class="grid grid-cols-2 gap-2 text-sm">
                            <div>Confluence Status:</div>
                            <div id="confluence-status">-</div>
                            
                            <div>Remedy Status:</div>
                            <div id="remedy-status">-</div>
                            
                            <div>Indexed Documents:</div>
                            <div id="indexed-docs">-</div>
                            
                            <div>Uptime:</div>
                            <div id="system-uptime">-</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Metrics modal -->
    <div id="metrics-modal" class="fixed inset-0 bg-gray-500 bg-opacity-75 flex items-center justify-center hidden z-50">
        <div class="bg-white rounded-lg max-w-4xl w-full p-6">
            <div class="flex justify-between items-center mb-4">
                <h3 class="text-lg font-medium">System Metrics</h3>
                <button id="metrics-close" class="text-gray-400 hover:text-gray-500">
                    <i class="fas fa-times"></i>
                </button>
            </div>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <!-- Query count -->
                <div>
                    <h4 class="text-sm font-medium text-gray-700 mb-2">Usage Statistics</h4>
                    <div class="bg-gray-50 rounded-md p-3">
                        <div class="grid grid-cols-2 gap-2 text-sm">
                            <div>Total Queries:</div>
                            <div id="metric-query-count">-</div>
                            
                            <div>Avg. Response Time:</div>
                            <div id="metric-avg-time">-</div>
                            
                            <div>Confluence Queries:</div>
                            <div id="metric-confluence-count">-</div>
                            
                            <div>Remedy Queries:</div>
                            <div id="metric-remedy-count">-</div>
                        </div>
                    </div>
                </div>
                
                <!-- Response time chart -->
                <div>
                    <h4 class="text-sm font-medium text-gray-700 mb-2">Response Time (last 7 days)</h4>
                    <div class="bg-gray-50 rounded-md p-3 h-48">
                        <canvas id="response-time-chart"></canvas>
                    </div>
                </div>
                
                <!-- Top queries -->
                <div>
                    <h4 class="text-sm font-medium text-gray-700 mb-2">Top Queries</h4>
                    <div class="bg-gray-50 rounded-md p-3 h-48 overflow-y-auto">
                        <ul id="top-queries-list" class="divide-y divide-gray-200">
                            <!-- Populated by JavaScript -->
                        </ul>
                    </div>
                </div>
                
                <!-- Usage by source -->
                <div>
                    <h4 class="text-sm font-medium text-gray-700 mb-2">Usage by Source</h4>
                    <div class="bg-gray-50 rounded-md p-3 h-48 flex justify-center items-center">
                        <canvas id="source-distribution-chart"></canvas>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Error modal -->
    <div id="error-modal" class="fixed inset-0 bg-gray-500 bg-opacity-75 flex items-center justify-center hidden z-50">
        <div class="bg-white rounded-lg max-w-md w-full p-6">
            <div class="flex items-center mb-4">
                <div class="flex-shrink-0">
                    <i class="fas fa-exclamation-triangle text-red-500 text-xl"></i>
                </div>
                <h3 class="ml-3 text-lg font-medium text-gray-900">Error</h3>
            </div>
            
            <div class="mt-2">
                <p id="error-message" class="text-sm text-gray-500"></p>
            </div>
            
            <div class="mt-4">
                <button id="error-close" class="inline-flex justify-center w-full rounded-md border border-transparent shadow-sm px-4 py-2 bg-red-600 text-base font-medium text-white hover:bg-red-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-red-500 sm:text-sm">
                    Close
                </button>
            </div>
        </div>
    </div>
    
    <!-- JavaScript -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Global variables
            let currentQueryId = null;
            let feedbackRating = null;
            let systemReady = false;
            
            // DOM elements
            const queryInput = document.getElementById('query-input');
            const submitButton = document.getElementById('submit-query');
            const responseContainer = document.getElementById('response-container');
            const responseLoading = document.getElementById('response-loading');
            const responseContent = document.getElementById('response-content');
            const queryText = document.getElementById('query-text');
            const answerText = document.getElementById('answer-text');
            const confidenceValue = document.getElementById('confidence-value');
            const timeValue = document.getElementById('time-value');
            const sourcesList = document.getElementById('sources-list');
            const indexingAlert = document.getElementById('indexing-alert');
            const indexingMessage = document.getElementById('indexing-message');
            const indexingProgress = document.getElementById('indexing-progress');
            const systemStatusBadge = document.getElementById('system-status-badge');
            const historyContainer = document.getElementById('history-container');
            const historyList = document.getElementById('history-list');
            
            // Modal elements
            const feedbackModal = document.getElementById('feedback-modal');
            const feedbackText = document.getElementById('feedback-text');
            const feedbackSubmit = document.getElementById('feedback-submit');
            const feedbackCancel = document.getElementById('feedback-cancel');
            const feedbackPositive = document.getElementById('feedback-positive');
            const feedbackNegative = document.getElementById('feedback-negative');
            const feedbackComment = document.getElementById('feedback-comment');
            
            const settingsModal = document.getElementById('settings-modal');
            const settingsButton = document.getElementById('settings-button');
            const settingsClose = document.getElementById('settings-close');
            const startIndexingButton = document.getElementById('start-indexing');
            const clearCacheButton = document.getElementById('clear-cache');
            const confluenceStatus = document.getElementById('confluence-status');
            const remedyStatus = document.getElementById('remedy-status');
            const indexedDocs = document.getElementById('indexed-docs');
            const systemUptime = document.getElementById('system-uptime');
            
            const metricsModal = document.getElementById('metrics-modal');
            const metricsButton = document.getElementById('metrics-button');
            const metricsClose = document.getElementById('metrics-close');
            
            const errorModal = document.getElementById('error-modal');
            const errorMessage = document.getElementById('error-message');
            const errorClose = document.getElementById('error-close');
            
            // Example queries
            const exampleQueries = document.querySelectorAll('.example-query');
            
            // Initialize system status
            checkSystemStatus();
            
            // Set up polling for system status
            setInterval(checkSystemStatus, 5000);
            
            // Event listeners
            submitButton.addEventListener('click', handleSubmitQuery);
            queryInput.addEventListener('keydown', function(event) {
                if (event.key === 'Enter' && event.ctrlKey) {
                    handleSubmitQuery();
                }
            });
            
            // Example query click handlers
            exampleQueries.forEach(button => {
                button.addEventListener('click', function() {
                    queryInput.value = this.textContent;
                    handleSubmitQuery();
                });
            });
            
            // Feedback handlers
            feedbackPositive.addEventListener('click', function() {
                submitFeedback(5);
            });
            
            feedbackNegative.addEventListener('click', function() {
                submitFeedback(1);
            });
            
            feedbackComment.addEventListener('click', function() {
                showFeedbackModal();
            });
            
            feedbackSubmit.addEventListener('click', function() {
                submitFeedbackWithComments(feedbackRating || 3, feedbackText.value);
                hideFeedbackModal();
            });
            
            feedbackCancel.addEventListener('click', hideFeedbackModal);
            
            // Settings modal handlers
            settingsButton.addEventListener('click', function() {
                settingsModal.classList.remove('hidden');
                updateSystemInfo();
            });
            
            settingsClose.addEventListener('click', function() {
                settingsModal.classList.add('hidden');
            });
            
            startIndexingButton.addEventListener('click', startIndexing);
            clearCacheButton.addEventListener('click', clearCache);
            
            // Metrics modal handlers
            metricsButton.addEventListener('click', function() {
                metricsModal.classList.remove('hidden');
                loadMetrics();
            });
            
            metricsClose.addEventListener('click', function() {
                metricsModal.classList.add('hidden');
            });
            
            // Error modal handler
            errorClose.addEventListener('click', function() {
                errorModal.classList.add('hidden');
            });
            
            // Functions
            
            // Check system status
            function checkSystemStatus() {
                fetch('/status')
                    .then(response => response.json())
                    .then(data => {
                        systemReady = data.system_ready;
                        
                        // Update status badge
                        if (systemReady) {
                            systemStatusBadge.textContent = 'Ready';
                            systemStatusBadge.className = 'ml-3 px-2 py-1 text-xs rounded-full bg-green-100 text-green-800';
                            indexingAlert.classList.add('hidden');
                        } else {
                            systemStatusBadge.textContent = 'Indexing';
                            systemStatusBadge.className = 'ml-3 px-2 py-1 text-xs rounded-full bg-yellow-100 text-yellow-800';
                            
                            // Check indexing status
                            checkIndexingStatus();
                        }
                    })
                    .catch(error => {
                        systemStatusBadge.textContent = 'Error';
                        systemStatusBadge.className = 'ml-3 px-2 py-1 text-xs rounded-full bg-red-100 text-red-800';
                        console.error('Error checking system status:', error);
                    });
            }
            
            // Check indexing status
            function checkIndexingStatus() {
                fetch('/indexing-status')
                    .then(response => response.json())
                    .then(data => {
                        if (data.status !== 'not_started' && data.status !== 'complete') {
                            indexingAlert.classList.remove('hidden');
                            
                            // Determine message based on status
                            let message = 'Indexing content...';
                            if (data.status === 'indexing_confluence') {
                                message = 'Indexing Confluence content...';
                            } else if (data.status === 'indexing_remedy') {
                                message = 'Indexing Remedy tickets...';
                            }
                            
                            indexingMessage.textContent = message;
                            
                            // Update progress
                            if (data.total > 0) {
                                const progress = Math.round((data.progress / data.total) * 100);
                                indexingProgress.textContent = `${progress}% (${data.progress}/${data.total})`;
                            } else {
                                indexingProgress.textContent = '';
                            }
                        } else {
                            indexingAlert.classList.add('hidden');
                        }
                    })
                    .catch(error => {
                        console.error('Error checking indexing status:', error);
                    });
            }
            
            // Handle query submission
            function handleSubmitQuery() {
                const query = queryInput.value.trim();
                if (!query) return;
                
                if (!systemReady) {
                    showError('The system is still indexing content. Please wait until indexing is complete.');
                    return;
                }
                
                // Show response container
                responseContainer.classList.remove('hidden');
                responseLoading.classList.remove('hidden');
                responseContent.classList.add('hidden');
                
                // Scroll to response
                responseContainer.scrollIntoView({ behavior: 'smooth' });
                
                // Get source type
                const sourceType = document.querySelector('input[name="source-type"]:checked').value;
                
                // Make API request
                fetch('/query', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        query: query,
                        source_type: sourceType
                    })
                })
                .then(response => response.json())
                .then(data => {
                    if (data.success) {
                        displayResponse(data);
                        addToHistory(query, data);
                    } else {
                        showError(data.error || 'An error occurred processing your query');
                        responseContainer.classList.add('hidden');
                    }
                })
                .catch(error => {
                    console.error('Error submitting query:', error);
                    showError('Failed to communicate with the server');
                    responseContainer.classList.add('hidden');
                });
            }
            
            // Display response
            function displayResponse(data) {
                // Store current query ID for feedback
                currentQueryId = data.query_id;
                feedbackRating = null;
                
                // Show content, hide loading
                responseLoading.classList.add('hidden');
                responseContent.classList.remove('hidden');
                
                // Set query text
                queryText.textContent = data.query;
                
                // Format and display answer using markdown
                answerText.innerHTML = marked.parse(data.response_text);
                
                // Apply syntax highlighting to code blocks
                document.querySelectorAll('pre code').forEach((block) => {
                    hljs.highlightBlock(block);
                });
                
                // Set confidence
                const confidence = data.confidence || 0;
                confidenceValue.textContent = `${(confidence * 100).toFixed(0)}%`;
                
                // Add confidence color
                if (confidence >= 0.8) {
                    confidenceValue.className = 'ml-1 font-medium text-green-600';
                } else if (confidence >= 0.5) {
                    confidenceValue.className = 'ml-1 font-medium text-yellow-600';
                } else {
                    confidenceValue.className = 'ml-1 font-medium text-red-600';
                }
                
                // Set response time
                const responseTime = data.processing_time || 0;
                timeValue.textContent = `${responseTime.toFixed(2)}s`;
                
                // Add sources
                sourcesList.innerHTML = '';
                if (data.sources && data.sources.length > 0) {
                    data.sources.forEach(source => {
                        const sourceTag = document.createElement('span');
                        
                        // Determine source type
                        let sourceType = '';
                        if (source.startsWith('confluence_')) {
                            sourceType = 'confluence';
                        } else if (source.startsWith('remedy_')) {
                            sourceType = 'remedy';
                        }
                        
                        sourceTag.className = `source-tag source-${sourceType}`;
                        sourceTag.textContent = source;
                        sourceTag.addEventListener('click', () => {
                            // View source details (not implemented)
                            console.log('View source:', source);
                        });
                        
                        sourcesList.appendChild(sourceTag);
                    });
                } else {
                    const noSources = document.createElement('p');
                    noSources.className = 'text-sm text-gray-500';
                    noSources.textContent = 'No specific sources cited';
                    sourcesList.appendChild(noSources);
                }
                
                // Show history container
                historyContainer.classList.remove('hidden');
            }
            
            // Add to history
            function addToHistory(query, data) {
                // Create history item
                const historyItem = document.createElement('div');
                historyItem.className = 'bg-gray-50 rounded-md p-3 hover:bg-gray-100';
                
                // Add query text
                const queryElement = document.createElement('p');
                queryElement.className = 'font-medium text-gray-900';
                queryElement.textContent = query;
                historyItem.appendChild(queryElement);
                
                // Add snippet of response
                const snippetElement = document.createElement('p');
                snippetElement.className = 'text-sm text-gray-600 mt-1';
                
                // Get first sentence as snippet
                const firstSentence = data.response_text.split(/[.!?]/)
                                        .filter(s => s.trim().length > 0)[0];
                                        
                snippetElement.textContent = firstSentence ? 
                                            (firstSentence + (firstSentence.endsWith('.') ? '' : '.')) : 
                                            'No response';
                historyItem.appendChild(snippetElement);
                
                // Add source indicators if available
                if (data.sources && data.sources.length > 0) {
                    const sourcesElement = document.createElement('div');
                    sourcesElement.className = 'flex mt-2';
                    
                    // Count sources by type
                    const confluenceSources = data.sources.filter(s => s.startsWith('confluence_')).length;
                    const remedySources = data.sources.filter(s => s.startsWith('remedy_')).length;
                    
                    if (confluenceSources > 0) {
                        const tag = document.createElement('span');
                        tag.className = 'source-tag source-confluence';
                        tag.textContent = `${confluenceSources} Confluence`;
                        sourcesElement.appendChild(tag);
                    }
                    
                    if (remedySources > 0) {
                        const tag = document.createElement('span');
                        tag.className = 'source-tag source-remedy';
                        tag.textContent = `${remedySources} Remedy`;
                        sourcesElement.appendChild(tag);
                    }
                    
                    historyItem.appendChild(sourcesElement);
                }
                
                // Add click handler to re-run query
                historyItem.addEventListener('click', function() {
                    queryInput.value = query;
                    handleSubmitQuery();
                });
                
                // Add to history list
                historyList.prepend(historyItem);
            }
            
            // Show error message
            function showError(message) {
                errorMessage.textContent = message;
                errorModal.classList.remove('hidden');
            }
            
            // Submit feedback
            function submitFeedback(rating) {
                if (!currentQueryId) return;
                
                feedbackRating = rating;
                
                // Send feedback
                fetch('/feedback', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        query_id: currentQueryId,
                        rating: rating
                    })
                })
                .then(response => response.json())
                .then(data => {
                    if (data.success) {
                        // Show a subtle notification
                        const notification = document.createElement('div');
                        notification.className = 'fixed bottom-4 right-4 bg-green-100 border-l-4 border-green-500 text-green-700 p-4 rounded shadow fade-in';
                        notification.textContent = 'Thank you for your feedback!';
                        document.body.appendChild(notification);
                        
                        // Remove after 3 seconds
                        setTimeout(() => {
                            notification.remove();
                        }, 3000);
                    } else {
                        showError('Failed to submit feedback');
                    }
                })
                .catch(error => {
                    console.error('Error submitting feedback:', error);
                    showError('Failed to submit feedback');
                });
            }
            
            // Submit feedback with comments
            function submitFeedbackWithComments(rating, comments) {
                if (!currentQueryId) return;
                
                // Send feedback
                fetch('/feedback', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        query_id: currentQueryId,
                        rating: rating,
                        comments: comments
                    })
                })
                .then(response => response.json())
                .then(data => {
                    if (data.success) {
                        // Show a subtle notification
                        const notification = document.createElement('div');
                        notification.className = 'fixed bottom-4 right-4 bg-green-100 border-l-4 border-green-500 text-green-700 p-4 rounded shadow fade-in';
                        notification.textContent = 'Thank you for your detailed feedback!';
                        document.body.appendChild(notification);
                        
                        // Remove after 3 seconds
                        setTimeout(() => {
                            notification.remove();
                        }, 3000);
                    } else {
                        showError('Failed to submit feedback');
                    }
                })
                .catch(error => {
                    console.error('Error submitting feedback:', error);
                    showError('Failed to submit feedback');
                });
            }
            
            // Show feedback modal
            function showFeedbackModal() {
                feedbackModal.classList.remove('hidden');
                feedbackText.focus();
            }
            
            // Hide feedback modal
            function hideFeedbackModal() {
                feedbackModal.classList.add('hidden');
                feedbackText.value = '';
            }
            
            // Start indexing
            function startIndexing() {
                fetch('/start-indexing', {
                    method: 'POST'
                })
                .then(response => response.json())
                .then(data => {
                    // Show a notification
                    const notification = document.createElement('div');
                    notification.className = 'fixed bottom-4 right-4 bg-blue-100 border-l-4 border-blue-500 text-blue-700 p-4 rounded shadow fade-in';
                    notification.textContent = 'Indexing started. This may take a few minutes.';
                    document.body.appendChild(notification);
                    
                    // Remove after 5 seconds
                    setTimeout(() => {
                        notification.remove();
                    }, 5000);
                    
                    // Update system status
                    systemStatusBadge.textContent = 'Indexing';
                    systemStatusBadge.className = 'ml-3 px-2 py-1 text-xs rounded-full bg-yellow-100 text-yellow-800';
                    
                    // Set a timeout to check indexing status
                    setTimeout(checkIndexingStatus, 1000);
                })
                .catch(error => {
                    console.error('Error starting indexing:', error);
                    showError('Failed to start indexing');
                });
                
                // Close settings modal
                settingsModal.classList.add('hidden');
            }
            
            // Clear cache
            function clearCache() {
                fetch('/clear-cache', {
                    method: 'POST'
                })
                .then(response => response.json())
                .then(data => {
                    if (data.success) {
                        // Show a notification
                        const notification = document.createElement('div');
                        notification.className = 'fixed bottom-4 right-4 bg-green-100 border-l-4 border-green-500 text-green-700 p-4 rounded shadow fade-in';
                        notification.textContent = 'Cache cleared successfully';
                        document.body.appendChild(notification);
                        
                        // Remove after 3 seconds
                        setTimeout(() => {
                            notification.remove();
                        }, 3000);
                    } else {
                        showError('Failed to clear cache');
                    }
                })
                .catch(error => {
                    console.error('Error clearing cache:', error);
                    showError('Failed to clear cache');
                });
                
                // Close settings modal
                settingsModal.classList.add('hidden');
            }
            
            // Update system info
            function updateSystemInfo() {
                fetch('/status')
                    .then(response => response.json())
                    .then(data => {
                        confluenceStatus.textContent = data.confluence_available ? 'Connected' : 'Disconnected';
                        confluenceStatus.className = data.confluence_available ? 'text-green-600' : 'text-red-600';
                        
                        remedyStatus.textContent = data.remedy_available ? 'Connected' : 'Disconnected';
                        remedyStatus.className = data.remedy_available ? 'text-green-600' : 'text-red-600';
                        
                        indexedDocs.textContent = `${data.confluence_docs_count + data.remedy_docs_count} (${data.confluence_docs_count} Confluence, ${data.remedy_docs_count} Remedy)`;
                        
                        systemUptime.textContent = formatUptime(data.uptime);
                    })
                    .catch(error => {
                        console.error('Error updating system info:', error);
                    });
            }
            
            // Format uptime
            function formatUptime(seconds) {
                const days = Math.floor(seconds / 86400);
                const hours = Math.floor((seconds % 86400) / 3600);
                const minutes = Math.floor((seconds % 3600) / 60);
                
                let parts = [];
                if (days > 0) parts.push(`${days}d`);
                if (hours > 0) parts.push(`${hours}h`);
                if (minutes > 0) parts.push(`${minutes}m`);
                
                return parts.length > 0 ? parts.join(' ') : 'Just started';
            }
            
            // Load metrics
            function loadMetrics() {
                fetch('/metrics')
                    .then(response => response.json())
                    .then(data => {
                        // Update metric values
                        document.getElementById('metric-query-count').textContent = data.query_count;
                        document.getElementById('metric-avg-time').textContent = `${data.avg_response_time.toFixed(2)}s`;
                        
                        // Source distribution
                        const confluenceCount = data.source_distribution.confluence || 0;
                        const remedyCount = data.source_distribution.remedy || 0;
                        const bothCount = data.source_distribution.both || 0;
                        
                        document.getElementById('metric-confluence-count').textContent = confluenceCount + bothCount;
                        document.getElementById('metric-remedy-count').textContent = remedyCount + bothCount;
                        
                        // Top queries
                        const topQueriesList = document.getElementById('top-queries-list');
                        topQueriesList.innerHTML = '';
                        
                        if (data.top_queries && data.top_queries.length > 0) {
                            data.top_queries.forEach(item => {
                                const li = document.createElement('li');
                                li.className = 'py-2';
                                li.innerHTML = `
                                    <div class="flex justify-between">
                                        <span class="text-sm">${item.query}</span>
                                        <span class="text-xs text-gray-500">${item.count}x</span>
                                    </div>
                                `;
                                topQueriesList.appendChild(li);
                            });
                        } else {
                            const li = document.createElement('li');
                            li.className = 'py-2 text-center text-sm text-gray-500';
                            li.textContent = 'No queries yet';
                            topQueriesList.appendChild(li);
                        }
                        
                        // Response time chart
                        createResponseTimeChart(data.daily_usage);
                        
                        // Source distribution chart
                        createSourceDistributionChart(data.source_distribution);
                    })
                    .catch(error => {
                        console.error('Error loading metrics:', error);
                    });
            }
            
            // Create response time chart
            function createResponseTimeChart(dailyUsage) {
                const ctx = document.getElementById('response-time-chart').getContext('2d');
                
                // Process data
                const labels = [];
                const queryData = [];
                const timeData = [];
                
                if (dailyUsage) {
                    Object.keys(dailyUsage).sort().forEach(date => {
                        labels.push(date);
                        queryData.push(dailyUsage[date].query_count);
                        timeData.push(dailyUsage[date].avg_response_time);
                    });
                }
                
                // Create chart
                new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: labels,
                        datasets: [
                            {
                                label: 'Queries',
                                data: queryData,
                                backgroundColor: 'rgba(99, 102, 241, 0.2)',
                                borderColor: 'rgba(99, 102, 241, 1)',
                                borderWidth: 2,
                                tension: 0.1,
                                yAxisID: 'y'
                            },
                            {
                                label: 'Avg. Time (s)',
                                data: timeData,
                                backgroundColor: 'rgba(245, 158, 11, 0.2)',
                                borderColor: 'rgba(245, 158, 11, 1)',
                                borderWidth: 2,
                                tension: 0.1,
                                yAxisID: 'y1'
                            }
                        ]
                    },
                    options: {
                        scales: {
                            y: {
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Queries'
                                }
                            },
                            y1: {
                                position: 'right',
                                beginAtZero: true,
                                title: {
                                    display: true,
                                    text: 'Time (s)'
                                }
                            }
                        },
                        plugins: {
                            legend: {
                                position: 'top',
                            }
                        },
                        maintainAspectRatio: false
                    }
                });
            }
            
            // Create source distribution chart
            function createSourceDistributionChart(sourceDistribution) {
                const ctx = document.getElementById('source-distribution-chart').getContext('2d');
                
                // Process data
                const confluenceCount = sourceDistribution.confluence || 0;
                const remedyCount = sourceDistribution.remedy || 0;
                const bothCount = sourceDistribution.both || 0;
                
                // Create chart
                new Chart(ctx, {
                    type: 'doughnut',
                    data: {
                        labels: ['Confluence Only', 'Remedy Only', 'Both Sources'],
                        datasets: [{
                            data: [confluenceCount, remedyCount, bothCount],
                            backgroundColor: [
                                'rgba(59, 130, 246, 0.8)',
                                'rgba(16, 185, 129, 0.8)',
                                'rgba(139, 92, 246, 0.8)'
                            ],
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: {
                            legend: {
                                position: 'bottom',
                            }
                        }
                    }
                });
            }
        });
    </script>
</body>
</html>













Advanced Response Formatter

"""
Advanced response formatter with support for multiple output formats,
source attribution, and confidence scoring.
"""
import logging
import re
import json
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import markdown
import html

logger = logging.getLogger(__name__)

class ResponseFormatter:
    """
    Formats responses from the RAG system with various outputs formats,
    confidence scoring, and source attribution.
    """
    
    def __init__(self, add_timestamps: bool = False, 
                 add_confidence: bool = True,
                 default_output_format: str = "markdown"):
        """
        Initialize the response formatter.
        
        Args:
            add_timestamps: Whether to add timestamps to responses
            add_confidence: Whether to add confidence scores
            default_output_format: Default output format (markdown, html, text)
        """
        self.add_timestamps = add_timestamps
        self.add_confidence = add_confidence
        self.default_output_format = default_output_format
        
        # Output format handlers
        self.format_handlers = {
            "markdown": self._format_markdown,
            "html": self._format_html,
            "text": self._format_text,
            "json": self._format_json
        }
    
    def format_response(self, 
                       answer: str, 
                       source_documents: List[Dict[str, Any]], 
                       confidence_score: float = 0.0,
                       metadata: Optional[Dict[str, Any]] = None,
                       output_format: Optional[str] = None) -> Dict[str, Any]:
        """
        Format a response with sources and confidence.
        
        Args:
            answer: The answer text
            source_documents: List of source documents with metadata
            confidence_score: Confidence score (0.0 to 1.0)
            metadata: Additional metadata to include
            output_format: Output format (markdown, html, text, json)
            
        Returns:
            Formatted response
        """
        # Use default format if not specified
        if not output_format:
            output_format = self.default_output_format
            
        # Validate output format
        if output_format not in self.format_handlers:
            logger.warning(f"Unknown output format: {output_format}, falling back to {self.default_output_format}")
            output_format = self.default_output_format
            
        # Process source documents
        processed_sources = self._process_sources(source_documents)
        
        # Call the appropriate format handler
        formatted_content = self.format_handlers[output_format](
            answer=answer,
            sources=processed_sources,
            confidence_score=confidence_score,
            metadata=metadata or {}
        )
        
        # Create final response
        response = {
            "answer": answer,
            "formatted_answer": formatted_content,
            "confidence": round(confidence_score, 4),
            "sources": processed_sources,
            "output_format": output_format
        }
        
        # Add metadata if provided
        if metadata:
            response["metadata"] = metadata
            
        # Add timestamp if configured
        if self.add_timestamps:
            response["timestamp"] = datetime.now().isoformat()
            
        return response
    
    def _process_sources(self, source_documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Process and deduplicate source documents.
        
        Args:
            source_documents: List of source documents
            
        Returns:
            Processed and filtered sources
        """
        if not source_documents:
            return []
            
        # Track unique sources by ID or URL
        unique_sources = {}
        
        for source in source_documents:
            # Generate a key for deduplication
            if "id" in source:
                key = source["id"]
            elif "url" in source:
                key = source["url"]
            elif "title" in source and "source" in source:
                key = f"{source['source']}:{source['title']}"
            else:
                # Use the whole source as the key
                key = str(source)
                
            # Skip if we already have this source, unless this one has a higher score
            if key in unique_sources and source.get("score", 0) <= unique_sources[key].get("score", 0):
                continue
                
            # Add to unique sources
            unique_sources[key] = source
            
        # Convert back to list and sort by score if available
        sources = list(unique_sources.values())
        sources.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # Limit to top 10 sources
        return sources[:10]
    
    def _format_markdown(self, 
                        answer: str, 
                        sources: List[Dict[str, Any]], 
                        confidence_score: float,
                        metadata: Dict[str, Any]) -> str:
        """Format the response as Markdown."""
        lines = []
        
        # Add the answer
        lines.append(answer)
        
        # Add sources if available
        if sources:
            lines.append("\n\n### Sources")
            for i, source in enumerate(sources):
                source_info = self._format_source_markdown(source, i+1)
                lines.append(source_info)
        
        # Add confidence if configured
        if self.add_confidence and confidence_score > 0:
            confidence_text = self._format_confidence_markdown(confidence_score)
            lines.append(confidence_text)
        
        return "\n".join(lines)
    
    def _format_html(self, 
                    answer: str, 
                    sources: List[Dict[str, Any]], 
                    confidence_score: float,
                    metadata: Dict[str, Any]) -> str:
        """Format the response as HTML."""
        lines = []
        
        # Convert answer to HTML (assuming it might have markdown formatting)
        answer_html = markdown.markdown(answer)
        lines.append(f"<div class='answer'>{answer_html}</div>")
        
        # Add sources if available
        if sources:
            lines.append("<div class='sources'>")
            lines.append("<h3>Sources</h3>")
            lines.append("<ul>")
            for i, source in enumerate(sources):
                source_info = self._format_source_html(source, i+1)
                lines.append(f"<li>{source_info}</li>")
            lines.append("</ul>")
            lines.append("</div>")
        
        # Add confidence if configured
        if self.add_confidence and confidence_score > 0:
            confidence_html = self._format_confidence_html(confidence_score)
            lines.append(confidence_html)
        
        return "\n".join(lines)
    
    def _format_text(self, 
                    answer: str, 
                    sources: List[Dict[str, Any]], 
                    confidence_score: float,
                    metadata: Dict[str, Any]) -> str:
        """Format the response as plain text."""
        lines = []
        
        # Add the answer
        lines.append(answer)
        
        # Add sources if available
        if sources:
            lines.append("\n\nSources:")
            for i, source in enumerate(sources):
                source_info = self._format_source_text(source, i+1)
                lines.append(source_info)
        
        # Add confidence if configured
        if self.add_confidence and confidence_score > 0:
            confidence_text = self._format_confidence_text(confidence_score)
            lines.append(confidence_text)
        
        return "\n".join(lines)
    
    def _format_json(self, 
                    answer: str, 
                    sources: List[Dict[str, Any]], 
                    confidence_score: float,
                    metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Format the response as JSON (returns a dictionary)."""
        # For JSON, we'll return a structured object
        result = {
            "answer": answer,
            "sources": sources,
            "confidence": confidence_score
        }
        
        # Add metadata if provided
        if metadata:
            result["metadata"] = metadata
            
        # Add timestamp if configured
        if self.add_timestamps:
            result["timestamp"] = datetime.now().isoformat()
            
        return result
    
    def _format_source_markdown(self, source: Dict[str, Any], index: int) -> str:
        """Format a source document reference in Markdown."""
        # Start with index
        source_info = f"{index}. "
        
        # Add title if available
        if "title" in source:
            source_info += f"**{source['title']}**"
        
        # Add source information
        if "source" in source:
            source_info += f" ({source['source']})"
            
        # Add URL as a link if available
        if "url" in source:
            # If we already have title, add the link after
            if "title" in source:
                source_info += f" - [{source['url']}]({source['url']})"
            else:
                # Use URL as the link text
                source_info += f"[{source['url']}]({source['url']})"
                
        # Add date if available
        if "date" in source:
            source_info += f" - {source['date']}"
            
        # Add score if available
        if "score" in source:
            # Format as percentage with 2 decimal places
            score_percent = round(source["score"] * 100, 2)
            source_info += f" (Relevance: {score_percent}%)"
            
        # Add section or page if available
        if "section" in source:
            source_info += f" - Section: {source['section']}"
        elif "page" in source:
            source_info += f" - Page: {source['page']}"
            
        return source_info
    
    def _format_source_html(self, source: Dict[str, Any], index: int) -> str:
        """Format a source document reference in HTML."""
        parts = []
        
        # Add title if available
        if "title" in source:
            parts.append(f"<strong>{html.escape(str(source['title']))}</strong>")
        
        # Add source information
        if "source" in source:
            parts.append(f"<span class='source-name'>({html.escape(str(source['source']))})</span>")
            
        # Add URL as a link if available
        if "url" in source:
            parts.append(f"<a href='{html.escape(source['url'])}' target='_blank'>{html.escape(source['url'])}</a>")
                
        # Add date if available
        if "date" in source:
            parts.append(f"<span class='source-date'>{html.escape(str(source['date']))}</span>")
            
        # Add score if available
        if "score" in source:
            # Format as percentage with 2 decimal places
            score_percent = round(source["score"] * 100, 2)
            parts.append(f"<span class='source-score'>Relevance: {score_percent}%</span>")
            
        # Add section or page if available
        if "section" in source:
            parts.append(f"<span class='source-section'>Section: {html.escape(str(source['section']))}</span>")
        elif "page" in source:
            parts.append(f"<span class='source-page'>Page: {html.escape(str(source['page']))}</span>")
            
        return " - ".join(parts)
    
    def _format_source_text(self, source: Dict[str, Any], index: int) -> str:
        """Format a source document reference in plain text."""
        # Start with index
        source_info = f"{index}. "
        
        # Add title if available
        if "title" in source:
            source_info += f"{source['title']}"
        
        # Add source information
        if "source" in source:
            source_info += f" ({source['source']})"
            
        # Add URL if available
        if "url" in source:
            source_info += f" - {source['url']}"
                
        # Add date if available
        if "date" in source:
            source_info += f" - {source['date']}"
            
        # Add section or page if available
        if "section" in source:
            source_info += f" - Section: {source['section']}"
        elif "page" in source:
            source_info += f" - Page: {source['page']}"
            
        return source_info
    
    def _format_confidence_markdown(self, confidence_score: float) -> str:
        """Format confidence score in Markdown."""
        confidence_percent = round(confidence_score * 100, 2)
        
        if confidence_score >= 0.9:
            return f"\n\n**Confidence**: High ({confidence_percent}%)"
        elif confidence_score >= 0.7:
            return f"\n\n**Confidence**: Medium ({confidence_percent}%)"
        else:
            return f"\n\n**Confidence**: Low ({confidence_percent}%)"
    
    def _format_confidence_html(self, confidence_score: float) -> str:
        """Format confidence score in HTML."""
        confidence_percent = round(confidence_score * 100, 2)
        
        if confidence_score >= 0.9:
            confidence_class = "high"
            confidence_level = "High"
        elif confidence_score >= 0.7:
            confidence_class = "medium"
            confidence_level = "Medium"
        else:
            confidence_class = "low"
            confidence_level = "Low"
            
        return f"<div class='confidence confidence-{confidence_class}'><strong>Confidence</strong>: {confidence_level} ({confidence_percent}%)</div>"
    
    def _format_confidence_text(self, confidence_score: float) -> str:
        """Format confidence score in plain text."""
        confidence_percent = round(confidence_score * 100, 2)
        
        if confidence_score >= 0.9:
            return f"\nConfidence: High ({confidence_percent}%)"
        elif confidence_score >= 0.7:
            return f"\nConfidence: Medium ({confidence_percent}%)"
        else:
            return f"\nConfidence: Low ({confidence_percent}%)"


class SourceAttributionManager:
    """
    Manages source attribution for generated responses,
    with in-text citation marking and reference generation.
    """
    
    def __init__(self, citation_style: str = "numbered"):
        """
        Initialize the source attribution manager.
        
        Args:
            citation_style: Style of citations (numbered, author-date, footnote)
        """
        self.citation_style = citation_style
    
    def add_citations(self, text: str, sources: List[Dict[str, Any]]) -> str:
        """
        Add citations to text based on source relevance.
        
        Args:
            text: Text to add citations to
            sources: List of source documents
            
        Returns:
            Text with added citations
        """
        if not sources:
            return text
            
        # Nothing to do if no sources
        if not sources:
            return text
            
        # Map sources to indices
        source_map = {i+1: source for i, source in enumerate(sources)}
        
        # For "numbered" citation style
        if self.citation_style == "numbered":
            # Add numbered citations after sentences where content might be from a source
            for source_idx, source in source_map.items():
                # Get source content
                source_content = source.get("content", "")
                
                if not source_content:
                    continue
                    
                # Find sentences in the text that might be from this source
                text = self._add_numbered_citations(text, source_content, source_idx)
                
        # For "author-date" citation style
        elif self.citation_style == "author-date":
            for source_idx, source in source_map.items():
                # Get author and date
                author = source.get("author", "Unknown")
                date = source.get("date", "n.d.")
                
                if isinstance(date, str) and len(date) >= 4:
                    # Extract year if date is a string
                    year_match = re.search(r'\b(19|20)\d{2}\b', date)
                    if year_match:
                        date = year_match.group(0)
                        
                citation = f"({author}, {date})"
                
                # Get source content
                source_content = source.get("content", "")
                
                if not source_content:
                    continue
                    
                # Find sentences in the text that might be from this source
                text = self._add_author_date_citations(text, source_content, citation)
        
        # For "footnote" citation style
        elif self.citation_style == "footnote":
            footnotes = []
            
            for source_idx, source in source_map.items():
                # Format footnote text
                footnote = self._format_footnote(source)
                footnotes.append(footnote)
                
                # Get source content
                source_content = source.get("content", "")
                
                if not source_content:
                    continue
                    
                # Find sentences in the text that might be from this source
                text = self._add_footnote_citations(text, source_content, source_idx)
                
            # Add footnotes at the end
            if footnotes:
                text += "\n\n---\n"
                for i, footnote in enumerate(footnotes):
                    text += f"[^{i+1}]: {footnote}\n"
        
        return text
    
    def _add_numbered_citations(self, text: str, source_content: str, source_idx: int) -> str:
        """Add numbered citations to text."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        source_sentences = re.split(r'(?<=[.!?])\s+', source_content)
        
        # Create a set of source sentence fragments for faster matching
        source_fragments = set()
        for sentence in source_sentences:
            # Add the whole sentence
            source_fragments.add(sentence.lower())
            
            # Add fragments of the sentence (for partial matching)
            words = sentence.split()
            if len(words) > 5:
                for i in range(len(words) - 4):
                    fragment = " ".join(words[i:i+5]).lower()
                    source_fragments.add(fragment)
        
        # Add citations to sentences that match source content
        cited_sentences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # Check if any source fragment is in this sentence
            should_cite = False
            for fragment in source_fragments:
                if len(fragment) > 10 and fragment in sentence_lower:
                    should_cite = True
                    break
            
            if should_cite:
                # Check if sentence already has a citation
                if re.search(r'\[\d+\]$', sentence):
                    # Add source to existing citation
                    sentence = re.sub(r'\[(\d+)\]$', f'[\\1,{source_idx}]', sentence)
                else:
                    # Add new citation
                    sentence += f" [{source_idx}]"
                    
            cited_sentences.append(sentence)
        
        return " ".join(cited_sentences)
    
    def _add_author_date_citations(self, text: str, source_content: str, citation: str) -> str:
        """Add author-date citations to text."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        source_sentences = re.split(r'(?<=[.!?])\s+', source_content)
        
        # Create a set of source sentence fragments for faster matching
        source_fragments = set()
        for sentence in source_sentences:
            # Add the whole sentence
            source_fragments.add(sentence.lower())
            
            # Add fragments of the sentence (for partial matching)
            words = sentence.split()
            if len(words) > 5:
                for i in range(len(words) - 4):
                    fragment = " ".join(words[i:i+5]).lower()
                    source_fragments.add(fragment)
        
        # Add citations to sentences that match source content
        cited_sentences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # Check if any source fragment is in this sentence
            should_cite = False
            for fragment in source_fragments:
                if len(fragment) > 10 and fragment in sentence_lower:
                    should_cite = True
                    break
            
            if should_cite:
                # Check if sentence already has a citation
                citation_pattern = r'\([^)]+\)$'
                if re.search(citation_pattern, sentence):
                    # Don't add duplicate citation
                    if citation not in sentence:
                        # Remove trailing space and period if present
                        sentence = re.sub(r'\s*\.$', '', sentence)
                        # Add new citation
                        sentence += f" {citation}."
                else:
                    # Remove trailing space and period if present
                    sentence = re.sub(r'\s*\.$', '', sentence)
                    # Add citation
                    sentence += f" {citation}."
                    
            cited_sentences.append(sentence)
        
        return " ".join(cited_sentences)
    
    def _add_footnote_citations(self, text: str, source_content: str, source_idx: int) -> str:
        """Add footnote citations to text."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        source_sentences = re.split(r'(?<=[.!?])\s+', source_content)
        
        # Create a set of source sentence fragments for faster matching
        source_fragments = set()
        for sentence in source_sentences:
            # Add the whole sentence
            source_fragments.add(sentence.lower())
            
            # Add fragments of the sentence (for partial matching)
            words = sentence.split()
            if len(words) > 5:
                for i in range(len(words) - 4):
                    fragment = " ".join(words[i:i+5]).lower()
                    source_fragments.add(fragment)
        
        # Add citations to sentences that match source content
        cited_sentences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # Check if any source fragment is in this sentence
            should_cite = False
            for fragment in source_fragments:
                if len(fragment) > 10 and fragment in sentence_lower:
                    should_cite = True
                    break
            
            if should_cite:
                # Check if sentence already has a citation
                footnote_pattern = r'\[\^(\d+)\]$'
                match = re.search(footnote_pattern, sentence)
                if match:
                    # Don't add duplicate citation
                    footnote_numbers = [int(n) for n in match.group(1).split(',')]
                    if source_idx not in footnote_numbers:
                        # Get existing citation
                        existing_citation = match.group(0)
                        # Remove existing citation
                        sentence = sentence.replace(existing_citation, '')
                        # Add combined citation
                        footnote_numbers.append(source_idx)
                        footnote_numbers.sort()
                        footnote_str = ','.join([str(n) for n in footnote_numbers])
                        sentence += f"[^{footnote_str}]"
                else:
                    # Add new citation
                    sentence += f"[^{source_idx}]"
                    
            cited_sentences.append(sentence)
        
        return " ".join(cited_sentences)
    
    def _format_footnote(self, source: Dict[str, Any]) -> str:
        """Format a source as a footnote."""
        parts = []
        
        # Add author if available
        if "author" in source:
            parts.append(source["author"])
            
        # Add title if available
        if "title" in source:
            parts.append(f""{source['title']}"")
            
        # Add source if available
        if "source" in source:
            parts.append(source["source"])
            
        # Add date if available
        if "date" in source:
            parts.append(source["date"])
            
        # Add URL if available
        if "url" in source:
            parts.append(source["url"])
            
        return ", ".join(parts)
    
    def generate_references(self, sources: List[Dict[str, Any]], style: str = "apa") -> str:
        """
        Generate formatted references list for sources.
        
        Args:
            sources: List of source documents
            style: Citation style (apa, mla, chicago)
            
        Returns:
            Formatted references list
        """
        if not sources:
            return ""
            
        if style == "apa":
            return self._generate_apa_references(sources)
        elif style == "mla":
            return self._generate_mla_references(sources)
        elif style == "chicago":
            return self._generate_chicago_references(sources)
        else:
            return self._generate_generic_references(sources)
    
    def _generate_apa_references(self, sources: List[Dict[str, Any]]) -> str:
        """Generate APA style references."""
        references = []
        
        for source in sources:
            ref_parts = []
            
            # Add author
            author = source.get("author", "")
            if author:
                # If multiple authors, format appropriately
                if isinstance(author, list):
                    if len(author) == 1:
                        ref_parts.append(f"{author[0]}.")
                    elif len(author) == 2:
                        ref_parts.append(f"{author[0]} & {author[1]}.")
                    else:
                        ref_parts.append(f"{author[0]} et al.")
                else:
                    ref_parts.append(f"{author}.")
            
            # Add date
            date = source.get("date", "n.d.")
            if isinstance(date, str) and len(date) >= 4:
                # Extract year if date is a string
                year_match = re.search(r'\b(19|20)\d{2}\b', date)
                if year_match:
                    date = year_match.group(0)
            ref_parts.append(f"({date}).")
            
            # Add title
            title = source.get("title", "")
            if title:
                ref_parts.append(f"{title}.")
            
            # Add source/publisher
            source_name = source.get("source", "")
            if source_name:
                ref_parts.append(f"{source_name}.")
            
            # Add URL
            url = source.get("url", "")
            if url:
                ref_parts.append(f"Retrieved from {url}")
            
            references.append(" ".join(ref_parts))
            
        return "\n\n".join(references)
    
    def _generate_mla_references(self, sources: List[Dict[str, Any]]) -> str:
        """Generate MLA style references."""
        references = []
        
        for source in sources:
            ref_parts = []
            
            # Add author
            author = source.get("author", "")
            if author:
                # If multiple authors, format appropriately
                if isinstance(author, list):
                    if len(author) == 1:
                        ref_parts.append(f"{author[0]}.")
                    elif len(author) == 2:
                        ref_parts.append(f"{author[0]} and {author[1]}.")
                    else:
                        ref_parts.append(f"{author[0]} et al.")
                else:
                    ref_parts.append(f"{author}.")
            
            # Add title
            title = source.get("title", "")
            if title:
                ref_parts.append(f""{title}."")
            
            # Add source/publisher
            source_name = source.get("source", "")
            if source_name:
                ref_parts.append(f"{source_name},")
            
            # Add date
            date = source.get("date", "")
            if date:
                ref_parts.append(date + ",")
            
            # Add URL
            url = source.get("url", "")
            if url:
                ref_parts.append(url + ".")
            
            references.append(" ".join(ref_parts))
            
        return "\n\n".join(references)
    
    def _generate_chicago_references(self, sources: List[Dict[str, Any]]) -> str:
        """Generate Chicago style references."""
        references = []
        
        for source in sources:
            ref_parts = []
            
            # Add author
            author = source.get("author", "")
            if author:
                # If multiple authors, format appropriately
                if isinstance(author, list):
                    if len(author) == 1:
                        ref_parts.append(f"{author[0]}.")
                    elif len(author) == 2:
                        ref_parts.append(f"{author[0]} and {author[1]}.")
                    else:
                        ref_parts.append(f"{author[0]} et al.")
                else:
                    ref_parts.append(f"{author}.")
            
            # Add title
            title = source.get("title", "")
            if title:
                ref_parts.append(f""{title}."")
            
            # Add source/publisher
            source_name = source.get("source", "")
            if source_name:
                ref_parts.append(f"{source_name},")
            
            # Add date
            date = source.get("date", "")
            if date:
                ref_parts.append(date + ".")
            
            # Add URL
            url = source.get("url", "")
            if url:
                ref_parts.append(url + ".")
            
            references.append(" ".join(ref_parts))
            
        return "\n\n".join(references)
    
    def _generate_generic_references(self, sources: List[Dict[str, Any]]) -> str:
        """Generate generic references."""
        references = []
        
        for i, source in enumerate(sources):
            ref_parts = [f"[{i+1}]"]
            
            # Add title
            title = source.get("title", "")
            if title:
                ref_parts.append(f"{title}.")
            
            # Add author
            author = source.get("author", "")
            if author:
                if isinstance(author, list):
                    ref_parts.append(f"By {', '.join(author)}.")
                else:
                    ref_parts.append(f"By {author}.")
            
            # Add source/publisher
            source_name = source.get("source", "")
            if source_name:
                ref_parts.append(f"From {source_name}.")
            
            # Add date
            date = source.get("date", "")
            if date:
                ref_parts.append(f"({date})")
            
            # Add URL
            url = source.get("url", "")
            if url:
                ref_parts.append(f"URL: {url}")
            
            references.append(" ".join(ref_parts))
            
        return "\n".join(references)


class ResponseTemplates:
    """
    Pre-defined templates for various response scenarios.
    """
    
    def __init__(self):
        """Initialize response templates."""
        # Templates for different scenarios
        self.templates = {
            "not_found": [
                "I couldn't find any information about {query} in the knowledge base.",
                "I don't have any specific information about {query} in my sources.",
                "The knowledge base doesn't contain information related to {query}."
            ],
            "low_confidence": [
                "Based on the limited information I have, {answer}",
                "While I'm not entirely confident, here's what I found: {answer}",
                "With some uncertainty, I can tell you that {answer}"
            ],
            "multiple_answers": [
                "There are multiple perspectives on this:\n\n{answers}",
                "The sources provide different viewpoints:\n\n{answers}",
                "I found several answers to your question:\n\n{answers}"
            ],
            "contradiction": [
                "The sources contain contradictory information:\n\n{contradiction}",
                "I found conflicting information in the knowledge base:\n\n{contradiction}",
                "There seems to be differing views on this:\n\n{contradiction}"
            ],
            "uncertain": [
                "I'm not certain, but {answer}",
                "I don't have enough information to give a definitive answer, but {answer}",
                "Based on limited information, {answer}"
            ],
            "partial": [
                "I can partially answer this: {answer}",
                "I have some information about this: {answer}",
                "While I don't have complete information, I can tell you that {answer}"
            ],
            "follow_up": [
                "Would you like to know more about {topic}?",
                "I can provide more details about {topic} if you're interested.",
                "Would you like me to elaborate on {topic}?"
            ]
        }
    
    def get_template(self, template_type: str, **kwargs) -> str:
        """
        Get a template for a specific response type.
        
        Args:
            template_type: Type of template to retrieve
            **kwargs: Variables to fill in the template
            
        Returns:
            Formatted template string
        """
        import random
        
        if template_type not in self.templates:
            return kwargs.get("answer", "")
            
        # Get a random template of this type
        templates = self.templates[template_type]
        template = random.choice(templates)
        
        # Fill in the template with provided variables
        try:
            return template.format(**kwargs)
        except KeyError as e:
            logger.warning(f"Missing template variable: {str(e)}")
            return template


class MultiModalResponseFormatter:
    """
    Formatter for multi-modal responses including text, tables, and visualizations.
    """
    
    def __init__(self):
        """Initialize multi-modal response formatter."""
        pass
    
    def format_tabular_data(self, data, max_rows: int = 10, format_type: str = "markdown") -> str:
        """
        Format tabular data as a table.
        
        Args:
            data: DataFrame or list of lists with tabular data
            max_rows: Maximum number of rows to include
            format_type: Output format (markdown, html, text)
            
        Returns:
            Formatted table string
        """
        import pandas as pd
        
        # Convert to DataFrame if not already
        if not isinstance(data, pd.DataFrame):
            if isinstance(data, list) and data and isinstance(data[0], list):
                # Lists of lists - use first row as headers
                headers = data[0]
                rows = data[1:]
                df = pd.DataFrame(rows, columns=headers)
            elif isinstance(data, list) and data and isinstance(data[0], dict):
                # List of dictionaries
                df = pd.DataFrame(data)
            else:
                # Can't convert to DataFrame
                return "Unable to format data as table"
        else:
            df = data
            
        # Limit rows
        if len(df) > max_rows:
            df = df.head(max_rows)
            footer = f"\n(Showing {max_rows} of {len(df)} rows)"
        else:
            footer = ""
            
        # Format based on type
        if format_type == "markdown":
            table_str = df.to_markdown(index=False)
            if footer:
                table_str += footer
            return table_str
            
        elif format_type == "html":
            table_str = df.to_html(index=False, classes="table table-striped")
            if footer:
                table_str += f"<p><em>{footer}</em></p>"
            return table_str
            
        elif format_type == "text":
            # Simple text table
            col_widths = [max(len(str(x)) for x in df[col].tolist() + [col]) for col in df.columns]
            header = " | ".join(str(col).ljust(width) for col, width in zip(df.columns, col_widths))
            separator = "-+-".join("-" * width for width in col_widths)
            rows = []
            for _, row in df.iterrows():
                rows.append(" | ".join(str(val).ljust(width) for val, width in zip(row, col_widths)))
            
            table_str = header + "\n" + separator + "\n" + "\n".join(rows)
            if footer:
                table_str += footer
            return table_str
            
        else:
            return str(df)
    
    def format_chart_data(self, data, chart_type: str) -> Dict[str, Any]:
        """
        Format data for chart visualization.
        
        Args:
            data: Data for the chart
            chart_type: Type of chart (bar, line, pie, etc.)
            
        Returns:
            Dictionary with chart configuration
        """
        import pandas as pd
        
        # Convert to DataFrame if not already
        if not isinstance(data, pd.DataFrame):
            if isinstance(data, list) and data and isinstance(data[0], dict):
                df = pd.DataFrame(data)
            else:
                # Can't process
                return {"error": "Data format not supported for charts"}
        else:
            df = data
            
        # Prepare chart configuration based on chart type
        if chart_type == "bar":
            # For bar charts, we need categories and values
            if len(df.columns) < 2:
                return {"error": "Need at least two columns for bar chart"}
                
            # Use first column as categories, second as values
            categories = df.iloc[:, 0].tolist()
            values = df.iloc[:, 1].tolist()
            
            return {
                "type": "bar",
                "data": {
                    "labels": categories,
                    "datasets": [{
                        "label": df.columns[1],
                        "data": values
                    }]
                }
            }
            
        elif chart_type == "line":
            # For line charts, we need x and y values
            if len(df.columns) < 2:
                return {"error": "Need at least two columns for line chart"}
                
            # Use first column as x, second as y
            x_values = df.iloc[:, 0].tolist()
            y_values = df.iloc[:, 1].tolist()
            
            return {
                "type": "line",
                "data": {
                    "labels": x_values,
                    "datasets": [{
                        "label": df.columns[1],
                        "data": y_values
                    }]
                }
            }
            
        elif chart_type == "pie":
            # For pie charts, we need categories and values
            if len(df.columns) < 2:
                return {"error": "Need at least two columns for pie chart"}
                
            # Use first column as categories, second as values
            categories = df.iloc[:, 0].tolist()
            values = df.iloc[:, 1].tolist()
            
            return {
                "type": "pie",
                "data": {
                    "labels": categories,
                    "datasets": [{
                        "data": values
                    }]
                }
            }
            
        else:
            return {"error": f"Unsupported chart type: {chart_type}"}
    
    def format_code(self, code: str, language: str) -> str:
        """
        Format code with syntax highlighting.
        
        Args:
            code: Code to format
            language: Programming language
            
        Returns:
            Formatted code string
        """
        # For Markdown output
        return f"```{language}\n{code}\n```"
    
    def format_result_highlights(self, source_texts: List[str], query: str) -> List[Dict[str, Any]]:
        """
        Extract and format highlights from source texts based on query.
        
        Args:
            source_texts: List of source text strings
            query: Query string
            
        Returns:
            List of highlight objects with text and position
        """
        import re
        
        if not source_texts:
            return []
            
        # Extract query keywords (remove common words)
        stop_words = {"the", "a", "an", "and", "in", "on", "at", "of", "to", "for", "with", "by"}
        query_words = set(query.lower().split()) - stop_words
        
        highlights = []
        
        for text in source_texts:
            text_lower = text.lower()
            
            # Look for exact matches to query
            query_lower = query.lower()
            if query_lower in text_lower:
                start = text_lower.find(query_lower)
                end = start + len(query_lower)
                
                # Get some context
                context_start = max(0, start - 50)
                context_end = min(len(text), end + 50)
                
                # Extract context and highlighted text
                context = text[context_start:context_end]
                highlighted_text = text[start:end]
                
                highlights.append({
                    "text": context,
                    "highlight": highlighted_text,
                    "start": start - context_start,
                    "end": end - context_start,
                    "score": 1.0
                })
                continue
            
            # Look for matches to query words
            for word in query_words:
                if len(word) < 4:
                    continue
                    
                # Find matches for this word
                for match in re.finditer(r'\b' + re.escape(word) + r'\b', text_lower):
                    start = match.start()
                    end = match.end()
                    
                    # Get some context
                    context_start = max(0, start - 30)
                    context_end = min(len(text), end + 30)
                    
                    # Extract context and highlighted text
                    context = text[context_start:context_end]
                    highlighted_text = text[start:end]
                    
                    highlights.append({
                        "text": context,
                        "highlight": highlighted_text,
                        "start": start - context_start,
                        "end": end - context_start,
                        "score": 0.7
                    })
        
        # Sort by score (highest first) and return top 5
        highlights.sort(key=lambda x: x["score"], reverse=True)
        return highlights[:5]









User Feedback Collection and Continuous Learning

"""
User feedback collection and continuous learning system to improve RAG performance.
Allows tracking feedback, analyzing patterns, and improving the system over time.
"""
import logging
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union, Set
from collections import defaultdict, Counter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tabulate import tabulate
import dataclasses
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)

@dataclass
class FeedbackEntry:
    """Data class for feedback entries."""
    id: str
    user_id: Optional[str]
    query: str
    response: str
    feedback_type: str  # thumbs_up, thumbs_down, rating, correction
    rating: Optional[float] = None  # 1-5 star rating if provided
    comment: Optional[str] = None  # User comment if provided
    source_docs: List[Dict[str, Any]] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    category: Optional[str] = None  # Feedback category if applicable
    tags: List[str] = field(default_factory=list)  # Tags for easier analysis
    applied: bool = False  # Whether feedback has been applied to improve the system
    retrieval_metrics: Dict[str, Any] = field(default_factory=dict)
    generation_metrics: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "id": self.id,
            "user_id": self.user_id,
            "query": self.query,
            "response": self.response,
            "feedback_type": self.feedback_type,
            "rating": self.rating,
            "comment": self.comment,
            "source_docs": self.source_docs,
            "timestamp": self.timestamp.isoformat(),
            "metadata": self.metadata,
            "category": self.category,
            "tags": self.tags,
            "applied": self.applied,
            "retrieval_metrics": self.retrieval_metrics,
            "generation_metrics": self.generation_metrics
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FeedbackEntry':
        """Create from dictionary."""
        # Parse timestamp if it's a string
        timestamp = data.get("timestamp")
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)
        else:
            timestamp = datetime.now()
            
        return cls(
            id=data["id"],
            user_id=data.get("user_id"),
            query=data["query"],
            response=data["response"],
            feedback_type=data["feedback_type"],
            rating=data.get("rating"),
            comment=data.get("comment"),
            source_docs=data.get("source_docs", []),
            timestamp=timestamp,
            metadata=data.get("metadata", {}),
            category=data.get("category"),
            tags=data.get("tags", []),
            applied=data.get("applied", False),
            retrieval_metrics=data.get("retrieval_metrics", {}),
            generation_metrics=data.get("generation_metrics", {})
        )


class FeedbackStore:
    """Store user feedback in a database-like structure."""
    
    def __init__(self, storage_dir: str = "feedback_data"):
        """
        Initialize the feedback store.
        
        Args:
            storage_dir: Directory to store feedback data
        """
        self.storage_dir = storage_dir
        os.makedirs(storage_dir, exist_ok=True)
        
        # Main storage file
        self.storage_file = os.path.join(storage_dir, "feedback.jsonl")
        
        # Index files
        self.user_index_file = os.path.join(storage_dir, "user_index.json")
        self.query_index_file = os.path.join(storage_dir, "query_index.json")
        self.tag_index_file = os.path.join(storage_dir, "tag_index.json")
        self.category_index_file = os.path.join(storage_dir, "category_index.json")
        
        # In-memory cache
        self.feedback_cache = {}
        self.user_index = defaultdict(list)
        self.query_index = defaultdict(list)
        self.tag_index = defaultdict(list)
        self.category_index = defaultdict(list)
        
        # Load existing data
        self._load_data()
    
    def _load_data(self):
        """Load existing feedback data from disk."""
        # Load feedback entries
        if os.path.exists(self.storage_file):
            with open(self.storage_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            entry_dict = json.loads(line)
                            entry = FeedbackEntry.from_dict(entry_dict)
                            self.feedback_cache[entry.id] = entry
                        except json.JSONDecodeError:
                            logger.warning(f"Couldn't parse feedback entry: {line}")
                        except Exception as e:
                            logger.warning(f"Error loading feedback entry: {str(e)}")
        
        # Load indexes
        self._load_index(self.user_index_file, self.user_index)
        self._load_index(self.query_index_file, self.query_index)
        self._load_index(self.tag_index_file, self.tag_index)
        self._load_index(self.category_index_file, self.category_index)
        
        # If indexes are empty but we have feedback entries, rebuild indexes
        if not self.user_index and self.feedback_cache:
            self._rebuild_indexes()
    
    def _load_index(self, file_path: str, index: defaultdict):
        """Load an index file into a defaultdict."""
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    for key, values in data.items():
                        index[key] = values
            except Exception as e:
                logger.warning(f"Error loading index {file_path}: {str(e)}")
    
    def _save_index(self, file_path: str, index: defaultdict):
        """Save an index to a file."""
        try:
            with open(file_path, 'w') as f:
                json.dump(dict(index), f)
        except Exception as e:
            logger.warning(f"Error saving index {file_path}: {str(e)}")
    
    def _rebuild_indexes(self):
        """Rebuild all indexes from feedback entries."""
        # Clear existing indexes
        self.user_index.clear()
        self.query_index.clear()
        self.tag_index.clear()
        self.category_index.clear()
        
        # Rebuild indexes
        for entry_id, entry in self.feedback_cache.items():
            # User index
            if entry.user_id:
                self.user_index[entry.user_id].append(entry_id)
                
            # Query index - use lowercase, normalized query
            normalized_query = self._normalize_query(entry.query)
            self.query_index[normalized_query].append(entry_id)
            
            # Tag index
            for tag in entry.tags:
                self.tag_index[tag].append(entry_id)
                
            # Category index
            if entry.category:
                self.category_index[entry.category].append(entry_id)
        
        # Save indexes
        self._save_index(self.user_index_file, self.user_index)
        self._save_index(self.query_index_file, self.query_index)
        self._save_index(self.tag_index_file, self.tag_index)
        self._save_index(self.category_index_file, self.category_index)
    
    def _normalize_query(self, query: str) -> str:
        """Normalize query for indexing."""
        # Simple normalization: lowercase and strip whitespace
        return query.lower().strip()
    
    def add_feedback(self, feedback: FeedbackEntry) -> bool:
        """
        Add a feedback entry to the store.
        
        Args:
            feedback: The feedback entry to add
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Add to cache
            self.feedback_cache[feedback.id] = feedback
            
            # Update indexes
            if feedback.user_id:
                self.user_index[feedback.user_id].append(feedback.id)
                
            normalized_query = self._normalize_query(feedback.query)
            self.query_index[normalized_query].append(feedback.id)
            
            for tag in feedback.tags:
                self.tag_index[tag].append(feedback.id)
                
            if feedback.category:
                self.category_index[feedback.category].append(feedback.id)
                
            # Save to storage file
            with open(self.storage_file, 'a') as f:
                f.write(json.dumps(feedback.to_dict()) + '\n')
                
            # Save updated indexes
            self._save_index(self.user_index_file, self.user_index)
            self._save_index(self.query_index_file, self.query_index)
            self._save_index(self.tag_index_file, self.tag_index)
            self._save_index(self.category_index_file, self.category_index)
                
            return True
            
        except Exception as e:
            logger.error(f"Error adding feedback: {str(e)}")
            return False
    
    def get_feedback(self, feedback_id: str) -> Optional[FeedbackEntry]:
        """
        Get a feedback entry by ID.
        
        Args:
            feedback_id: The feedback entry ID
            
        Returns:
            The feedback entry or None if not found
        """
        return self.feedback_cache.get(feedback_id)
    
    def update_feedback(self, feedback: FeedbackEntry) -> bool:
        """
        Update an existing feedback entry.
        
        Args:
            feedback: The updated feedback entry
            
        Returns:
            True if successful, False otherwise
        """
        if feedback.id not in self.feedback_cache:
            return False
            
        try:
            # Get the old entry to update indexes if needed
            old_entry = self.feedback_cache[feedback.id]
            
            # Update the cache
            self.feedback_cache[feedback.id] = feedback
            
            # Update indexes if needed
            if old_entry.user_id != feedback.user_id:
                if old_entry.user_id:
                    self.user_index[old_entry.user_id].remove(feedback.id)
                if feedback.user_id:
                    self.user_index[feedback.user_id].append(feedback.id)
                    
            old_normalized_query = self._normalize_query(old_entry.query)
            new_normalized_query = self._normalize_query(feedback.query)
            if old_normalized_query != new_normalized_query:
                self.query_index[old_normalized_query].remove(feedback.id)
                self.query_index[new_normalized_query].append(feedback.id)
                
            # Update tags
            old_tags = set(old_entry.tags)
            new_tags = set(feedback.tags)
            
            for tag in old_tags - new_tags:
                self.tag_index[tag].remove(feedback.id)
                
            for tag in new_tags - old_tags:
                self.tag_index[tag].append(feedback.id)
                
            # Update category
            if old_entry.category != feedback.category:
                if old_entry.category:
                    self.category_index[old_entry.category].remove(feedback.id)
                if feedback.category:
                    self.category_index[feedback.category].append(feedback.id)
                    
            # Rewrite the entire storage file
            # This is inefficient for large datasets, but simple
            with open(self.storage_file, 'w') as f:
                for entry in self.feedback_cache.values():
                    f.write(json.dumps(entry.to_dict()) + '\n')
                    
            # Save updated indexes
            self._save_index(self.user_index_file, self.user_index)
            self._save_index(self.query_index_file, self.query_index)
            self._save_index(self.tag_index_file, self.tag_index)
            self._save_index(self.category_index_file, self.category_index)
                
            return True
            
        except Exception as e:
            logger.error(f"Error updating feedback: {str(e)}")
            return False
    
    def delete_feedback(self, feedback_id: str) -> bool:
        """
        Delete a feedback entry.
        
        Args:
            feedback_id: The feedback entry ID
            
        Returns:
            True if successful, False otherwise
        """
        if feedback_id not in self.feedback_cache:
            return False
            
        try:
            # Get the entry to update indexes
            entry = self.feedback_cache[feedback_id]
            
            # Remove from indexes
            if entry.user_id:
                self.user_index[entry.user_id].remove(feedback_id)
                
            normalized_query = self._normalize_query(entry.query)
            self.query_index[normalized_query].remove(feedback_id)
            
            for tag in entry.tags:
                self.tag_index[tag].remove(feedback_id)
                
            if entry.category:
                self.category_index[entry.category].remove(feedback_id)
                
            # Remove from cache
            del self.feedback_cache[feedback_id]
            
            # Rewrite the entire storage file
            with open(self.storage_file, 'w') as f:
                for entry in self.feedback_cache.values():
                    f.write(json.dumps(entry.to_dict()) + '\n')
                    
            # Save updated indexes
            self._save_index(self.user_index_file, self.user_index)
            self._save_index(self.query_index_file, self.query_index)
            self._save_index(self.tag_index_file, self.tag_index)
            self._save_index(self.category_index_file, self.category_index)
                
            return True
            
        except Exception as e:
            logger.error(f"Error deleting feedback: {str(e)}")
            return False
    
    def search_feedback(self, 
                       user_id: Optional[str] = None, 
                       query: Optional[str] = None,
                       tags: Optional[List[str]] = None,
                       category: Optional[str] = None,
                       feedback_type: Optional[str] = None,
                       min_rating: Optional[float] = None,
                       max_rating: Optional[float] = None,
                       start_time: Optional[datetime] = None,
                       end_time: Optional[datetime] = None,
                       limit: int = 100) -> List[FeedbackEntry]:
        """
        Search for feedback entries matching criteria.
        
        Args:
            user_id: Filter by user ID
            query: Filter by query text (substring match)
            tags: Filter by tags (must have all tags)
            category: Filter by category
            feedback_type: Filter by feedback type
            min_rating: Filter by minimum rating
            max_rating: Filter by maximum rating
            start_time: Filter by start time
            end_time: Filter by end time
            limit: Maximum number of results
            
        Returns:
            List of matching feedback entries
        """
        # Start with all entries
        results = list(self.feedback_cache.values())
        
        # Apply filters
        if user_id:
            results = [entry for entry in results if entry.user_id == user_id]
            
        if query:
            query_lower = query.lower()
            results = [entry for entry in results if query_lower in entry.query.lower()]
            
        if tags:
            results = [entry for entry in results if all(tag in entry.tags for tag in tags)]
            
        if category:
            results = [entry for entry in results if entry.category == category]
            
        if feedback_type:
            results = [entry for entry in results if entry.feedback_type == feedback_type]
            
        if min_rating is not None:
            results = [entry for entry in results if entry.rating is not None and entry.rating >= min_rating]
            
        if max_rating is not None:
            results = [entry for entry in results if entry.rating is not None and entry.rating <= max_rating]
            
        if start_time:
            results = [entry for entry in results if entry.timestamp >= start_time]
            
        if end_time:
            results = [entry for entry in results if entry.timestamp <= end_time]
            
        # Sort by timestamp (newest first) and limit results
        results.sort(key=lambda x: x.timestamp, reverse=True)
        
        return results[:limit]
    
    def get_similar_queries(self, query: str, threshold: float = 0.8, limit: int = 10) -> List[Tuple[str, float]]:
        """
        Find similar queries based on string similarity.
        
        Args:
            query: The query to find similar ones to
            threshold: Similarity threshold (0.0 to 1.0)
            limit: Maximum number of results
            
        Returns:
            List of tuples (query, similarity_score)
        """
        try:
            from difflib import SequenceMatcher
            
            normalized_query = self._normalize_query(query)
            
            results = []
            
            for stored_query in self.query_index.keys():
                similarity = SequenceMatcher(None, normalized_query, stored_query).ratio()
                if similarity >= threshold:
                    results.append((stored_query, similarity))
                    
            # Sort by similarity (highest first) and limit results
            results.sort(key=lambda x: x[1], reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"Error finding similar queries: {str(e)}")
            return []
    
    def get_feedback_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the stored feedback.
        
        Returns:
            Dictionary with various statistics
        """
        stats = {
            "total_entries": len(self.feedback_cache),
            "unique_users": len(self.user_index),
            "unique_queries": len(self.query_index),
            "feedback_types": {},
            "rating_distribution": {1: 0, 2: 0, 3: 0, 4: 0, 5: 0},
            "categories": {},
            "tags": {},
            "timeline": {
                "daily": {},
                "weekly": {},
                "monthly": {}
            }
        }
        
        # Calculate stats
        feedback_types = {}
        categories = {}
        tags = Counter()
        
        for entry in self.feedback_cache.values():
            # Feedback types
            feedback_type = entry.feedback_type
            if feedback_type not in feedback_types:
                feedback_types[feedback_type] = 0
            feedback_types[feedback_type] += 1
            
            # Rating distribution
            if entry.rating is not None:
                rating = min(5, max(1, int(entry.rating)))
                stats["rating_distribution"][rating] += 1
                
            # Categories
            if entry.category:
                if entry.category not in categories:
                    categories[entry.category] = 0
                categories[entry.category] += 1
                
            # Tags
            for tag in entry.tags:
                tags[tag] += 1
                
            # Timeline
            date_str = entry.timestamp.strftime("%Y-%m-%d")
            week_str = entry.timestamp.strftime("%Y-%W")
            month_str = entry.timestamp.strftime("%Y-%m")
            
            if date_str not in stats["timeline"]["daily"]:
                stats["timeline"]["daily"][date_str] = 0
            stats["timeline"]["daily"][date_str] += 1
            
            if week_str not in stats["timeline"]["weekly"]:
                stats["timeline"]["weekly"][week_str] = 0
            stats["timeline"]["weekly"][week_str] += 1
            
            if month_str not in stats["timeline"]["monthly"]:
                stats["timeline"]["monthly"][month_str] = 0
            stats["timeline"]["monthly"][month_str] += 1
            
        # Add calculated stats
        stats["feedback_types"] = feedback_types
        stats["categories"] = categories
        stats["tags"] = dict(tags.most_common(20))
        
        return stats


class FeedbackAnalyzer:
    """Analyze user feedback to identify patterns and improvement opportunities."""
    
    def __init__(self, feedback_store: FeedbackStore):
        """
        Initialize the feedback analyzer.
        
        Args:
            feedback_store: The feedback store to analyze
        """
        self.feedback_store = feedback_store
    
    def get_most_common_issues(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get the most common issues from negative feedback.
        
        Args:
            limit: Maximum number of issues to return
            
        Returns:
            List of issue dictionaries with counts and examples
        """
        # Get negative feedback (thumbs down or low ratings)
        negative_feedback = self.feedback_store.search_feedback(
            feedback_type="thumbs_down",
            limit=1000
        )
        
        # Also get low ratings
        low_ratings = self.feedback_store.search_feedback(
            max_rating=2.0,
            limit=1000
        )
        
        # Combine and deduplicate
        all_negative = {f.id: f for f in negative_feedback + low_ratings}
        negative_feedback = list(all_negative.values())
        
        if not negative_feedback:
            return []
            
        # Group by query similarity
        from difflib import SequenceMatcher
        
        # Group similar queries
        query_groups = []
        
        for feedback in negative_feedback:
            query = feedback.query.lower()
            
            # Check if query is similar to an existing group
            found_group = False
            for group in query_groups:
                representative = group["representative"]
                if SequenceMatcher(None, query, representative).ratio() > 0.7:
                    group["feedback"].append(feedback)
                    found_group = True
                    break
                    
            if not found_group:
                # Create a new group
                query_groups.append({
                    "representative": query,
                    "feedback": [feedback]
                })
        
        # Sort groups by size (largest first)
        query_groups.sort(key=lambda x: len(x["feedback"]), reverse=True)
        
        # Format results
        results = []
        for group in query_groups[:limit]:
            examples = []
            comments = []
            
            for feedback in group["feedback"][:3]:
                examples.append({
                    "query": feedback.query,
                    "response": feedback.response[:200] + "..." if len(feedback.response) > 200 else feedback.response,
                    "rating": feedback.rating,
                    "feedback_type": feedback.feedback_type
                })
                
                if feedback.comment:
                    comments.append(feedback.comment)
                    
            results.append({
                "query_pattern": group["representative"],
                "count": len(group["feedback"]),
                "examples": examples,
                "comments": comments[:5]  # Limit to 5 comments
            })
            
        return results
    
    def analyze_query_performance(self, query: str) -> Dict[str, Any]:
        """
        Analyze performance for a specific query or similar queries.
        
        Args:
            query: The query to analyze
            
        Returns:
            Dictionary with performance metrics
        """
        # Find similar queries
        similar_queries = self.feedback_store.get_similar_queries(query, threshold=0.7, limit=10)
        
        if not similar_queries:
            return {"error": "No similar queries found in feedback store"}
            
        # Get feedback for each similar query
        all_feedback = []
        for similar_query, similarity in similar_queries:
            # Find entries in the query index
            entries = []
            for entry_id in self.feedback_store.query_index.get(similar_query, []):
                entry = self.feedback_store.get_feedback(entry_id)
                if entry:
                    entries.append(entry)
                    
            all_feedback.extend(entries)
            
        if not all_feedback:
            return {"error": "No feedback found for similar queries"}
            
        # Calculate performance metrics
        positive_count = 0
        negative_count = 0
        ratings = []
        
        for feedback in all_feedback:
            if feedback.feedback_type == "thumbs_up":
                positive_count += 1
            elif feedback.feedback_type == "thumbs_down":
                negative_count += 1
                
            if feedback.rating is not None:
                ratings.append(feedback.rating)
                
        total_count = positive_count + negative_count
        positive_rate = positive_count / total_count if total_count > 0 else 0
        
        avg_rating = sum(ratings) / len(ratings) if ratings else None
        
        # Get retrieval and generation metrics
        retrieval_metrics = defaultdict(list)
        generation_metrics = defaultdict(list)
        
        for feedback in all_feedback:
            for metric, value in feedback.retrieval_metrics.items():
                if isinstance(value, (int, float)):
                    retrieval_metrics[metric].append(value)
                    
            for metric, value in feedback.generation_metrics.items():
                if isinstance(value, (int, float)):
                    generation_metrics[metric].append(value)
        
        # Calculate average metrics
        avg_retrieval_metrics = {}
        for metric, values in retrieval_metrics.items():
            if values:
                avg_retrieval_metrics[metric] = sum(values) / len(values)
                
        avg_generation_metrics = {}
        for metric, values in generation_metrics.items():
            if values:
                avg_generation_metrics[metric] = sum(values) / len(values)
                
        # Compile results
        return {
            "query": query,
            "similar_queries": [q for q, _ in similar_queries],
            "total_feedback": len(all_feedback),
            "positive_count": positive_count,
            "negative_count": negative_count,
            "positive_rate": positive_rate,
            "average_rating": avg_rating,
            "average_retrieval_metrics": avg_retrieval_metrics,
            "average_generation_metrics": avg_generation_metrics,
            "examples": [
                {
                    "query": feedback.query,
                    "response": feedback.response[:200] + "..." if len(feedback.response) > 200 else feedback.response,
                    "feedback_type": feedback.feedback_type,
                    "rating": feedback.rating,
                    "comment": feedback.comment
                }
                for feedback in all_feedback[:5]  # Limit to 5 examples
            ]
        }
    
    def analyze_source_document_performance(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Analyze which source documents are most helpful or problematic.
        
        Args:
            limit: Maximum number of documents to analyze
            
        Returns:
            List of document performance dictionaries
        """
        # Get all feedback with source documents
        all_feedback = list(self.feedback_store.feedback_cache.values())
        
        # Track document usage and feedback
        document_stats = defaultdict(lambda: {
            "total_uses": 0,
            "positive_feedback": 0,
            "negative_feedback": 0,
            "ratings": []
        })
        
        for feedback in all_feedback:
            # Skip if no source documents
            if not feedback.source_docs:
                continue
                
            # Extract source information
            for source in feedback.source_docs:
                # Create a unique identifier for the document
                if "id" in source:
                    doc_id = source["id"]
                elif "title" in source and "source" in source:
                    doc_id = f"{source['source']}:{source['title']}"
                else:
                    # Skip if we can't identify the document
                    continue
                    
                # Update statistics
                document_stats[doc_id]["total_uses"] += 1
                
                if feedback.feedback_type == "thumbs_up":
                    document_stats[doc_id]["positive_feedback"] += 1
                elif feedback.feedback_type == "thumbs_down":
                    document_stats[doc_id]["negative_feedback"] += 1
                    
                if feedback.rating is not None:
                    document_stats[doc_id]["ratings"].append(feedback.rating)
                    
                # Store document metadata if not already present
                if "metadata" not in document_stats[doc_id]:
                    document_stats[doc_id]["metadata"] = {
                        "title": source.get("title", doc_id),
                        "source": source.get("source", "Unknown"),
                        "url": source.get("url", "")
                    }
        
        # Calculate additional metrics
        for doc_id, stats in document_stats.items():
            # Total feedback count
            feedback_count = stats["positive_feedback"] + stats["negative_feedback"]
            
            # Positivity rate
            stats["positivity_rate"] = stats["positive_feedback"] / feedback_count if feedback_count > 0 else 0
            
            # Average rating
            stats["average_rating"] = sum(stats["ratings"]) / len(stats["ratings"]) if stats["ratings"] else None
            
            # Usage rate
            total_docs_uses = sum(s["total_uses"] for s in document_stats.values())
            stats["usage_rate"] = stats["total_uses"] / total_docs_uses if total_docs_uses > 0 else 0
        
        # Convert to list and sort by total uses (most used first)
        docs_list = [{"doc_id": doc_id, **stats} for doc_id, stats in document_stats.items()]
        docs_list.sort(key=lambda x: x["total_uses"], reverse=True)
        
        return docs_list[:limit]
    
    def generate_performance_report(self, time_period: str = "last_30_days") -> Dict[str, Any]:
        """
        Generate a comprehensive performance report.
        
        Args:
            time_period: Time period for report (last_7_days, last_30_days, all_time)
            
        Returns:
            Dictionary with report data
        """
        # Set time filter based on time period
        if time_period == "last_7_days":
            start_time = datetime.now() - timedelta(days=7)
        elif time_period == "last_30_days":
            start_time = datetime.now() - timedelta(days=30)
        else:  # all_time
            start_time = None
            
        # Get feedback for the time period
        if start_time:
            feedback = self.feedback_store.search_feedback(start_time=start_time, limit=10000)
        else:
            feedback = list(self.feedback_store.feedback_cache.values())
            
        if not feedback:
            return {"error": "No feedback data available for the selected time period"}
            
        # Overall metrics
        total_feedback = len(feedback)
        feedback_types = Counter([f.feedback_type for f in feedback])
        ratings = [f.rating for f in feedback if f.rating is not None]
        avg_rating = sum(ratings) / len(ratings) if ratings else None
        
        # Calculate trends
        if start_time:
            # Group by day
            daily_counts = defaultdict(int)
            daily_ratings = defaultdict(list)
            
            for f in feedback:
                day_str = f.timestamp.strftime("%Y-%m-%d")
                daily_counts[day_str] += 1
                if f.rating is not None:
                    daily_ratings[day_str].append(f.rating)
                    
            # Calculate daily average ratings
            daily_avg_ratings = {}
            for day, day_ratings in daily_ratings.items():
                if day_ratings:
                    daily_avg_ratings[day] = sum(day_ratings) / len(day_ratings)
                    
            # Sort by date
            trend_dates = sorted(daily_counts.keys())
            trend_counts = [daily_counts[date] for date in trend_dates]
            trend_ratings = [daily_avg_ratings.get(date) for date in trend_dates]
        else:
            trend_dates = []
            trend_counts = []
            trend_ratings = []
            
        # Get most common issues
        common_issues = self.get_most_common_issues(limit=5)
        
        # Get source document performance
        doc_performance = self.analyze_source_document_performance(limit=10)
        
        # Categorize feedback by query type (using tags as proxy)
        query_categories = Counter()
        for f in feedback:
            for tag in f.tags:
                query_categories[tag] += 1
                
        # Compile the report
        report = {
            "time_period": time_period,
            "total_feedback": total_feedback,
            "feedback_types": dict(feedback_types),
            "average_rating": avg_rating,
            "rating_distribution": {
                "1": ratings.count(1) if ratings else 0,
                "2": ratings.count(2) if ratings else 0,
                "3": ratings.count(3) if ratings else 0,
                "4": ratings.count(4) if ratings else 0,
                "5": ratings.count(5) if ratings else 0
            },
            "trends": {
                "dates": trend_dates,
                "counts": trend_counts,
                "ratings": trend_ratings
            },
            "common_issues": common_issues,
            "top_documents": doc_performance,
            "query_categories": dict(query_categories.most_common(10))
        }
        
        return report
    
    def plot_feedback_trends(self, time_period: str = "last_30_days", metric: str = "count") -> str:
        """
        Generate a plot of feedback trends over time.
        
        Args:
            time_period: Time period for trends (last_7_days, last_30_days, last_90_days)
            metric: Metric to plot (count, rating, positive_rate)
            
        Returns:
            Path to saved plot image
        """
        try:
            import matplotlib.pyplot as plt
            import matplotlib.dates as mdates
            from datetime import datetime, timedelta
            
            # Set time filter based on time period
            if time_period == "last_7_days":
                start_time = datetime.now() - timedelta(days=7)
                date_format = '%Y-%m-%d'
                title_period = "Last 7 Days"
            elif time_period == "last_30_days":
                start_time = datetime.now() - timedelta(days=30)
                date_format = '%Y-%m-%d'
                title_period = "Last 30 Days"
            elif time_period == "last_90_days":
                start_time = datetime.now() - timedelta(days=90)
                date_format = '%Y-%m-%d'
                title_period = "Last 90 Days"
            else:  # all_time
                start_time = None
                date_format = '%Y-%m'
                title_period = "All Time"
                
            # Get feedback for the time period
            if start_time:
                feedback = self.feedback_store.search_feedback(start_time=start_time, limit=10000)
            else:
                feedback = list(self.feedback_store.feedback_cache.values())
                
            if not feedback:
                raise ValueError("No feedback data available for the selected time period")
                
            # Group by day
            daily_data = defaultdict(list)
            
            for f in feedback:
                day_str = f.timestamp.strftime(date_format)
                daily_data[day_str].append(f)
                
            # Calculate metrics
            dates = sorted(daily_data.keys())
            x_values = [datetime.strptime(d, date_format) for d in dates]
            
            if metric == "count":
                y_values = [len(daily_data[d]) for d in dates]
                y_label = "Feedback Count"
                title = f"Feedback Count Trend - {title_period}"
            elif metric == "rating":
                y_values = []
                for d in dates:
                    ratings = [f.rating for f in daily_data[d] if f.rating is not None]
                    avg_rating = sum(ratings) / len(ratings) if ratings else None
                    y_values.append(avg_rating)
                y_label = "Average Rating"
                title = f"Average Rating Trend - {title_period}"
            elif metric == "positive_rate":
                y_values = []
                for d in dates:
                    positive = sum(1 for f in daily_data[d] if f.feedback_type == "thumbs_up")
                    negative = sum(1 for f in daily_data[d] if f.feedback_type == "thumbs_down")
                    total = positive + negative
                    rate = positive / total if total > 0 else None
                    y_values.append(rate)
                y_label = "Positive Feedback Rate"
                title = f"Positive Feedback Rate Trend - {title_period}"
            else:
                raise ValueError(f"Unknown metric: {metric}")
                
            # Create the plot
            plt.figure(figsize=(10, 6))
            plt.plot(x_values, y_values, marker='o', linestyle='-')
            plt.title(title)
            plt.xlabel("Date")
            plt.ylabel(y_label)
            plt.grid(True)
            
            # Format x-axis dates
            if time_period == "last_7_days":
                plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
            elif time_period == "last_30_days":
                plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
                plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))
            elif time_period == "last_90_days":
                plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
                plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))
            else:  # all_time
                plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
                
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # Save the plot
            output_dir = os.path.join(self.feedback_store.storage_dir, "plots")
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            output_path = os.path.join(output_dir, f"feedback_{metric}_{time_period}_{timestamp}.png")
            
            plt.savefig(output_path)
            plt.close()
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error plotting feedback trends: {str(e)}")
            return str(e)


class ContinuousLearningManager:
    """Manage continuous learning from user feedback to improve the RAG system."""
    
    def __init__(self, feedback_store: FeedbackStore, data_dir: str = "learning_data"):
        """
        Initialize the continuous learning manager.
        
        Args:
            feedback_store: The feedback store
            data_dir: Directory to store learning data
        """
        self.feedback_store = feedback_store
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
        
        # File to track applied feedback
        self.applied_feedback_file = os.path.join(data_dir, "applied_feedback.json")
        
        # Applied feedback tracking
        self.applied_feedback = set()
        
        # Load applied feedback
        self._load_applied_feedback()
    
    def _load_applied_feedback(self):
        """Load the set of applied feedback IDs."""
        if os.path.exists(self.applied_feedback_file):
            try:
                with open(self.applied_feedback_file, 'r') as f:
                    self.applied_feedback = set(json.load(f))
            except Exception as e:
                logger.warning(f"Error loading applied feedback: {str(e)}")
    
    def _save_applied_feedback(self):
        """Save the set of applied feedback IDs."""
        try:
            with open(self.applied_feedback_file, 'w') as f:
                json.dump(list(self.applied_feedback), f)
        except Exception as e:
            logger.warning(f"Error saving applied feedback: {str(e)}")
    
    def get_learning_opportunities(self, min_feedback_count: int = 5) -> List[Dict[str, Any]]:
        """
        Identify opportunities for system improvement based on feedback.
        
        Args:
            min_feedback_count: Minimum number of feedback entries to consider an opportunity
            
        Returns:
            List of learning opportunity dictionaries
        """
        # Get feedback analyzer
        analyzer = FeedbackAnalyzer(self.feedback_store)
        
        # Get most common issues
        common_issues = analyzer.get_most_common_issues(limit=20)
        
        # Filter issues with enough feedback
        opportunities = []
        for issue in common_issues:
            if issue["count"] >= min_feedback_count:
                # Check if any of the feedback entries have been applied
                example_queries = [example["query"] for example in issue["examples"]]
                
                # Unapplied feedback entries for these queries
                unapplied_entries = []
                for query in example_queries:
                    entries = self.feedback_store.search_feedback(query=query)
                    unapplied_entries.extend([e for e in entries if e.id not in self.applied_feedback])
                
                if unapplied_entries:
                    opportunity = {
                        "query_pattern": issue["query_pattern"],
                        "feedback_count": issue["count"],
                        "examples": issue["examples"],
                        "comments": issue["comments"],
                        "unapplied_entries": len(unapplied_entries),
                        "priority": self._calculate_priority(issue, unapplied_entries)
                    }
                    opportunities.append(opportunity)
        
        # Sort by priority (highest first)
        opportunities.sort(key=lambda x: x["priority"], reverse=True)
        
        return opportunities
    
    def _calculate_priority(self, issue: Dict[str, Any], unapplied_entries: List[FeedbackEntry]) -> float:
        """
        Calculate priority score for a learning opportunity.
        
        Args:
            issue: Issue dictionary
            unapplied_entries: Unapplied feedback entries
            
        Returns:
            Priority score (0.0 to 1.0)
        """
        # Factors:
        # 1. Number of feedback entries
        # 2. Recency of feedback
        # 3. Severity of feedback (ratings, comments)
        # 4. Number of unapplied entries
        
        # Number of feedback entries (0.0 to 0.3)
        count_score = min(0.3, issue["count"] / 50)
        
        # Recency of feedback (0.0 to 0.2)
        if unapplied_entries:
            # Get most recent entry
            most_recent = max(unapplied_entries, key=lambda x: x.timestamp)
            days_ago = (datetime.now() - most_recent.timestamp).days
            recency_score = 0.2 * max(0, 30 - days_ago) / 30
        else:
            recency_score = 0
            
        # Severity of feedback (0.0 to 0.3)
        # Get average rating
        ratings = [e.rating for e in unapplied_entries if e.rating is not None]
        if ratings:
            avg_rating = sum(ratings) / len(ratings)
            # Lower ratings = higher severity
            severity_score = 0.3 * (5 - avg_rating) / 4
        else:
            # If no ratings, check feedback type
            negative_count = sum(1 for e in unapplied_entries if e.feedback_type == "thumbs_down")
            severity_score = 0.3 * (negative_count / len(unapplied_entries)) if unapplied_entries else 0
            
        # Number of unapplied entries (0.0 to 0.2)
        unapplied_score = 0.2 * min(1.0, len(unapplied_entries) / 20)
        
        # Calculate total priority score
        priority = count_score + recency_score + severity_score + unapplied_score
        
        return priority
    
    def update_retrieval_weights(self, opportunity: Dict[str, Any]) -> Dict[str, Any]:
        """
        Update retrieval weights based on feedback.
        
        Args:
            opportunity: Learning opportunity dictionary
            
        Returns:
            Dictionary with update results
        """
        # This would integrate with your retrieval system to adjust weights
        # Here we'll just simulate the process and return what would be updated
        
        # Get all queries for this opportunity
        queries = [example["query"] for example in opportunity["examples"]]
        
        # Get all feedback entries for these queries
        all_entries = []
        for query in queries:
            entries = self.feedback_store.search_feedback(query=query)
            all_entries.extend([e for e in entries if e.id not in self.applied_feedback])
            
        if not all_entries:
            return {"error": "No unapplied feedback entries found"}
            
        # Analyze source documents
        documents_feedback = defaultdict(lambda: {"thumbs_up": 0, "thumbs_down": 0})
        
        for entry in all_entries:
            for source in entry.source_docs:
                # Create a unique identifier for the document
                if "id" in source:
                    doc_id = source["id"]
                elif "title" in source and "source" in source:
                    doc_id = f"{source['source']}:{source['title']}"
                else:
                    # Skip if we can't identify the document
                    continue
                    
                # Update counts based on feedback type
                if entry.feedback_type == "thumbs_up":
                    documents_feedback[doc_id]["thumbs_up"] += 1
                elif entry.feedback_type == "thumbs_down":
                    documents_feedback[doc_id]["thumbs_down"] += 1
        
        # Calculate adjustment factors
        adjustments = {}
        for doc_id, counts in documents_feedback.items():
            total = counts["thumbs_up"] + counts["thumbs_down"]
            if total < 3:
                # Skip if not enough feedback
                continue
                
            # Calculate adjustment factor (-0.5 to 0.5)
            adjustment = 0.5 * (counts["thumbs_up"] - counts["thumbs_down"]) / total
            
            # Safety bounds
            adjustment = max(-0.5, min(0.5, adjustment))
            
            adjustments[doc_id] = adjustment
        
        # Mark feedback as applied
        for entry in all_entries:
            self.applied_feedback.add(entry.id)
            
            # Update entry in the store
            entry.applied = True
            self.feedback_store.update_feedback(entry)
            
        # Save applied feedback
        self._save_applied_feedback()
        
        return {
            "query_pattern": opportunity["query_pattern"],
            "applied_entries": len(all_entries),
            "weight_adjustments": adjustments
        }
    
    def generate_query_expansion_rules(self, opportunity: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate query expansion rules based on feedback.
        
        Args:
            opportunity: Learning opportunity dictionary
            
        Returns:
            Dictionary with query expansion rules
        """
        # Get all queries for this opportunity
        queries = [example["query"] for example in opportunity["examples"]]
        
        # Get all feedback entries for these queries
        all_entries = []
        for query in queries:
            entries = self.feedback_store.search_feedback(query=query)
            all_entries.extend([e for e in entries if e.id not in self.applied_feedback])
            
        if not all_entries:
            return {"error": "No unapplied feedback entries found"}
            
        # Analyze effective source documents from positive feedback
        positive_entries = [e for e in all_entries if e.feedback_type == "thumbs_up" or (e.rating is not None and e.rating >= 4)]
        
        if not positive_entries:
            return {"error": "No positive feedback entries found"}
            
        # Extract terms from effective source documents
        terms = []
        for entry in positive_entries:
            for source in entry.source_docs:
                if "content" in source and isinstance(source["content"], str):
                    # Extract important terms using simple frequency analysis
                    from collections import Counter
                    import re
                    
                    # Tokenize and normalize
                    tokens = re.findall(r'\b[a-zA-Z]{3,}\b', source["content"].lower())
                    
                    # Remove common stop words
                    stop_words = {"the", "and", "for", "not", "with", "this", "that", "has", "have", 
                                 "had", "are", "were", "was", "will", "been", "being", "they", "them"}
                    tokens = [t for t in tokens if t not in stop_words]
                    
                    # Add to terms list
                    terms.extend(tokens)
        
        # Count term frequency
        term_counts = Counter(terms)
        
        # Get the most common terms
        common_terms = [term for term, count in term_counts.most_common(10) if count >= 3]
        
        # Create expansion rules
        expansions = {
            "query_pattern": opportunity["query_pattern"],
            "expanded_terms": common_terms,
            "source_documents": [
                source.get("title", source.get("id", "Unknown"))
                for entry in positive_entries
                for source in entry.source_docs
                if "title" in source or "id" in source
            ]
        }
        
        # Mark feedback as applied
        for entry in all_entries:
            self.applied_feedback.add(entry.id)
            
            # Update entry in the store
            entry.applied = True
            self.feedback_store.update_feedback(entry)
            
        # Save applied feedback
        self._save_applied_feedback()
        
        return expansions
    
    def generate_training_examples(self, min_positive_rating: float = 4.0) -> Dict[str, Any]:
        """
        Generate training examples from feedback for fine-tuning.
        
        Args:
            min_positive_rating: Minimum rating to consider a positive example
            
        Returns:
            Dictionary with training examples
        """
        # Get positive examples (high ratings)
        positive_examples = self.feedback_store.search_feedback(
            min_rating=min_positive_rating,
            limit=1000
        )
        
        # Get negative examples (low ratings)
        negative_examples = self.feedback_store.search_feedback(
            max_rating=2.0,
            limit=1000
        )
        
        # Format examples
        formatted_positive = []
        for example in positive_examples:
            if example.id in self.applied_feedback:
                # Skip if already applied
                continue
                
            formatted_positive.append({
                "query": example.query,
                "response": example.response,
                "source_docs": [
                    {
                        "id": source.get("id", ""),
                        "title": source.get("title", ""),
                        "source": source.get("source", ""),
                        "content": source.get("content", "")[:500] if "content" in source else ""
                    }
                    for source in example.source_docs
                ],
                "rating": example.rating
            })
            
        formatted_negative = []
        for example in negative_examples:
            if example.id in self.applied_feedback:
                # Skip if already applied
                continue
                
            formatted_negative.append({
                "query": example.query,
                "response": example.response,
                "source_docs": [
                    {
                        "id": source.get("id", ""),
                        "title": source.get("title", ""),
                        "source": source.get("source", ""),
                        "content": source.get("content", "")[:500] if "content" in source else ""
                    }
                    for source in example.source_docs
                ],
                "rating": example.rating,
                "comment": example.comment
            })
        
        # Save to files
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        positive_file = os.path.join(self.data_dir, f"positive_examples_{timestamp}.json")
        negative_file = os.path.join(self.data_dir, f"negative_examples_{timestamp}.json")
        
        try:
            with open(positive_file, 'w') as f:
                json.dump(formatted_positive, f, indent=2)
                
            with open(negative_file, 'w') as f:
                json.dump(formatted_negative, f, indent=2)
                
            # Mark feedback as applied
            for example in positive_examples + negative_examples:
                if example.id not in self.applied_feedback:
                    self.applied_feedback.add(example.id)
                    
                    # Update entry in the store
                    example.applied = True
                    self.feedback_store.update_feedback(example)
                    
            # Save applied feedback
            self._save_applied_feedback()
            
            return {
                "positive_examples": len(formatted_positive),
                "negative_examples": len(formatted_negative),
                "positive_file": positive_file,
                "negative_file": negative_file
            }
            
        except Exception as e:
            logger.error(f"Error generating training examples: {str(e)}")
            return {"error": str(e)}
    
    def apply_learning_opportunity(self, opportunity_id: str, action: str) -> Dict[str, Any]:
        """
        Apply a learning opportunity to improve the system.
        
        Args:
            opportunity_id: ID of the learning opportunity
            action: Action to take (update_weights, generate_expansions, etc.)
            
        Returns:
            Dictionary with results
        """
        # Get learning opportunities
        opportunities = self.get_learning_opportunities()
        
        # Find the requested opportunity
        opportunity = None
        for opp in opportunities:
            if opp["query_pattern"] == opportunity_id:
                opportunity = opp
                break
                
        if not opportunity:
            return {"error": f"Learning opportunity not found: {opportunity_id}"}
            
        # Apply the requested action
        if action == "update_weights":
            return self.update_retrieval_weights(opportunity)
        elif action == "generate_expansions":
            return self.generate_query_expansion_rules(opportunity)
        elif action == "generate_examples":
            return self.generate_training_examples()
        else:
            return {"error": f"Unknown action: {action}"}


class FeedbackRouter:
    """Routes feedback to appropriate handlers and integrates with the RAG system."""
    
    def __init__(self, feedback_store: FeedbackStore = None, learning_manager: ContinuousLearningManager = None):
        """
        Initialize the feedback router.
        
        Args:
            feedback_store: The feedback store (created if not provided)
            learning_manager: The continuous learning manager (created if not provided)
        """
        # Create feedback store if not provided
        if feedback_store is None:
            self.feedback_store = FeedbackStore()
        else:
            self.feedback_store = feedback_store
            
        # Create learning manager if not provided
        if learning_manager is None:
            self.learning_manager = ContinuousLearningManager(self.feedback_store)
        else:
            self.learning_manager = learning_manager
            
        # Feedback analyzer
        self.feedback_analyzer = FeedbackAnalyzer(self.feedback_store)
        
        # Categorization rules (for automatic tagging)
        self.categorization_rules = [
            {
                "name": "retrieval_issue",
                "patterns": ["wrong info", "irrelevant", "missing info", "not found", "couldn't find"],
                "tags": ["retrieval_issue"]
            },
            {
                "name": "formatting_issue",
                "patterns": ["format", "layout", "presentation", "display"],
                "tags": ["formatting_issue"]
            },
            {
                "name": "accuracy_issue",
                "patterns": ["incorrect", "wrong", "false", "inaccurate", "error"],
                "tags": ["accuracy_issue"]
            },
            {
                "name": "positive_feedback",
                "patterns": ["great", "excellent", "perfect", "good job", "helpful"],
                "tags": ["positive_feedback"]
            }
        ]
    
    def process_feedback(self, 
                        query: str, 
                        response: str,
                        feedback_type: str,
                        user_id: Optional[str] = None,
                        rating: Optional[float] = None,
                        comment: Optional[str] = None,
                        source_docs: Optional[List[Dict[str, Any]]] = None,
                        metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Process user feedback and route it appropriately.
        
        Args:
            query: The original query
            response: The response that received feedback
            feedback_type: Type of feedback (thumbs_up, thumbs_down, rating)
            user_id: ID of the user providing feedback
            rating: Numerical rating (1-5) if provided
            comment: User comment if provided
            source_docs: Source documents used for the response
            metadata: Additional metadata
            
        Returns:
            Dictionary with process results
        """
        # Generate a unique ID for this feedback
        feedback_id = f"feedback_{int(time.time())}_{hash(query+response) % 10000}"
        
        # Auto-categorize based on comment or feedback type
        category = self._auto_categorize(feedback_type, comment, rating)
        
        # Auto-tag based on comment
        tags = self._auto_tag(comment)
        
        # Calculate retrieval and generation metrics
        retrieval_metrics = {}
        generation_metrics = {}
        
        if metadata:
            # Extract metrics from metadata if available
            if "retrieval" in metadata:
                retrieval_metrics = metadata["retrieval"]
            if "generation" in metadata:
                generation_metrics = metadata["generation"]
                
        # Create feedback entry
        feedback = FeedbackEntry(
            id=feedback_id,
            user_id=user_id,
            query=query,
            response=response,
            feedback_type=feedback_type,
            rating=rating,
            comment=comment,
            source_docs=source_docs or [],
            timestamp=datetime.now(),
            metadata=metadata or {},
            category=category,
            tags=tags,
            retrieval_metrics=retrieval_metrics,
            generation_metrics=generation_metrics
        )
        
        # Add to feedback store
        success = self.feedback_store.add_feedback(feedback)
        
        if not success:
            return {"error": "Failed to store feedback"}
            
        # Check for similar queries with different feedback
        similar_queries = self.feedback_store.get_similar_queries(query, threshold=0.9, limit=5)
        
        contradictory_feedback = []
        for similar_query, similarity in similar_queries:
            # Skip exact matches
            if similarity == 1.0:
                continue
                
            # Get feedback for this query
            query_feedback = []
            for entry_id in self.feedback_store.query_index.get(similar_query, []):
                entry = self.feedback_store.get_feedback(entry_id)
                if entry and entry.id != feedback_id:  # Skip the current feedback
                    query_feedback.append(entry)
                    
            # Check for contradictory feedback
            for entry in query_feedback:
                if entry.feedback_type != feedback_type or (rating is not None and entry.rating is not None and 
                                                          abs(rating - entry.rating) >= 2):
                    contradictory_feedback.append({
                        "query": entry.query,
                        "feedback_type": entry.feedback_type,
                        "rating": entry.rating,
                        "similarity": similarity
                    })
        
        # Check if this feedback creates a learning opportunity
        is_learning_opportunity = False
        if feedback_type == "thumbs_down" or (rating is not None and rating <= 2):
            # Get similar query pattern
            opportunities = self.learning_manager.get_learning_opportunities(min_feedback_count=3)
            
            for opportunity in opportunities:
                if any(example["query"] == query for example in opportunity["examples"]):
                    is_learning_opportunity = True
                    break
        
        return {
            "success": True,
            "feedback_id": feedback_id,
            "category": category,
            "tags": tags,
            "contradictory_feedback": contradictory_feedback[:3],
            "is_learning_opportunity": is_learning_opportunity
        }
    
    def _auto_categorize(self, feedback_type: str, comment: Optional[str], rating: Optional[float]) -> Optional[str]:
        """
        Automatically categorize feedback based on type, comment, and rating.
        
        Args:
            feedback_type: Type of feedback
            comment: User comment
            rating: Numerical rating
            
        Returns:
            Category string or None
        """
        if feedback_type == "thumbs_up" or (rating is not None and rating >= 4):
            return "positive"
            
        if feedback_type == "thumbs_down" or (rating is not None and rating <= 2):
            # Try to categorize based on comment
            if comment:
                comment_lower = comment.lower()
                
                if any(pattern in comment_lower for pattern in ["wrong info", "irrelevant", "not accurate", "incorrect"]):
                    return "inaccurate"
                    
                if any(pattern in comment_lower for pattern in ["missing", "not found", "incomplete", "more info"]):
                    return "incomplete"
                    
                if any(pattern in comment_lower for pattern in ["format", "display", "presentation"]):
                    return "formatting"
                    
                if any(pattern in comment_lower for pattern in ["slow", "timeout", "took too long"]):
                    return "performance"
                    
            return "negative"
            
        # Neutral feedback
        if rating is not None and rating == 3:
            return "neutral"
            
        return None
    
    def _auto_tag(self, comment: Optional[str]) -> List[str]:
        """
        Automatically generate tags based on comment content.
        
        Args:
            comment: User comment
            
        Returns:
            List of tags
        """
        if not comment:
            return []
            
        comment_lower = comment.lower()
        tags = []
        
        # Apply categorization rules
        for rule in self.categorization_rules:
            if any(pattern in comment_lower for pattern in rule["patterns"]):
                tags.extend(rule["tags"])
                
        # Add domain-specific tags (examples)
        domains = ["technical", "financial", "medical", "legal", "educational"]
        for domain in domains:
            if domain in comment_lower:
                tags.append(f"domain_{domain}")
                
        return list(set(tags))  # Remove duplicates
    
    def get_feedback_dashboard_data(self) -> Dict[str, Any]:
        """
        Get data for a feedback dashboard.
        
        Returns:
            Dictionary with dashboard data
        """
        # Get basic stats
        stats = self.feedback_store.get_feedback_stats()
        
        # Get recent feedback (last 20)
        recent_feedback = self.feedback_store.search_feedback(limit=20)
        
        # Format for display
        formatted_recent = []
        for feedback in recent_feedback:
            formatted_recent.append({
                "id": feedback.id,
                "query": feedback.query,
                "feedback_type": feedback.feedback_type,
                "rating": feedback.rating,
                "timestamp": feedback.timestamp.isoformat(),
                "comment": feedback.comment[:100] + "..." if feedback.comment and len(feedback.comment) > 100 else feedback.comment
            })
            
        # Get learning opportunities
        opportunities = self.learning_manager.get_learning_opportunities()
        
        # Get top issues
        issues = self.feedback_analyzer.get_most_common_issues(limit=5)
        
        # Generate performance report
        report = self.feedback_analyzer.generate_performance_report(time_period="last_30_days")
        
        return {
            "stats": stats,
            "recent_feedback": formatted_recent,
            "learning_opportunities": opportunities,
            "top_issues": issues,
            "performance_report": report
        }
    
    def get_query_analysis(self, query: str) -> Dict[str, Any]:
        """
        Get detailed analysis for a specific query.
        
        Args:
            query: The query to analyze
            
        Returns:
            Dictionary with query analysis data
        """
        # Get query performance analysis
        performance = self.feedback_analyzer.analyze_query_performance(query)
        
        # Get similar queries
        similar_queries = self.feedback_store.get_similar_queries(query, threshold=0.7, limit=10)
        
        # Get all feedback for this query
        feedback = self.feedback_store.search_feedback(query=query, limit=100)
        
        # Format feedback for display
        formatted_feedback = []
        for entry in feedback:
            formatted_feedback.append({
                "id": entry.id,
                "feedback_type": entry.feedback_type,
                "rating": entry.rating,
                "timestamp": entry.timestamp.isoformat(),
                "comment": entry.comment,
                "source_count": len(entry.source_docs),
                "user_id": entry.user_id
            })
            
        return {
            "query": query,
            "performance": performance,
            "similar_queries": similar_queries,
            "feedback": formatted_feedback,
            "feedback_count": len(formatted_feedback)
        }









Modern Frontend UI (index.html)

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enterprise Knowledge Assistant</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f9fafb;
        }
        
        .chat-container {
            height: calc(100vh - 12rem);
        }
        
        .message-bubble {
            max-width: 80%;
            border-radius: 1rem;
            padding: 0.75rem 1rem;
            margin-bottom: 0.75rem;
            position: relative;
            line-height: 1.5;
        }
        
        .user-message {
            background-color: #e9f3ff;
            color: #0b306a;
            border-top-right-radius: 0.25rem;
            margin-left: auto;
        }
        
        .assistant-message {
            background-color: #f3f4f6;
            color: #1f2937;
            border-top-left-radius: 0.25rem;
        }
        
        .system-message {
            background-color: #fff7ed;
            color: #9a3412;
            font-size: 0.875rem;
            text-align: center;
            max-width: 100%;
            margin: 0.5rem auto;
        }
        
        .typing-indicator {
            display: inline-block;
            position: relative;
            width: 50px;
            height: 20px;
        }
        
        .typing-indicator span {
            height: 8px;
            width: 8px;
            float: left;
            margin: 0 1px;
            background-color: #9ca3af;
            display: block;
            border-radius: 50%;
            opacity: 0.4;
        }
        
        .typing-indicator span:nth-of-type(1) {
            animation: 1s blink infinite 0.3333s;
        }
        
        .typing-indicator span:nth-of-type(2) {
            animation: 1s blink infinite 0.6666s;
        }
        
        .typing-indicator span:nth-of-type(3) {
            animation: 1s blink infinite 0.9999s;
        }
        
        @keyframes blink {
            50% {
                opacity: 1;
            }
        }
        
        .source-tag {
            display: inline-block;
            padding: 0.125rem 0.375rem;
            font-size: 0.6875rem;
            font-weight: 500;
            border-radius: 0.25rem;
            margin-right: 0.375rem;
            margin-bottom: 0.25rem;
        }
        
        .confluence-tag {
            background-color: #e0f2fe;
            color: #0369a1;
        }
        
        .remedy-tag {
            background-color: #e0e7ff;
            color: #4338ca;
        }
        
        .code-block {
            background-color: #1e293b;
            color: #e2e8f0;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.75rem 0;
            overflow-x: auto;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.5;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 0.75rem 0;
            font-size: 0.875rem;
        }
        
        table th {
            background-color: #f3f4f6;
            font-weight: 600;
            text-align: left;
            padding: 0.5rem 0.75rem;
            border: 1px solid #e5e7eb;
        }
        
        table td {
            padding: 0.5rem 0.75rem;
            border: 1px solid #e5e7eb;
        }
        
        table tr:nth-child(even) {
            background-color: #f9fafb;
        }
        
        .source-section {
            background-color: #f3f4f6;
            border-radius: 0.5rem;
            padding: 0.75rem;
            margin-top: 0.75rem;
            font-size: 0.875rem;
            border-left: 3px solid #6b7280;
        }
        
        .source-section h4 {
            font-weight: 600;
            margin-bottom: 0.25rem;
        }
        
        .source-section p {
            margin: 0;
            color: #4b5563;
        }
        
        .source-link {
            color: #2563eb;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            font-size: 0.75rem;
            margin-top: 0.25rem;
        }
        
        .source-link:hover {
            text-decoration: underline;
        }
        
        .source-link i {
            font-size: 0.625rem;
            margin-left: 0.25rem;
        }
        
        .settings-panel {
            transition: right 0.3s ease-in-out;
        }
        
        /* Custom scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        
        ::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }
        
        ::-webkit-scrollbar-thumb {
            background: #c1c1c1;
            border-radius: 4px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: #a8a8a8;
        }
        
        /* Markdown content styling */
        .markdown-content h1 {
            font-size: 1.5rem;
            font-weight: 700;
            margin: 1rem 0 0.75rem 0;
        }
        
        .markdown-content h2 {
            font-size: 1.25rem;
            font-weight: 600;
            margin: 0.75rem 0 0.5rem 0;
        }
        
        .markdown-content h3 {
            font-size: 1.125rem;
            font-weight: 600;
            margin: 0.75rem 0 0.5rem 0;
        }
        
        .markdown-content p {
            margin: 0.5rem 0;
        }
        
        .markdown-content ul, .markdown-content ol {
            margin: 0.5rem 0;
            padding-left: 1.5rem;
        }
        
        .markdown-content ul {
            list-style-type: disc;
        }
        
        .markdown-content ol {
            list-style-type: decimal;
        }
        
        .markdown-content a {
            color: #2563eb;
            text-decoration: none;
        }
        
        .markdown-content a:hover {
            text-decoration: underline;
        }
        
        .markdown-content pre {
            background-color: #f3f4f6;
            padding: 0.75rem;
            border-radius: 0.25rem;
            overflow-x: auto;
            margin: 0.75rem 0;
        }
        
        .markdown-content code {
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 0.875rem;
            padding: 0.125rem 0.25rem;
            background-color: #f3f4f6;
            border-radius: 0.25rem;
        }
        
        .markdown-content blockquote {
            border-left: 3px solid #e5e7eb;
            padding-left: 1rem;
            margin: 0.75rem 0;
            color: #6b7280;
        }
        
        .settings-toggle-button {
            position: fixed;
            right: 1.5rem;
            bottom: 1.5rem;
            z-index: 50;
            background-color: white;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            border-radius: 9999px;
            padding: 0.75rem;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }
        
        .settings-toggle-button:hover {
            transform: scale(1.05);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        
        .pulse-effect {
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0% {
                box-shadow: 0 0 0 0 rgba(59, 130, 246, 0.7);
            }
            70% {
                box-shadow: 0 0 0 10px rgba(59, 130, 246, 0);
            }
            100% {
                box-shadow: 0 0 0 0 rgba(59, 130, 246, 0);
            }
        }
    </style>
</head>
<body>
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <!-- Header -->
        <header class="py-6">
            <div class="flex items-center justify-between">
                <div class="flex items-center space-x-3">
                    <div class="text-blue-600 text-3xl">
                        <i class="fas fa-brain"></i>
                    </div>
                    <div>
                        <h1 class="text-2xl font-bold text-gray-900">Enterprise Knowledge Assistant</h1>
                        <p class="text-sm text-gray-500">Powered by RAG technology</p>
                    </div>
                </div>
                <div>
                    <button id="clear-chat-btn" class="inline-flex items-center px-3 py-2 border border-gray-300 shadow-sm text-sm leading-4 font-medium rounded-md text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500">
                        <i class="fas fa-trash-alt mr-2"></i>
                        Clear Chat
                    </button>
                </div>
            </div>
        </header>
        
        <!-- Main Content -->
        <main class="bg-white shadow rounded-lg">
            <!-- Chat Interface -->
            <div class="chat-container flex flex-col overflow-hidden">
                <!-- Chat Messages -->
                <div id="chat-messages" class="flex-1 overflow-y-auto p-4">
                    <!-- Welcome Message -->
                    <div class="message-bubble assistant-message markdown-content">
                        <p> Hello! I'm your Enterprise Knowledge Assistant. I can help you find information from your organization's Confluence and Remedy systems.</p>
                        <p class="mt-2">You can ask me questions like:</p>
                        <ul>
                            <li>What's the status of incident INC1234567?</li>
                            <li>How do I reset my password?</li>
                            <li>What's the current deployment process?</li>
                            <li>Show me information about the Q2 project plan</li>
                        </ul>
                        <p class="mt-2">By default, I'll search both Confluence and Remedy, but you can specify which source you prefer.</p>
                    </div>
                </div>
                
                <!-- Chat Input -->
                <div class="border-t border-gray-200 p-4 bg-gray-50">
                    <div class="flex space-x-3">
                        <div class="source-selector flex space-x-1 items-center">
                            <select id="source-selector" class="block pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                                <option value="both" selected>Both Sources</option>
                                <option value="confluence">Confluence Only</option>
                                <option value="remedy">Remedy Only</option>
                            </select>
                        </div>
                        <div class="flex-1 relative rounded-md shadow-sm">
                            <input type="text" id="message-input" class="block w-full pr-10 sm:text-sm border-gray-300 rounded-md focus:ring-blue-500 focus:border-blue-500" placeholder="Ask me anything...">
                            <div class="absolute inset-y-0 right-0 pr-3 flex items-center">
                                <button id="send-message-btn" type="button" class="inline-flex items-center p-1 border border-transparent rounded-full shadow-sm text-white bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500">
                                    <svg class="h-5 w-5" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true">
                                        <path d="M10.894 2.553a1 1 0 00-1.788 0l-7 14a1 1 0 001.169 1.409l5-1.429A1 1 0 009 15.571V11a1 1 0 112 0v4.571a1 1 0 00.725.962l5 1.428a1 1 0 001.17-1.408l-7-14z" />
                                    </svg>
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </main>
        
        <!-- Footer -->
        <footer class="mt-8 text-center text-sm text-gray-500">
            <p>&copy; 2025 Your Company Name. All rights reserved.</p>
        </footer>
    </div>
    
    <!-- Settings Panel Toggle Button -->
    <button id="settings-toggle" class="settings-toggle-button pulse-effect">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 text-blue-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z" />
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
        </svg>
    </button>
    
    <!-- Settings Panel -->
    <div id="settings-panel" class="settings-panel fixed top-0 right-0 w-80 h-full bg-white shadow-xl transform translate-x-full transition-transform duration-300 z-50 overflow-y-auto">
        <div class="p-4 border-b border-gray-200 flex justify-between items-center">
            <h2 class="text-lg font-medium text-gray-900">Settings</h2>
            <button id="close-settings" class="text-gray-400 hover:text-gray-500">
                <svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                </svg>
            </button>
        </div>
        
        <div class="p-4">
            <div class="mb-6">
                <h3 class="text-sm font-medium text-gray-900 mb-2">Response Format</h3>
                <div class="space-y-2">
                    <div class="flex items-center">
                        <input id="format-concise" name="response-format" type="radio" checked class="h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300">
                        <label for="format-concise" class="ml-2 block text-sm text-gray-700">Concise</label>
                    </div>
                    <div class="flex items-center">
                        <input id="format-detailed" name="response-format" type="radio" class="h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300">
                        <label for="format-detailed" class="ml-2 block text-sm text-gray-700">Detailed</label>
                    </div>
                </div>
            </div>
            
            <div class="mb-6">
                <h3 class="text-sm font-medium text-gray-900 mb-2">Source Attribution</h3>
                <div class="relative flex items-start">
                    <div class="flex items-center h-5">
                        <input id="show-sources" type="checkbox" checked class="h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded">
                    </div>
                    <div class="ml-3 text-sm">
                        <label for="show-sources" class="text-gray-700">Show source references</label>
                        <p class="text-gray-500">Include links to original content in responses</p>
                    </div>
                </div>
            </div>
            
            <div class="mb-6">
                <h3 class="text-sm font-medium text-gray-900 mb-2">Language</h3>
                <select id="language-selector" class="mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                    <option value="en">English</option>
                    <option value="es">Espaol</option>
                    <option value="fr">Franais</option>
                    <option value="de">Deutsch</option>
                    <option value="zh"></option>
                    <option value="ja"></option>
                </select>
            </div>
            
            <div>
                <h3 class="text-sm font-medium text-gray-900 mb-2">Theme</h3>
                <div class="flex space-x-2">
                    <button class="w-8 h-8 rounded-full bg-white border border-gray-300 focus:outline-none ring-2 ring-offset-2 ring-blue-500"></button>
                    <button class="w-8 h-8 rounded-full bg-gray-900 border border-gray-900 focus:outline-none"></button>
                    <button class="w-8 h-8 rounded-full bg-blue-900 border border-blue-900 focus:outline-none"></button>
                    <button class="w-8 h-8 rounded-full bg-indigo-700 border border-indigo-700 focus:outline-none"></button>
                </div>
            </div>
        </div>
    </div>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const messageInput = document.getElementById('message-input');
            const sendButton = document.getElementById('send-message-btn');
            const chatMessages = document.getElementById('chat-messages');
            const sourceSelector = document.getElementById('source-selector');
            const clearChatButton = document.getElementById('clear-chat-btn');
            const settingsToggle = document.getElementById('settings-toggle');
            const settingsPanel = document.getElementById('settings-panel');
            const closeSettings = document.getElementById('close-settings');
            
            // Remove pulse effect after a while
            setTimeout(() => {
                settingsToggle.classList.remove('pulse-effect');
            }, 5000);
            
            // Settings panel toggle
            settingsToggle.addEventListener('click', function() {
                settingsPanel.classList.toggle('translate-x-full');
            });
            
            closeSettings.addEventListener('click', function() {
                settingsPanel.classList.add('translate-x-full');
            });
            
            // Send message on enter key
            messageInput.addEventListener('keydown', function(e) {
                if (e.key === 'Enter') {
                    sendMessage();
                }
            });
            
            // Send message on button click
            sendButton.addEventListener('click', sendMessage);
            
            // Clear chat history
            clearChatButton.addEventListener('click', function() {
                if (confirm('Are you sure you want to clear the chat history?')) {
                    chatMessages.innerHTML = '';
                    // Add welcome message back
                    addAssistantMessage(`
                        <p> Hello! I'm your Enterprise Knowledge Assistant. I can help you find information from your organization's Confluence and Remedy systems.</p>
                        <p class="mt-2">You can ask me questions like:</p>
                        <ul>
                            <li>What's the status of incident INC1234567?</li>
                            <li>How do I reset my password?</li>
                            <li>What's the current deployment process?</li>
                            <li>Show me information about the Q2 project plan</li>
                        </ul>
                        <p class="mt-2">By default, I'll search both Confluence and Remedy, but you can specify which source you prefer.</p>
                    `);
                    // Clear message history in local storage
                    localStorage.removeItem('chatHistory');
                }
            });
            
            function sendMessage() {
                const message = messageInput.value.trim();
                if (!message) return;
                
                // Display user message
                addUserMessage(message);
                
                // Clear input
                messageInput.value = '';
                
                // Show typing indicator
                addTypingIndicator();
                
                // Get selected source
                const selectedSource = sourceSelector.value;
                
                // Make API request
                fetchResponse(message, selectedSource);
            }
            
            function addUserMessage(message) {
                const messageElement = document.createElement('div');
                messageElement.className = 'message-bubble user-message';
                messageElement.textContent = message;
                chatMessages.appendChild(messageElement);
                scrollToBottom();
                
                // Save to chat history
                saveMessageToHistory('user', message);
            }
            
            function addAssistantMessage(message, sources = []) {
                // Remove typing indicator if exists
                removeTypingIndicator();
                
                const messageElement = document.createElement('div');
                messageElement.className = 'message-bubble assistant-message markdown-content';
                messageElement.innerHTML = message;
                
                // Add sources if available
                if (sources && sources.length > 0) {
                    const sourcesElement = document.createElement('div');
                    sourcesElement.className = 'source-section mt-3';
                    
                    const sourcesTitle = document.createElement('h4');
                    sourcesTitle.textContent = 'Sources:';
                    sourcesElement.appendChild(sourcesTitle);
                    
                    sources.forEach(source => {
                        const sourceLink = document.createElement('a');
                        sourceLink.className = 'source-link block';
                        sourceLink.href = source.url;
                        sourceLink.target = '_blank';
                        sourceLink.innerHTML = `${source.title} <i class="fas fa-external-link-alt"></i>`;
                        sourcesElement.appendChild(sourceLink);
                    });
                    
                    messageElement.appendChild(sourcesElement);
                }
                
                chatMessages.appendChild(messageElement);
                scrollToBottom();
                
                // Save to chat history
                saveMessageToHistory('assistant', message, sources);
            }
            
            function addSystemMessage(message) {
                const messageElement = document.createElement('div');
                messageElement.className = 'message-bubble system-message';
                messageElement.textContent = message;
                chatMessages.appendChild(messageElement);
                scrollToBottom();
            }
            
            function addTypingIndicator() {
                removeTypingIndicator(); // Remove existing indicator if any
                
                const typingElement = document.createElement('div');
                typingElement.className = 'message-bubble assistant-message typing-indicator-container';
                typingElement.innerHTML = `
                    <div class="typing-indicator">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                `;
                typingElement.id = 'typing-indicator';
                chatMessages.appendChild(typingElement);
                scrollToBottom();
            }
            
            function removeTypingIndicator() {
                const typingIndicator = document.getElementById('typing-indicator');
                if (typingIndicator) {
                    typingIndicator.remove();
                }
            }
            
            function scrollToBottom() {
                chatMessages.scrollTop = chatMessages.scrollHeight;
            }
            
            function saveMessageToHistory(role, content, sources = []) {
                let history = JSON.parse(localStorage.getItem('chatHistory') || '[]');
                history.push({ role, content, sources, timestamp: new Date().toISOString() });
                localStorage.setItem('chatHistory', JSON.stringify(history));
            }
            
            function loadChatHistory() {
                const history = JSON.parse(localStorage.getItem('chatHistory') || '[]');
                if (history.length > 0) {
                    // Clear the welcome message
                    chatMessages.innerHTML = '';
                    
                    // Add messages from history
                    history.forEach(msg => {
                        if (msg.role === 'user') {
                            addUserMessage(msg.content);
                        } else if (msg.role === 'assistant') {
                            addAssistantMessage(msg.content, msg.sources);
                        }
                    });
                }
            }
            
            async function fetchResponse(query, source) {
                try {
                    // Get format preference
                    const formatConcise = document.getElementById('format-concise').checked;
                    const showSources = document.getElementById('show-sources').checked;
                    const language = document.getElementById('language-selector').value;
                    
                    const response = await fetch('/api/query', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            query: query,
                            source: source,
                            format: formatConcise ? 'concise' : 'detailed',
                            include_sources: showSources,
                            language: language
                        }),
                    });
                    
                    if (!response.ok) {
                        throw new Error('API request failed');
                    }
                    
                    const data = await response.json();
                    
                    // Remove typing indicator
                    removeTypingIndicator();
                    
                    // Display response
                    if (data.success) {
                        addAssistantMessage(data.response, data.sources);
                    } else {
                        addSystemMessage('Sorry, I encountered an error while processing your request.');
                    }
                } catch (error) {
                    console.error('Error:', error);
                    removeTypingIndicator();
                    addSystemMessage('Sorry, there was an error connecting to the server. Please try again later.');
                }
            }
            
            // Sample mock function for testing UI
            function mockApiResponse(query, source) {
                // Remove typing indicator after a delay to simulate processing
                setTimeout(() => {
                    // Example response based on source
                    if (source === 'confluence' || source === 'both') {
                        addAssistantMessage(`
                            <h3>Password Reset Process</h3>
                            <p>Based on our company policy, here's how to reset your password:</p>
                            <ol>
                                <li>Go to the <a href="#">password portal</a></li>
                                <li>Click "Forgot Password"</li>
                                <li>Enter your email address</li>
                                <li>Follow the instructions in the email you receive</li>
                            </ol>
                            <p>Remember that passwords must be changed every 90 days and must contain:</p>
                            <ul>
                                <li>At least 8 characters</li>
                                <li>Upper and lowercase letters</li>
                                <li>At least one number</li>
                                <li>At least one special character</li>
                            </ul>
                        `, [
                            { title: "Password Policy Document", url: "#" },
                            { title: "IT Security Guidelines", url: "#" }
                        ]);
                    } else if (source === 'remedy') {
                        addAssistantMessage(`
                            <p>I found information about your recent password reset ticket:</p>
                            <div class="bg-gray-50 rounded p-3 my-2">
                                <div class="grid grid-cols-2 gap-2 text-sm">
                                    <div class="font-medium">Ticket ID:</div>
                                    <div>INC0012345</div>
                                    <div class="font-medium">Status:</div>
                                    <div><span class="bg-green-100 text-green-800 px-2 py-0.5 rounded">Resolved</span></div>
                                    <div class="font-medium">Created:</div>
                                    <div>March 18, 2025 10:32 AM</div>
                                    <div class="font-medium">Resolved:</div>
                                    <div>March 18, 2025 2:15 PM</div>
                                </div>
                            </div>
                            <p>The issue was resolved by the service desk. They reset your password and sent the temporary password to your registered mobile number.</p>
                        `, [
                            { title: "Ticket INC0012345", url: "#" }
                        ]);
                    }
                }, 1500);
            }
            
            // Load chat history on page load
            loadChatHistory();
        });
    </script>
</body>
</html>














Frontend JavaScript (app.js)

/**
 * Enterprise Knowledge Assistant - Frontend JavaScript
 * 
 * This file handles the frontend logic for the chat interface,
 * making API calls to the backend, and rendering responses.
 */

// Configuration
const CONFIG = {
    apiEndpoint: '/api/query',
    feedbackEndpoint: '/api/feedback',
    defaultSource: 'both',
    defaultFormat: 'concise',
    defaultLanguage: 'en',
    typingDelay: {
        min: 500,  // Minimum typing delay in ms
        max: 2000  // Maximum typing delay in ms based on response length
    },
    maxHistoryItems: 50,
    markdownOptions: {
        headerIds: false,
        breaks: true
    }
};

// DOM Elements
const DOM = {
    messageInput: document.getElementById('message-input'),
    sendButton: document.getElementById('send-message-btn'),
    chatMessages: document.getElementById('chat-messages'),
    sourceSelector: document.getElementById('source-selector'),
    clearChatButton: document.getElementById('clear-chat-btn'),
    settingsToggle: document.getElementById('settings-toggle'),
    settingsPanel: document.getElementById('settings-panel'),
    closeSettings: document.getElementById('close-settings'),
    formatConcise: document.getElementById('format-concise'),
    formatDetailed: document.getElementById('format-detailed'),
    showSources: document.getElementById('show-sources'),
    languageSelector: document.getElementById('language-selector')
};

/**
 * Message Templates - HTML templates for different message types
 */
const MessageTemplates = {
    /**
     * Generate HTML for user message
     * @param {string} message - User message text
     * @param {string} timestamp - ISO timestamp string
     * @returns {string} HTML string
     */
    user: (message, timestamp) => `
        <div class="message-bubble user-message" data-timestamp="${timestamp || new Date().toISOString()}">
            ${escapeHtml(message)}
        </div>
    `,
    
    /**
     * Generate HTML for assistant message
     * @param {string} message - Assistant message content (can include HTML/markdown)
     * @param {Array} sources - Array of source objects
     * @param {string} timestamp - ISO timestamp string
     * @returns {string} HTML string
     */
    assistant: (message, sources = [], timestamp) => `
        <div class="message-bubble assistant-message markdown-content" data-timestamp="${timestamp || new Date().toISOString()}">
            ${message}
            ${sources && sources.length > 0 ? MessageTemplates.sources(sources) : ''}
        </div>
    `,
    
    /**
     * Generate HTML for system message
     * @param {string} message - System message text
     * @returns {string} HTML string
     */
    system: (message) => `
        <div class="message-bubble system-message">
            ${escapeHtml(message)}
        </div>
    `,
    
    /**
     * Generate HTML for typing indicator
     * @returns {string} HTML string
     */
    typingIndicator: () => `
        <div class="message-bubble assistant-message typing-indicator-container" id="typing-indicator">
            <div class="typing-indicator">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    `,
    
    /**
     * Generate HTML for source references
     * @param {Array} sources - Array of source objects with title and url properties
     * @returns {string} HTML string
     */
    sources: (sources) => `
        <div class="source-section mt-3">
            <h4>Sources:</h4>
            ${sources.map(source => `
                <a class="source-link block" href="${source.url}" target="_blank">
                    <span class="source-tag ${source.type === 'confluence' ? 'confluence-tag' : 'remedy-tag'}">
                        ${source.type === 'confluence' ? 'Confluence' : 'Remedy'}
                    </span>
                    ${escapeHtml(source.title)} <i class="fas fa-external-link-alt"></i>
                </a>
            `).join('')}
        </div>
    `,
    
    /**
     * Generate HTML for feedback buttons
     * @param {string} messageId - Unique ID for the message
     * @returns {string} HTML string
     */
    feedback: (messageId) => `
        <div class="feedback-buttons mt-2 flex items-center space-x-2">
            <button class="feedback-btn text-gray-400 hover:text-green-500" data-message-id="${messageId}" data-value="helpful">
                <i class="far fa-thumbs-up"></i>
            </button>
            <button class="feedback-btn text-gray-400 hover:text-red-500" data-message-id="${messageId}" data-value="unhelpful">
                <i class="far fa-thumbs-down"></i>
            </button>
        </div>
    `
};

/**
 * Chat Manager - Handles chat logic and UI interactions
 */
class ChatManager {
    constructor() {
        this.history = [];
        this.loadHistory();
        this.setupEventListeners();
    }
    
    /**
     * Set up all event listeners
     */
    setupEventListeners() {
        // Send message on enter key
        DOM.messageInput.addEventListener('keydown', e => {
            if (e.key === 'Enter') {
                this.sendMessage();
            }
        });
        
        // Send message on button click
        DOM.sendButton.addEventListener('click', () => this.sendMessage());
        
        // Clear chat history
        DOM.clearChatButton.addEventListener('click', () => this.clearChat());
        
        // Settings panel toggle
        DOM.settingsToggle.addEventListener('click', () => {
            DOM.settingsPanel.classList.toggle('translate-x-full');
        });
        
        // Close settings panel
        DOM.closeSettings.addEventListener('click', () => {
            DOM.settingsPanel.classList.add('translate-x-full');
        });
        
        // Setup feedback handler using delegation
        DOM.chatMessages.addEventListener('click', e => {
            const feedbackBtn = e.target.closest('.feedback-btn');
            if (feedbackBtn) {
                this.handleFeedback(
                    feedbackBtn.dataset.messageId,
                    feedbackBtn.dataset.value
                );
                
                // Highlight selected button
                const btnContainer = feedbackBtn.parentElement;
                btnContainer.querySelectorAll('.feedback-btn').forEach(btn => {
                    btn.classList.remove('text-green-500', 'text-red-500');
                    btn.classList.add('text-gray-400');
                });
                
                if (feedbackBtn.dataset.value === 'helpful') {
                    feedbackBtn.classList.remove('text-gray-400');
                    feedbackBtn.classList.add('text-green-500');
                } else {
                    feedbackBtn.classList.remove('text-gray-400');
                    feedbackBtn.classList.add('text-red-500');
                }
            }
        });
    }
    
    /**
     * Send a message and get a response
     */
    sendMessage() {
        const message = DOM.messageInput.value.trim();
        if (!message) return;
        
        // Display user message
        this.addUserMessage(message);
        
        // Clear input
        DOM.messageInput.value = '';
        
        // Show typing indicator
        this.addTypingIndicator();
        
        // Get query settings
        const querySettings = {
            source: DOM.sourceSelector.value,
            format: DOM.formatConcise.checked ? 'concise' : 'detailed',
            include_sources: DOM.showSources.checked,
            language: DOM.languageSelector.value
        };
        
        // Make API request
        this.fetchResponse(message, querySettings);
    }
    
    /**
     * Add a user message to the chat
     * @param {string} message - Message text
     */
    addUserMessage(message) {
        const timestamp = new Date().toISOString();
        const html = MessageTemplates.user(message, timestamp);
        this.appendMessageToChat(html);
        
        // Save to chat history
        this.saveMessageToHistory('user', message, [], timestamp);
    }
    
    /**
     * Add an assistant message to the chat
     * @param {string} message - Message content
     * @param {Array} sources - Array of source objects
     */
    addAssistantMessage(message, sources = []) {
        // Remove typing indicator if exists
        this.removeTypingIndicator();
        
        const timestamp = new Date().toISOString();
        const messageId = `msg-${Date.now()}`;
        
        // Construct HTML with both message content and feedback buttons
        let html = MessageTemplates.assistant(message, sources, timestamp);
        
        // Insert into chat
        this.appendMessageToChat(html);
        
        // Add feedback buttons after a slight delay
        setTimeout(() => {
            const messageElem = DOM.chatMessages.lastElementChild;
            if (messageElem && messageElem.classList.contains('assistant-message')) {
                const feedbackElem = document.createElement('div');
                feedbackElem.innerHTML = MessageTemplates.feedback(messageId);
                messageElem.appendChild(feedbackElem.firstElementChild);
            }
        }, 500);
        
        // Save to chat history
        this.saveMessageToHistory('assistant', message, sources, timestamp);
    }
    
    /**
     * Add a system message to the chat
     * @param {string} message - Message text
     */
    addSystemMessage(message) {
        const html = MessageTemplates.system(message);
        this.appendMessageToChat(html);
    }
    
    /**
     * Add a typing indicator to show the assistant is responding
     */
    addTypingIndicator() {
        this.removeTypingIndicator(); // Remove existing indicator if any
        const html = MessageTemplates.typingIndicator();
        this.appendMessageToChat(html, false); // Don't scroll for typing indicator
    }
    
    /**
     * Remove the typing indicator
     */
    removeTypingIndicator() {
        const typingIndicator = document.getElementById('typing-indicator');
        if (typingIndicator) {
            typingIndicator.remove();
        }
    }
    
    /**
     * Append a message to the chat container
     * @param {string} html - HTML string for the message
     * @param {boolean} scroll - Whether to scroll to bottom (default: true)
     */
    appendMessageToChat(html, scroll = true) {
        // Create temporary element to convert string to DOM
        const temp = document.createElement('div');
        temp.innerHTML = html;
        
        // Append the actual element
        DOM.chatMessages.appendChild(temp.firstElementChild);
        
        // Scroll to bottom if requested
        if (scroll) {
            this.scrollToBottom();
        }
    }
    
    /**
     * Scroll the chat container to the bottom
     */
    scrollToBottom() {
        DOM.chatMessages.scrollTop = DOM.chatMessages.scrollHeight;
    }
    
    /**
     * Save a message to the chat history
     * @param {string} role - Message role ('user' or 'assistant')
     * @param {string} content - Message content
     * @param {Array} sources - Array of source objects
     * @param {string} timestamp - ISO timestamp string
     */
    saveMessageToHistory(role, content, sources = [], timestamp = null) {
        // Add to in-memory history
        this.history.push({ 
            role, 
            content, 
            sources, 
            timestamp: timestamp || new Date().toISOString() 
        });
        
        // Trim history if it gets too long
        if (this.history.length > CONFIG.maxHistoryItems) {
            this.history = this.history.slice(-CONFIG.maxHistoryItems);
        }
        
        // Save to local storage
        localStorage.setItem('chatHistory', JSON.stringify(this.history));
    }
    
    /**
     * Load chat history from local storage
     */
    loadHistory() {
        try {
            const storedHistory = localStorage.getItem('chatHistory');
            if (storedHistory) {
                this.history = JSON.parse(storedHistory);
                
                if (this.history.length > 0) {
                    // Clear the welcome message if we have history
                    DOM.chatMessages.innerHTML = '';
                    
                    // Add messages from history
                    this.history.forEach(msg => {
                        if (msg.role === 'user') {
                            const html = MessageTemplates.user(msg.content, msg.timestamp);
                            this.appendMessageToChat(html, false);
                        } else if (msg.role === 'assistant') {
                            const html = MessageTemplates.assistant(msg.content, msg.sources, msg.timestamp);
                            this.appendMessageToChat(html, false);
                        }
                    });
                    
                    // Scroll to bottom after loading history
                    this.scrollToBottom();
                }
            }
        } catch (error) {
            console.error('Error loading chat history:', error);
            // If there's an error, reset history
            this.history = [];
        }
    }
    
    /**
     * Clear the chat history
     */
    clearChat() {
        if (confirm('Are you sure you want to clear the chat history?')) {
            // Clear DOM
            DOM.chatMessages.innerHTML = '';
            
            // Add welcome message back
            const welcomeMessage = `
                <p> Hello! I'm your Enterprise Knowledge Assistant. I can help you find information from your organization's Confluence and Remedy systems.</p>
                <p class="mt-2">You can ask me questions like:</p>
                <ul>
                    <li>What's the status of incident INC1234567?</li>
                    <li>How do I reset my password?</li>
                    <li>What's the current deployment process?</li>
                    <li>Show me information about the Q2 project plan</li>
                </ul>
                <p class="mt-2">By default, I'll search both Confluence and Remedy, but you can specify which source you prefer.</p>
            `;
            this.addAssistantMessage(welcomeMessage);
            
            // Clear history
            this.history = [];
            localStorage.removeItem('chatHistory');
        }
    }
    
    /**
     * Handle user feedback on assistant responses
     * @param {string} messageId - Unique ID of the message
     * @param {string} value - Feedback value ('helpful' or 'unhelpful')
     */
    async handleFeedback(messageId, value) {
        try {
            // Find the message in history
            const messageIndex = parseInt(messageId.split('-')[1]);
            
            // Get last user query and assistant response
            let userQuery = '';
            let assistantResponse = '';
            
            // Find last user message before this assistant message
            for (let i = this.history.length - 1; i >= 0; i--) {
                if (this.history[i].role === 'user') {
                    userQuery = this.history[i].content;
                    break;
                }
            }
            
            // Send feedback to the server
            const response = await fetch(CONFIG.feedbackEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    message_id: messageId,
                    feedback: value,
                    query: userQuery
                }),
            });
            
            if (!response.ok) {
                throw new Error('Failed to submit feedback');
            }
            
            console.log(`Feedback submitted: ${value}`);
            
            // Show a temporary confirmation
            const messageElement = document.querySelector(`[data-message-id="${messageId}"]`).closest('.message-bubble');
            const confirmationElement = document.createElement('div');
            confirmationElement.textContent = 'Thanks for your feedback!';
            confirmationElement.className = 'text-xs text-gray-500 mt-1';
            
            messageElement.appendChild(confirmationElement);
            
            // Remove confirmation after a delay
            setTimeout(() => {
                confirmationElement.remove();
            }, 3000);
            
        } catch (error) {
            console.error('Error submitting feedback:', error);
        }
    }
    
    /**
     * Fetch response from the API
     * @param {string} query - User query
     * @param {Object} settings - Query settings
     */
    async fetchResponse(query, settings) {
        try {
            const response = await fetch(CONFIG.apiEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    query: query,
                    source: settings.source,
                    format: settings.format,
                    include_sources: settings.include_sources,
                    language: settings.language
                }),
            });
            
            if (!response.ok) {
                throw new Error('API request failed');
            }
            
            const data = await response.json();
            
            // Calculate typing delay based on response length
            const responseLength = data.response ? data.response.length : 0;
            const typingDelay = Math.min(
                CONFIG.typingDelay.max,
                CONFIG.typingDelay.min + (responseLength / 10)
            );
            
            // Simulate typing delay for a more natural feel
            setTimeout(() => {
                // Display response
                if (data.success) {
                    this.addAssistantMessage(data.response, data.sources);
                } else {
                    this.addSystemMessage('Sorry, I encountered an error while processing your request.');
                }
            }, typingDelay);
            
        } catch (error) {
            console.error('Error:', error);
            this.removeTypingIndicator();
            this.addSystemMessage('Sorry, there was an error connecting to the server. Please try again later.');
        }
    }
}

/**
 * Escape HTML special characters to prevent XSS
 * @param {string} unsafe - String that might contain HTML
 * @returns {string} Escaped HTML string
 */
function escapeHtml(unsafe) {
    return unsafe
        .replace(/&/g, "&amp;")
        .replace(/</g, "&lt;")
        .replace(/>/g, "&gt;")
        .replace(/"/g, "&quot;")
        .replace(/'/g, "&#039;");
}

/**
 * Format timestamp into readable string
 * @param {string} timestamp - ISO timestamp string
 * @returns {string} Formatted timestamp
 */
function formatTimestamp(timestamp) {
    const date = new Date(timestamp);
    return date.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
}

// Initialize chat when DOM is ready
document.addEventListener('DOMContentLoaded', function() {
    window.chatManager = new ChatManager();
    
    // Remove pulse effect from settings button after a while
    setTimeout(() => {
        if (DOM.settingsToggle) {
            DOM.settingsToggle.classList.remove('pulse-effect');
        }
    }, 5000);
});











API Routes (routes.py)



"""
API routes for the RAG system.

This module defines the API endpoints for the RAG system, including query processing,
feedback collection, and system management.
"""
import time
import logging
from typing import Dict, Any, List, Optional, Union
from fastapi import FastAPI, APIRouter, Request, Response, HTTPException, BackgroundTasks, Depends, Header, Body, Query, Path
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from datetime import datetime
import uuid
import re

# Import custom modules
from config.config import config
from processors.document_processor import DocumentProcessor
from nlp.query_analysis import QueryAnalyzer
from retrieval.hybrid_search import HybridSearchRetriever
from nlp.answer_generation import AnswerGenerator
from connectors.confluence import ConfluenceConnector
from connectors.remedy import RemedyConnector
from utils.logger import setup_logger

# Setup logging
logger = setup_logger(__name__)

# Create API router
router = APIRouter()

# Models for request/response validation

class QueryRequest(BaseModel):
    """Request model for querying the system."""
    query: str = Field(..., description="User query text")
    source: str = Field("both", description="Source to query: 'confluence', 'remedy', or 'both'")
    format: str = Field("concise", description="Response format: 'concise' or 'detailed'")
    include_sources: bool = Field(True, description="Whether to include source references")
    language: str = Field("en", description="Response language code")
    
    class Config:
        schema_extra = {
            "example": {
                "query": "What is the status of incident INC1234567?",
                "source": "both",
                "format": "concise",
                "include_sources": True,
                "language": "en"
            }
        }

class Source(BaseModel):
    """Model for a content source reference."""
    title: str = Field(..., description="Title of the source")
    url: str = Field(..., description="URL to the source")
    type: str = Field(..., description="Type of source: 'confluence' or 'remedy'")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class QueryResponse(BaseModel):
    """Response model for query endpoint."""
    success: bool = Field(..., description="Whether the request was successful")
    query: str = Field(..., description="Original query text")
    response: str = Field(..., description="Generated response")
    sources: Optional[List[Source]] = Field(None, description="Source references")
    response_time: float = Field(..., description="Time taken to generate response in seconds")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "query": "What is the status of incident INC1234567?",
                "response": "Incident INC1234567 is currently in 'In Progress' status and is assigned to the Network Support team. It was reported on March 15, 2025, and has a Priority 2 (High) designation.",
                "sources": [
                    {
                        "title": "Incident INC1234567",
                        "url": "https://remedy.company.com/incident/INC1234567",
                        "type": "remedy",
                        "metadata": {"last_updated": "2025-03-18T14:23:00Z"}
                    }
                ],
                "response_time": 0.753,
                "metadata": {
                    "source_counts": {"confluence": 0, "remedy": 1},
                    "query_type": "incident_status"
                }
            }
        }

class FeedbackRequest(BaseModel):
    """Request model for user feedback."""
    message_id: str = Field(..., description="ID of the message receiving feedback")
    feedback: str = Field(..., description="Feedback value: 'helpful' or 'unhelpful'")
    query: Optional[str] = Field(None, description="Original query text")
    comments: Optional[str] = Field(None, description="User comments")
    
    class Config:
        schema_extra = {
            "example": {
                "message_id": "msg-1647832484562",
                "feedback": "helpful",
                "query": "What is the status of incident INC1234567?",
                "comments": "This was exactly what I needed!"
            }
        }

class FeedbackResponse(BaseModel):
    """Response model for feedback endpoint."""
    success: bool = Field(..., description="Whether the feedback was received")
    feedback_id: str = Field(..., description="Unique ID for the feedback")
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "feedback_id": "fb-1647832484562"
            }
        }

class SystemStatusResponse(BaseModel):
    """Response model for system status endpoint."""
    status: str = Field(..., description="System status")
    version: str = Field(..., description="System version")
    uptime: float = Field(..., description="System uptime in seconds")
    components: Dict[str, bool] = Field(..., description="Component status")
    
    class Config:
        schema_extra = {
            "example": {
                "status": "operational",
                "version": "1.0.0",
                "uptime": 86400.5,
                "components": {
                    "confluence_connector": True,
                    "remedy_connector": True,
                    "document_processor": True,
                    "hybrid_search": True,
                    "answer_generator": True
                }
            }
        }

# Global components (initialized in startup event)
components = {
    "confluence_connector": None,
    "remedy_connector": None,
    "document_processor": None,
    "query_analyzer": None,
    "hybrid_search": None,
    "answer_generator": None,
    "start_time": time.time()
}

# Source controller for managing which knowledge sources to query
class SourceController:
    """Controller for managing which knowledge sources to query."""
    
    @staticmethod
    def get_sources(source_preference: str) -> List[str]:
        """
        Get list of sources to query based on user preference.
        
        Args:
            source_preference: User's source preference ('confluence', 'remedy', or 'both')
            
        Returns:
            List of source names to query
        """
        if source_preference == "confluence":
            return ["confluence"]
        elif source_preference == "remedy":
            return ["remedy"]
        else:  # Default to both
            return ["confluence", "remedy"]
    
    @staticmethod
    def format_sources(sources: List[Dict[str, Any]]) -> List[Source]:
        """
        Format source information for response.
        
        Args:
            sources: List of source dictionaries
            
        Returns:
            List of formatted Source objects
        """
        formatted_sources = []
        
        for source in sources:
            source_type = "confluence" if "page_id" in source else "remedy"
            
            if source_type == "confluence":
                url = f"{config.get('confluence', 'url')}/pages/viewpage.action?pageId={source.get('page_id')}"
                title = source.get('title', 'Confluence Page')
            else:  # remedy
                url = f"{config.get('remedy', 'url')}/incident/{source.get('ticket_id')}"
                title = f"Ticket {source.get('ticket_id')}"
                
            formatted_sources.append(Source(
                title=title,
                url=url,
                type=source_type,
                metadata=source.get('metadata')
            ))
            
        return formatted_sources

# Request handling functions

@router.post("/query", response_model=QueryResponse)
async def process_query(request: QueryRequest, background_tasks: BackgroundTasks):
    """
    Process a user query and generate a response.
    
    Args:
        request: Query request containing the query text and parameters
        background_tasks: FastAPI background tasks
        
    Returns:
        Generated response with source information
    """
    start_time = time.time()
    query = request.query
    
    logger.info(f"Processing query: '{query}' with source={request.source}")
    
    try:
        # Validate components
        for key, component in components.items():
            if key != "start_time" and component is None:
                logger.error(f"Component {key} not initialized")
                raise HTTPException(status_code=500, detail="System not fully initialized")
        
        # Analyze query
        query_info = components["query_analyzer"].analyze_query(query)
        
        # Determine which sources to query
        sources_to_query = SourceController.get_sources(request.source)
        logger.debug(f"Querying sources: {sources_to_query}")
        
        # Retrieve relevant information from each source
        all_results = []
        source_counts = {"confluence": 0, "remedy": 0}
        
        if "confluence" in sources_to_query:
            try:
                # Query Confluence
                logger.debug("Querying Confluence...")
                confluence_results = components["confluence_connector"].search_content(query, limit=10)
                
                if confluence_results:
                    confluence_processed = []
                    for result in confluence_results:
                        if result:
                            # Process the content
                            processed_content = components["confluence_connector"].parse_content(result)
                            
                            # Add to results if valid
                            if processed_content and "metadata" in processed_content:
                                confluence_processed.append({
                                    "page_id": result.get("id"),
                                    "title": processed_content["metadata"].get("title"),
                                    "content": processed_content["text"],
                                    "metadata": {
                                        "created_at": processed_content["metadata"].get("created_at"),
                                        "updated_at": processed_content["metadata"].get("updated_at")
                                    }
                                })
                    
                    all_results.extend(confluence_processed)
                    source_counts["confluence"] = len(confluence_processed)
            except Exception as e:
                logger.error(f"Error querying Confluence: {str(e)}")
        
        if "remedy" in sources_to_query:
            try:
                # Query Remedy
                logger.debug("Querying Remedy...")
                
                # Check if query contains a ticket number
                ticket_match = re.search(r'INC\d+', query, re.IGNORECASE)
                
                if ticket_match:
                    # Direct ticket lookup
                    ticket_id = ticket_match.group(0)
                    ticket = components["remedy_connector"].get_ticket_with_details("incident", ticket_id)
                    
                    if ticket:
                        all_results.append({
                            "ticket_id": ticket_id,
                            "title": f"Incident {ticket_id}",
                            "content": f"Status: {ticket.get('status')}\nAssigned To: {ticket.get('assigned_to')}\nPriority: {ticket.get('priority')}\nDescription: {ticket.get('description')}",
                            "metadata": {
                                "created_at": ticket.get("created_at"),
                                "updated_at": ticket.get("updated_at"),
                                "status": ticket.get("status"),
                                "priority": ticket.get("priority")
                            }
                        })
                        source_counts["remedy"] = 1
                else:
                    # Keyword search
                    remedy_results = components["remedy_connector"].search_tickets_by_keyword(
                        "incident", [word for word in query.split() if len(word) > 3], limit=5
                    )
                    
                    if remedy_results:
                        remedy_processed = []
                        for result in remedy_results:
                            normalized = components["remedy_connector"]._normalize_ticket_fields(result, "incident")
                            remedy_processed.append({
                                "ticket_id": normalized.get("id"),
                                "title": f"Incident {normalized.get('id')}",
                                "content": f"Status: {normalized.get('status')}\nAssigned To: {normalized.get('assigned_to')}\nPriority: {normalized.get('priority')}\nDescription: {normalized.get('description')}",
                                "metadata": {
                                    "created_at": normalized.get("created_at"),
                                    "updated_at": normalized.get("updated_at"),
                                    "status": normalized.get("status"),
                                    "priority": normalized.get("priority")
                                }
                            })
                        
                        all_results.extend(remedy_processed)
                        source_counts["remedy"] = len(remedy_processed)
            except Exception as e:
                logger.error(f"Error querying Remedy: {str(e)}")
        
        # If no results, return a no-results message
        if not all_results:
            logger.info("No results found for query")
            
            response_text = "I couldn't find any relevant information to answer your query. Could you please rephrase or provide more details?"
            
            # Log query for later analysis
            background_tasks.add_task(log_unanswered_query, query, request.source)
            
            return QueryResponse(
                success=True,
                query=query,
                response=response_text,
                sources=None,
                response_time=time.time() - start_time,
                metadata={"query_info": query_info, "source_counts": source_counts}
            )
        
        # Generate answer from retrieved content
        answer_options = {
            "format": request.format,
            "include_sources": request.include_sources,
            "language": request.language
        }
        
        result = components["answer_generator"].generate_answer(query, all_results, query_info, answer_options)
        
        # Format sources for response
        formatted_sources = SourceController.format_sources(all_results) if request.include_sources else None
        
        # Log successful query
        background_tasks.add_task(log_successful_query, query, query_info, request.source, len(all_results))
        
        return QueryResponse(
            success=True,
            query=query,
            response=result["answer"],
            sources=formatted_sources,
            response_time=time.time() - start_time,
            metadata={
                "query_info": query_info,
                "source_counts": source_counts,
                "confidence": result.get("confidence", 0.0)
            }
        )
        
    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        
        # Log the error
        background_tasks.add_task(log_error, query, str(e), request.source)
        
        return QueryResponse(
            success=False,
            query=query,
            response="I'm sorry, but I encountered an error while processing your request. Please try again later.",
            sources=None,
            response_time=time.time() - start_time,
            metadata={"error": str(e)}
        )

@router.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(feedback: FeedbackRequest, background_tasks: BackgroundTasks):
    """
    Submit user feedback on a response.
    
    Args:
        feedback: Feedback request containing the feedback information
        background_tasks: FastAPI background tasks
        
    Returns:
        Confirmation of feedback receipt
    """
    logger.info(f"Received feedback '{feedback.feedback}' for message {feedback.message_id}")
    
    try:
        # Generate unique feedback ID
        feedback_id = f"fb-{int(time.time())}"
        
        # Store feedback in background task
        background_tasks.add_task(store_feedback, feedback_id, feedback)
        
        return FeedbackResponse(
            success=True,
            feedback_id=feedback_id
        )
        
    except Exception as e:
        logger.error(f"Error storing feedback: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to store feedback")

@router.get("/status", response_model=SystemStatusResponse)
async def get_system_status():
    """
    Get the current status of the system.
    
    Returns:
        System status information
    """
    # Check component status
    component_status = {}
    for key, component in components.items():
        if key == "start_time":
            continue
        component_status[key] = component is not None
    
    # Check if all critical components are functioning
    all_critical_ok = all([
        component_status.get("confluence_connector", False),
        component_status.get("remedy_connector", False),
        component_status.get("answer_generator", False)
    ])
    
    status = "operational" if all_critical_ok else "degraded"
    
    # Calculate uptime
    uptime = time.time() - components["start_time"]
    
    return SystemStatusResponse(
        status=status,
        version=config.get("app", "version", "1.0.0"),
        uptime=uptime,
        components=component_status
    )

# Background tasks

async def store_feedback(feedback_id: str, feedback: FeedbackRequest):
    """
    Store user feedback in the database.
    
    Args:
        feedback_id: Unique ID for the feedback
        feedback: Feedback data
    """
    try:
        # Here you would typically store feedback in a database
        # For now, we'll just log it
        logger.info(f"Storing feedback {feedback_id}: {feedback.dict()}")
        
        # TODO: Implement feedback storage in a database
        
    except Exception as e:
        logger.error(f"Error storing feedback {feedback_id}: {str(e)}")

async def log_successful_query(query: str, query_info: Dict[str, Any], source: str, result_count: int):
    """
    Log a successful query for analytics.
    
    Args:
        query: User query
        query_info: Analyzed query information
        source: Queried source
        result_count: Number of results returned
    """
    try:
        # Here you would typically log to a database or analytics service
        logger.info(f"Successful query: '{query}' with {result_count} results from {source}")
        
        # TODO: Implement analytics logging
        
    except Exception as e:
        logger.error(f"Error logging successful query: {str(e)}")

async def log_unanswered_query(query: str, source: str):
    """
    Log an unanswered query for analysis.
    
    Args:
        query: User query
        source: Queried source
    """
    try:
        # Log unanswered queries for later analysis
        logger.warning(f"Unanswered query: '{query}' from {source}")
        
        # TODO: Implement unanswered query logging
        
    except Exception as e:
        logger.error(f"Error logging unanswered query: {str(e)}")

async def log_error(query: str, error: str, source: str):
    """
    Log a query processing error.
    
    Args:
        query: User query
        error: Error message
        source: Queried source
    """
    try:
        logger.error(f"Query error: '{query}' from {source} - {error}")
        
        # TODO: Implement error logging
        
    except Exception as e:
        logger.error(f"Error logging query error: {str(e)}")

# Initialize components

def init_components():
    """Initialize all system components."""
    try:
        logger.info("Initializing RAG system components...")
        
        # Initialize document processor
        components["document_processor"] = DocumentProcessor(
            cache_dir=config.get("app", "cache_dir")
        )
        
        # Initialize query analyzer
        components["query_analyzer"] = QueryAnalyzer()
        
        # Initialize Confluence connector
        components["confluence_connector"] = ConfluenceConnector(
            base_url=config.get("confluence", "url"),
            username=config.get("confluence", "username"),
            api_token=config.get("confluence", "api_token"),
            space_key=config.get("confluence", "space_key")
        )
        
        # Initialize Remedy connector
        components["remedy_connector"] = RemedyConnector(
            base_url=config.get("remedy", "url"),
            username=config.get("remedy", "username"),
            password=config.get("remedy", "password"),
            server_name=config.get("remedy", "server_name")
        )
        
        # Initialize hybrid search
        components["hybrid_search"] = HybridSearchRetriever()
        
        # Initialize answer generator
        components["answer_generator"] = AnswerGenerator()
        
        logger.info("All components initialized successfully!")
        
    except Exception as e:
        logger.error(f"Error initializing components: {str(e)}")
        raise

# Startup and shutdown events

def startup_event():
    """Initialize components on application startup."""
    try:
        # Initialize components
        init_components()
        
        # Set start time
        components["start_time"] = time.time()
        
        logger.info("RAG system started successfully!")
        
    except Exception as e:
        logger.error(f"Error during startup: {str(e)}")
        raise

def shutdown_event():
    """Clean up resources on application shutdown."""
    try:
        logger.info("Shutting down RAG system...")
        
        # Clean up any connections or resources
        if components["confluence_connector"]:
            logger.info("Closing Confluence connection...")
            # Any cleanup needed
        
        if components["remedy_connector"]:
            logger.info("Closing Remedy connection...")
            # Any cleanup needed
        
        logger.info("RAG system shutdown complete")
        
    except Exception as e:
        logger.error(f"Error during shutdown: {str(e)}")

# Export startup and shutdown events
__all__ = ['router', 'startup_event', 'shutdown_event']









Main Application (app.py)

"""
Main application module for the RAG system.

This is the entry point for the application, which initializes the FastAPI server,
loads configuration, and sets up routes and middleware.
"""
import os
import sys
import logging
from pathlib import Path
from fastapi import FastAPI, Request, Response
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import custom modules
from config.config import config
from web.routes import router, startup_event, shutdown_event
from utils.logger import setup_logger

# Setup logging
logger = setup_logger(__name__)

# Create FastAPI app
app = FastAPI(
    title="Enterprise Knowledge Assistant",
    description="RAG-based system for retrieving and generating answers from corporate knowledge sources",
    version=config.get("app", "version", "1.0.0"),
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory=project_root / "web" / "static"), name="static")

# Setup templates
templates = Jinja2Templates(directory=project_root / "web" / "templates")

# Include API routes
app.include_router(router, prefix="/api")

# Root route
@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Serve the main application page."""
    return templates.TemplateResponse("index.html", {"request": request})

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring."""
    return {"status": "ok"}

# Error handlers
@app.exception_handler(404)
async def not_found_exception_handler(request: Request, exc: Exception):
    """Handle 404 errors."""
    if request.url.path.startswith("/api/"):
        return {"error": "Not found", "path": request.url.path}
    return templates.TemplateResponse(
        "error.html", 
        {"request": request, "status_code": 404, "message": "Page not found"}
    )

@app.exception_handler(500)
async def server_error_exception_handler(request: Request, exc: Exception):
    """Handle 500 errors."""
    logger.error(f"Server error: {str(exc)}")
    if request.url.path.startswith("/api/"):
        return {"error": "Internal server error", "detail": str(exc)}
    return templates.TemplateResponse(
        "error.html", 
        {"request": request, "status_code": 500, "message": "Internal server error"}
    )

# Register startup and shutdown events
@app.on_event("startup")
async def on_startup():
    """Execute startup event."""
    startup_event()

@app.on_event("shutdown")
async def on_shutdown():
    """Execute shutdown event."""
    shutdown_event()

# Run the application
if __name__ == "__main__":
    # Get port from config or environment
    port = int(os.environ.get("PORT", config.get("app", "port", 8000)))
    
    # Get host from config or environment
    host = os.environ.get("HOST", config.get("app", "host", "0.0.0.0"))
    
    # Get debug mode from config or environment
    debug = os.environ.get("DEBUG", config.get("app", "debug", "False")).lower() == "true"
    
    logger.info(f"Starting server on {host}:{port} (debug={debug})")
    
    uvicorn.run(
        "app:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info",
        access_log=True
    )














Advanced Answer Generator with Non-Hallucination Guarantees

"""
Advanced Answer Generator Module

This module provides a sophisticated answer generation system that synthesizes 
information from retrieved documents to create coherent, factual responses
while guaranteeing non-hallucination.
"""
import re
import logging
from typing import Dict, Any, List, Optional, Tuple, Set, Union
import numpy as np
from collections import defaultdict, Counter
import difflib
import json
import hashlib
import time
from datetime import datetime

# Import NLP utilities
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import nltk

# Ensure NLTK resources are available
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

# Setup logging
logger = logging.getLogger(__name__)

class AnswerGenerator:
    """
    Advanced answer generator with factual guarantees.
    
    This class implements a sophisticated answer generation approach that:
    1. Extracts evidence from retrieved documents
    2. Determines answer relevance and factuality
    3. Synthesizes information into a coherent response
    4. Provides source attribution
    5. Guarantees non-hallucination through strict evidence checking
    """
    
    def __init__(self):
        """Initialize the answer generator."""
        self.stop_words = set(stopwords.words('english'))
        
        # Confidence thresholds
        self.high_confidence_threshold = 0.8
        self.medium_confidence_threshold = 0.6
        self.citation_needed_threshold = 0.4
        
        # Templates for different response types
        self.templates = {
            "no_information": [
                "I don't have specific information about that in my knowledge base.",
                "I couldn't find relevant information about that topic.",
                "I don't have enough context to answer that question.",
                "That information doesn't appear to be available in the documents I can access."
            ],
            "uncertainty": [
                "Based on the available information, it appears that {}, but I'm not entirely certain.",
                "From what I can gather, {}, though the information is limited.",
                "The documents suggest that {}, but I'd recommend verifying this information.",
                "According to the available sources, {}, however this might require further confirmation."
            ],
            "citation_needed": [
                "I found some information suggesting that {}, but you should verify this from authoritative sources.",
                "While not completely certain, the documents indicate that {}.",
                "The information I have access to suggests {}, but this may be incomplete.",
                "Based on limited information, {}."
            ]
        }
        
        # Initialize tracking of generated answers for debugging
        self.answer_history = []
        self.max_history = 100  # Maximum number of answers to keep in history
    
    def generate_answer(self, query: str, retrieved_documents: List[Dict], 
                        query_info: Dict[str, Any], options: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Generate an answer for the given query based on retrieved documents.
        
        Args:
            query: User query
            retrieved_documents: List of retrieved document chunks/passages
            query_info: Analysis of the query (intent, entities, etc.)
            options: Generation options (format, sources, etc.)
            
        Returns:
            Dictionary with answer and metadata
        """
        start_time = time.time()
        logger.info(f"Generating answer for query: '{query}'")
        
        # Set default options if none provided
        if options is None:
            options = {
                "format": "concise",
                "include_sources": True,
                "language": "en"
            }
        
        # Extract evidence from documents
        evidence = self._extract_evidence(query, query_info, retrieved_documents)
        
        # If no evidence found, return a "no information" response
        if not evidence["evidence_items"]:
            logger.info("No relevant evidence found for query")
            
            no_info_response = np.random.choice(self.templates["no_information"])
            alternative_suggestion = self._generate_alternative_suggestion(query, query_info)
            
            if alternative_suggestion:
                no_info_response += f" {alternative_suggestion}"
                
            return {
                "answer": no_info_response,
                "confidence": 0.0,
                "sources": [],
                "processing_time": time.time() - start_time
            }
        
        # Determine answer approach based on query and evidence
        approach = self._determine_answer_approach(query, query_info, evidence)
        
        # Generate answer based on approach
        if approach == "direct_answer":
            answer_data = self._generate_direct_answer(query, query_info, evidence, options)
        elif approach == "synthesis":
            answer_data = self._generate_synthesis_answer(query, query_info, evidence, options)
        elif approach == "comparison":
            answer_data = self._generate_comparison_answer(query, query_info, evidence, options)
        elif approach == "explanation":
            answer_data = self._generate_explanation_answer(query, query_info, evidence, options)
        else:
            # Default to synthesis approach
            answer_data = self._generate_synthesis_answer(query, query_info, evidence, options)
        
        # Add answer to history (for debugging)
        self._add_to_answer_history(query, answer_data["answer"], evidence)
        
        # Calculate total processing time
        answer_data["processing_time"] = time.time() - start_time
        
        logger.info(f"Answer generated in {answer_data['processing_time']:.2f} seconds with confidence {answer_data['confidence']:.2f}")
        
        return answer_data
    
    def _extract_evidence(self, query: str, query_info: Dict[str, Any], 
                          documents: List[Dict]) -> Dict[str, Any]:
        """
        Extract relevant evidence from retrieved documents.
        
        Args:
            query: User query
            query_info: Analysis of the query
            documents: Retrieved documents
            
        Returns:
            Dictionary with evidence items and metadata
        """
        logger.debug(f"Extracting evidence from {len(documents)} documents")
        
        # Prepare query keywords for matching
        query_keywords = self._extract_keywords(query)
        
        # Evidence scores and items
        evidence_items = []
        all_sentences = []
        
        # Extract key entities from query info
        query_entities = [entity[0].lower() for entity in query_info.get("entities", [])]
        
        # Process each document
        for document in documents:
            try:
                # Skip documents without content
                if "content" not in document or not document["content"]:
                    continue
                
                # Get document content
                content = document["content"]
                
                # Split content into sentences
                sentences = sent_tokenize(content)
                
                # Score each sentence for relevance
                for sentence_idx, sentence in enumerate(sentences):
                    # Skip very short sentences
                    if len(sentence.split()) < 3:
                        continue
                    
                    # Calculate relevance score
                    relevance_score = self._calculate_relevance_score(sentence, query, query_keywords, query_entities)
                    
                    # Add to evidence if score is high enough
                    if relevance_score > 0.3:  # Threshold for relevance
                        evidence_items.append({
                            "text": sentence,
                            "relevance": relevance_score,
                            "document_id": document.get("ticket_id") or document.get("page_id"),
                            "document_title": document.get("title", "Unknown"),
                            "position": sentence_idx,
                            "source": "remedy" if "ticket_id" in document else "confluence"
                        })
                        
                    # Add to all sentences for context
                    all_sentences.append({
                        "text": sentence,
                        "document_id": document.get("ticket_id") or document.get("page_id"),
                        "position": sentence_idx
                    })
                        
            except Exception as e:
                logger.error(f"Error extracting evidence from document: {str(e)}")
        
        # Sort evidence by relevance
        evidence_items.sort(key=lambda x: x["relevance"], reverse=True)
        
        # Calculate overall confidence based on evidence quality
        confidence = self._calculate_evidence_confidence(evidence_items, query)
        
        # Return structured evidence
        return {
            "evidence_items": evidence_items,
            "all_sentences": all_sentences,
            "confidence": confidence,
            "query_keywords": query_keywords,
            "query_entities": query_entities
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        Extract important keywords from text.
        
        Args:
            text: Input text
            
        Returns:
            List of keywords
        """
        # Tokenize and convert to lowercase
        words = word_tokenize(text.lower())
        
        # Remove stopwords and short words
        keywords = [word for word in words 
                   if word not in self.stop_words 
                   and len(word) > 2
                   and word.isalnum()]
                   
        return keywords
    
    def _calculate_relevance_score(self, sentence: str, query: str, 
                                  query_keywords: List[str],
                                  query_entities: List[str]) -> float:
        """
        Calculate relevance score for a sentence.
        
        Args:
            sentence: Candidate sentence
            query: Original query
            query_keywords: Keywords from the query
            query_entities: Entities from the query
            
        Returns:
            Relevance score (0.0 to 1.0)
        """
        # Convert sentence to lowercase for comparison
        sentence_lower = sentence.lower()
        
        # Count keyword matches
        keyword_matches = sum(1 for keyword in query_keywords if keyword in sentence_lower)
        keyword_score = keyword_matches / max(len(query_keywords), 1)
        
        # Count entity matches (weighted higher)
        entity_matches = sum(1 for entity in query_entities if entity in sentence_lower)
        entity_score = entity_matches / max(len(query_entities), 1) if query_entities else 0
        
        # Calculate semantic similarity
        semantic_score = self._calculate_semantic_similarity(sentence, query)
        
        # Combine scores (weighted average)
        combined_score = (0.4 * keyword_score) + (0.4 * entity_score) + (0.2 * semantic_score)
        
        # Bonus for exact phrase matches
        query_phrases = self._extract_phrases(query)
        for phrase in query_phrases:
            if len(phrase) > 3 and phrase.lower() in sentence_lower:
                combined_score += 0.2
                
        # Cap at 1.0
        return min(combined_score, 1.0)
    
    def _extract_phrases(self, text: str) -> List[str]:
        """
        Extract meaningful phrases from text.
        
        Args:
            text: Input text
            
        Returns:
            List of phrases
        """
        # Simple phrase extraction by splitting on stopwords and punctuation
        # A more sophisticated implementation would use chunking or NP extraction
        words = word_tokenize(text)
        phrases = []
        current_phrase = []
        
        for word in words:
            if word.lower() in self.stop_words or not word.isalnum():
                if current_phrase:
                    phrases.append(" ".join(current_phrase))
                    current_phrase = []
            else:
                current_phrase.append(word)
                
        # Add the last phrase if it exists
        if current_phrase:
            phrases.append(" ".join(current_phrase))
            
        return phrases
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate semantic similarity between two texts.
        
        Args:
            text1: First text
            text2: Second text
            
        Returns:
            Similarity score (0.0 to 1.0)
        """
        # Simple implementation using word overlap
        # In a production system, this would use embeddings or a trained model
        
        words1 = set(self._extract_keywords(text1))
        words2 = set(self._extract_keywords(text2))
        
        if not words1 or not words2:
            return 0.0
            
        # Jaccard similarity
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _calculate_evidence_confidence(self, evidence_items: List[Dict], query: str) -> float:
        """
        Calculate overall confidence in the evidence.
        
        Args:
            evidence_items: List of evidence items
            query: Original query
            
        Returns:
            Confidence score (0.0 to 1.0)
        """
        if not evidence_items:
            return 0.0
            
        # Consider top 3 evidence items for confidence
        top_evidence = evidence_items[:3]
        
        # Average relevance of top evidence
        avg_relevance = sum(item["relevance"] for item in top_evidence) / len(top_evidence)
        
        # Check consistency between evidence items
        consistency_score = self._check_evidence_consistency(top_evidence)
        
        # Check coverage of query elements
        query_keywords = self._extract_keywords(query)
        coverage_score = self._calculate_query_coverage(top_evidence, query_keywords)
        
        # Combine scores (weighted)
        confidence = (0.5 * avg_relevance) + (0.3 * consistency_score) + (0.2 * coverage_score)
        
        return min(confidence, 1.0)
    
    def _check_evidence_consistency(self, evidence_items: List[Dict]) -> float:
        """
        Check consistency between evidence items.
        
        Args:
            evidence_items: List of evidence items
            
        Returns:
            Consistency score (0.0 to 1.0)
        """
        if len(evidence_items) <= 1:
            return 1.0  # Single item is consistent with itself
            
        # Extract sentences
        sentences = [item["text"] for item in evidence_items]
        
        # Calculate pairwise similarities
        similarities = []
        for i in range(len(sentences)):
            for j in range(i + 1, len(sentences)):
                sim = self._calculate_semantic_similarity(sentences[i], sentences[j])
                similarities.append(sim)
                
        # Average similarity as consistency score
        consistency = sum(similarities) / len(similarities) if similarities else 0.5
        
        return consistency
    
    def _calculate_query_coverage(self, evidence_items: List[Dict], 
                                 query_keywords: List[str]) -> float:
        """
        Calculate how well the evidence covers query keywords.
        
        Args:
            evidence_items: List of evidence items
            query_keywords: Keywords from the query
            
        Returns:
            Coverage score (0.0 to 1.0)
        """
        if not query_keywords:
            return 1.0  # No keywords to cover
            
        # Combine all evidence text
        combined_text = " ".join([item["text"].lower() for item in evidence_items])
        
        # Count covered keywords
        covered = sum(1 for keyword in query_keywords if keyword in combined_text)
        
        return covered / len(query_keywords)
    
    def _determine_answer_approach(self, query: str, query_info: Dict[str, Any], 
                                   evidence: Dict[str, Any]) -> str:
        """
        Determine the best approach for generating an answer.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Extracted evidence
            
        Returns:
            Approach name: 'direct_answer', 'synthesis', 'comparison', or 'explanation'
        """
        # Get query intent from query info
        intent = query_info.get("intent", "unknown")
        query_type = query_info.get("query_type", "unknown")
        
        # Check query patterns for direct answer
        if (query_type == "question" and 
            any(query.lower().startswith(w) for w in ["what is", "who is", "when is", "where is"])):
            return "direct_answer"
            
        # Check for comparison queries
        if (intent == "comparison" or 
            any(w in query.lower() for w in ["compare", "difference", "versus", "vs", "similar"])):
            return "comparison"
            
        # Check for explanation queries
        if (intent == "explanation" or query_type == "explanation" or
            any(w in query.lower() for w in ["explain", "why", "how does", "how do"])):
            return "explanation"
            
        # Default to synthesis for most queries
        return "synthesis"
    
    def _generate_direct_answer(self, query: str, query_info: Dict[str, Any], 
                              evidence: Dict[str, Any], options: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a direct answer for factual queries.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Extracted evidence
            options: Generation options
            
        Returns:
            Answer data dictionary
        """
        logger.debug("Generating direct answer")
        
        # Get the most relevant evidence
        top_evidence = evidence["evidence_items"][:3]
        
        if not top_evidence:
            return self._generate_no_answer_response()
            
        # For direct answers, we'll use the most relevant evidence sentence
        best_evidence = top_evidence[0]
        
        # Check if the evidence has high confidence
        if best_evidence["relevance"] > self.high_confidence_threshold:
            # Use the evidence directly
            answer_text = best_evidence["text"]
            
            # For some question types, refine the answer
            question_type = query_info.get("question_type")
            
            if question_type in ["what", "definition"]:
                # Format as a definition
                if any(e[0].lower() in best_evidence["text"].lower() for e in query_info.get("entities", [])):
                    # Extract subject from query
                    subject = next((e[0] for e in query_info.get("entities", [])), "")
                    if subject:
                        answer_text = f"{subject} {answer_text[0].lower() + answer_text[1:]}"
        else:
            # Lower confidence, use a template with the evidence
            template = np.random.choice(self.templates["uncertainty"])
            answer_text = template.format(best_evidence["text"])
            
        # Format based on concise or detailed option
        if options.get("format") == "detailed":
            # For detailed format, add context from other evidence
            additional_context = ""
            
            if len(top_evidence) > 1:
                additional_context = " " + top_evidence[1]["text"]
                
            answer_text = answer_text + additional_context
            
        # Add sources
        sources = [item["document_id"] for item in top_evidence]
        
        return {
            "answer": answer_text,
            "confidence": evidence["confidence"],
            "sources": sources
        }
    
    def _generate_synthesis_answer(self, query: str, query_info: Dict[str, Any], 
                                 evidence: Dict[str, Any], options: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a synthesized answer combining multiple evidence items.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Extracted evidence
            options: Generation options
            
        Returns:
            Answer data dictionary
        """
        logger.debug("Generating synthesis answer")
        
        # Get top evidence items
        top_evidence = evidence["evidence_items"][:5]
        
        if not top_evidence:
            return self._generate_no_answer_response()
            
        # Group evidence by source document
        evidence_by_doc = defaultdict(list)
        
        for item in top_evidence:
            doc_id = item["document_id"]
            evidence_by_doc[doc_id].append(item)
            
        # Synthesize information
        paragraphs = []
        
        # Handle the intro paragraph
        intro = self._generate_intro_paragraph(query, query_info, top_evidence)
        paragraphs.append(intro)
        
        # Organize evidence into coherent paragraphs
        if options.get("format") == "detailed":
            # For detailed format, include more information
            theme_paragraphs = self._organize_by_themes(top_evidence, query_keywords=evidence["query_keywords"])
            paragraphs.extend(theme_paragraphs)
        else:
            # For concise format, just add key points
            key_points = self._extract_key_points(top_evidence, query_info)
            
            if key_points:
                points_paragraph = " ".join(key_points)
                paragraphs.append(points_paragraph)
                
        # Add conclusion paragraph for detailed format
        if options.get("format") == "detailed" and len(paragraphs) > 1:
            conclusion = self._generate_conclusion_paragraph(query, query_info, top_evidence)
            paragraphs.append(conclusion)
            
        # Combine paragraphs into final answer
        answer_text = "\n\n".join(paragraphs)
        
        # Add sources
        sources = list(evidence_by_doc.keys())
        
        return {
            "answer": answer_text,
            "confidence": evidence["confidence"],
            "sources": sources
        }
    
    def _generate_intro_paragraph(self, query: str, query_info: Dict[str, Any], 
                                 evidence: List[Dict]) -> str:
        """
        Generate an introductory paragraph.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Evidence items
            
        Returns:
            Introductory paragraph
        """
        # Get entities from query
        entities = [entity[0] for entity in query_info.get("entities", [])]
        
        # Get the most relevant evidence
        best_evidence = evidence[0]["text"] if evidence else ""
        
        # Format intro based on query type
        question_type = query_info.get("question_type")
        
        if question_type in ["what", "definition"] and entities:
            # For definition questions
            intro = f"{entities[0]} {best_evidence[0].lower() + best_evidence[1:]}"
        elif question_type in ["how", "process"]:
            # For process questions
            intro = f"Here's how to {query.replace('how to ', '').replace('?', '')}: {best_evidence}"
        elif question_type in ["why", "reason"]:
            # For reason questions
            intro = f"The reason is that {best_evidence[0].lower() + best_evidence[1:]}"
        else:
            # Default intro
            intro = best_evidence
            
        return intro
    
    def _extract_key_points(self, evidence: List[Dict], query_info: Dict[str, Any]) -> List[str]:
        """
        Extract key points from evidence.
        
        Args:
            evidence: Evidence items
            query_info: Analysis of the query
            
        Returns:
            List of key points
        """
        # Start with all evidence texts
        all_points = [item["text"] for item in evidence]
        
        # Deduplicate and remove near-duplicates
        key_points = []
        
        for point in all_points:
            # Check if this point is too similar to existing points
            if not any(self._is_similar(point, existing) for existing in key_points):
                key_points.append(point)
                
            # Limit to 3 key points
            if len(key_points) >= 3:
                break
                
        return key_points
    
    def _is_similar(self, text1: str, text2: str, threshold: float = 0.8) -> bool:
        """
        Check if two texts are similar.
        
        Args:
            text1: First text
            text2: Second text
            threshold: Similarity threshold
            
        Returns:
            True if texts are similar
        """
        # Use difflib's SequenceMatcher for similarity check
        similarity = difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        return similarity > threshold
    
    def _organize_by_themes(self, evidence: List[Dict], 
                           query_keywords: List[str]) -> List[str]:
        """
        Organize evidence into thematic paragraphs.
        
        Args:
            evidence: Evidence items
            query_keywords: Keywords from the query
            
        Returns:
            List of thematic paragraphs
        """
        # Group evidence by themes
        themes = defaultdict(list)
        
        # Simple theme extraction based on keywords
        for item in evidence:
            text = item["text"]
            
            # Find the most prominent keyword in this evidence
            counts = Counter()
            for keyword in query_keywords:
                count = text.lower().count(keyword.lower())
                counts[keyword] += count
                
            # Assign to the most common theme
            if counts:
                top_theme = counts.most_common(1)[0][0]
                themes[top_theme].append(text)
            else:
                # No keyword match, put in "other" category
                themes["other"].append(text)
                
        # Convert theme groups to paragraphs
        paragraphs = []
        
        for theme, texts in themes.items():
            if theme != "other" and len(texts) > 0:
                theme_paragraph = " ".join(texts)
                paragraphs.append(theme_paragraph)
                
        # Add "other" theme last if it exists
        if "other" in themes and themes["other"]:
            other_paragraph = " ".join(themes["other"])
            paragraphs.append(other_paragraph)
            
        return paragraphs
    
    def _generate_conclusion_paragraph(self, query: str, query_info: Dict[str, Any], 
                                      evidence: List[Dict]) -> str:
        """
        Generate a conclusion paragraph.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Evidence items
            
        Returns:
            Conclusion paragraph
        """
        # Simple conclusion based on query intent
        intent = query_info.get("intent", "unknown")
        
        if intent == "explanation":
            return "This explanation provides the key information based on the available documentation."
        elif intent == "comparison":
            return "These are the main similarities and differences based on the available information."
        elif intent == "factual":
            return "These facts represent the most relevant information found in the knowledge base."
        else:
            return "This information is based on the available documentation in the knowledge base."
    
    def _generate_comparison_answer(self, query: str, query_info: Dict[str, Any], 
                                   evidence: Dict[str, Any], options: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a comparison answer for comparing entities.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Extracted evidence
            options: Generation options
            
        Returns:
            Answer data dictionary
        """
        logger.debug("Generating comparison answer")
        
        # Get top evidence items
        top_evidence = evidence["evidence_items"][:8]  # Get more evidence for comparison
        
        if not top_evidence:
            return self._generate_no_answer_response()
            
        # Extract entities to compare
        entities = [entity[0] for entity in query_info.get("entities", [])]
        
        if len(entities) < 2:
            # Try to extract comparison entities from the query
            comparison_terms = re.findall(r'(?:compare|between|versus|vs|or)\s+([a-zA-Z0-9_ ]+)\s+(?:and|vs|versus|or)\s+([a-zA-Z0-9_ ]+)', query, re.IGNORECASE)
            
            if comparison_terms:
                entities = list(comparison_terms[0])
            else:
                # Not enough entities for comparison, fall back to synthesis
                return self._generate_synthesis_answer(query, query_info, evidence, options)
                
        # Group evidence by entity
        evidence_by_entity = defaultdict(list)
        
        for item in top_evidence:
            # Assign to entity based on mention
            assigned = False
            for entity in entities:
                if entity.lower() in item["text"].lower():
                    evidence_by_entity[entity].append(item)
                    assigned = True
                    break
                    
            if not assigned:
                # If not clearly about one entity, add to all
                for entity in entities:
                    evidence_by_entity[entity].append(item)
                    
        # Start building the comparison
        paragraphs = []
        
        # Introduction
        intro = f"Here's a comparison between {' and '.join(entities)}:"
        paragraphs.append(intro)
        
        # Format based on concise or detailed option
        if options.get("format") == "detailed":
            # For each entity, create a section
            for entity in entities:
                entity_evidence = evidence_by_entity[entity]
                
                if entity_evidence:
                    entity_intro = f"\n\n{entity}:"
                    entity_details = " ".join([item["text"] for item in entity_evidence[:3]])
                    paragraphs.append(f"{entity_intro} {entity_details}")
                
            # Add a similarities and differences section if possible
            if len(entities) == 2 and all(evidence_by_entity[entity] for entity in entities):
                similarities, differences = self._extract_similarities_differences(
                    evidence_by_entity[entities[0]], 
                    evidence_by_entity[entities[1]]
                )
                
                if similarities:
                    paragraphs.append(f"\n\nSimilarities: {' '.join(similarities)}")
                    
                if differences:
                    paragraphs.append(f"\n\nDifferences: {' '.join(differences)}")
        else:
            # Concise format
            for entity in entities:
                entity_evidence = evidence_by_entity[entity]
                
                if entity_evidence:
                    entity_summary = f"\n\n{entity}: {entity_evidence[0]['text']}"
                    paragraphs.append(entity_summary)
        
        # Combine paragraphs
        answer_text = "".join(paragraphs)
        
        # Add sources
        sources = [item["document_id"] for item in top_evidence]
        
        return {
            "answer": answer_text,
            "confidence": evidence["confidence"],
            "sources": sources
        }
    
    def _extract_similarities_differences(self, evidence1: List[Dict], 
                                          evidence2: List[Dict]) -> Tuple[List[str], List[str]]:
        """
        Extract similarities and differences from evidence.
        
        Args:
            evidence1: Evidence for first entity
            evidence2: Evidence for second entity
            
        Returns:
            Tuple of (similarities, differences)
        """
        # Extract text content
        texts1 = [item["text"] for item in evidence1]
        texts2 = [item["text"] for item in evidence2]
        
        # Extract keywords
        keywords1 = set()
        for text in texts1:
            keywords1.update(self._extract_keywords(text))
            
        keywords2 = set()
        for text in texts2:
            keywords2.update(self._extract_keywords(text))
            
        # Find similarities and differences
        similarities = keywords1.intersection(keywords2)
        differences = keywords1.symmetric_difference(keywords2)
        
        # Convert to sentences
        similarity_sentences = []
        for sim in list(similarities)[:3]:  # Limit to top 3
            for text in texts1 + texts2:
                if sim in text.lower():
                    similarity_sentences.append(text)
                    break
                    
        difference_sentences = []
        for diff in list(differences)[:3]:  # Limit to top 3
            if diff in keywords1:
                for text in texts1:
                    if diff in text.lower():
                        difference_sentences.append(text)
                        break
            else:
                for text in texts2:
                    if diff in text.lower():
                        difference_sentences.append(text)
                        break
                        
        return similarity_sentences, difference_sentences
    
    def _generate_explanation_answer(self, query: str, query_info: Dict[str, Any], 
                                    evidence: Dict[str, Any], options: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate an explanation answer.
        
        Args:
            query: User query
            query_info: Analysis of the query
            evidence: Extracted evidence
            options: Generation options
            
        Returns:
            Answer data dictionary
        """
        logger.debug("Generating explanation answer")
        
        # Get top evidence items
        top_evidence = evidence["evidence_items"][:5]
        
        if not top_evidence:
            return self._generate_no_answer_response()
            
        # For explanations, we want to organize by causal relationships
        cause_evidence = []
        effect_evidence = []
        process_evidence = []
        general_evidence = []
        
        # Classify evidence
        for item in top_evidence:
            text = item["text"].lower()
            
            # Look for causal indicators
            if any(term in text for term in ['because', 'cause', 'reason', 'due to', 'leads to']):
                cause_evidence.append(item)
            elif any(term in text for term in ['result', 'effect', 'impact', 'outcome', 'consequently']):
                effect_evidence.append(item)
            elif any(term in text for term in ['step', 'process', 'procedure', 'method', 'how to']):
                process_evidence.append(item)
            else:
                general_evidence.append(item)
                
        # Build the explanation
        paragraphs = []
        
        # Start with introduction
        intro_prefix = "Here's an explanation of "
        intro_subject = next((entity[0] for entity in query_info.get("entities", [])), "this topic")
        intro = f"{intro_prefix}{intro_subject}:"
        paragraphs.append(intro)
        
        # Add process steps if available
        if process_evidence:
            process_texts = [f"{item['text']}" for item in process_evidence]
            process_paragraph = " ".join(process_texts)
            paragraphs.append(process_paragraph)
            
        # Add causes if available
        if cause_evidence:
            cause_texts = [item["text"] for item in cause_evidence]
            cause_paragraph = "The main reason is " + cause_texts[0].lower() if cause_texts else ""
            if len(cause_texts) > 1:
                cause_paragraph += " Additionally, " + " ".join(cause_texts[1:])
            paragraphs.append(cause_paragraph)
            
        # Add effects if available
        if effect_evidence:
            effect_texts = [item["text"] for item in effect_evidence]
            effect_paragraph = "This results in " + effect_texts[0].lower() if effect_texts else ""
            if len(effect_texts) > 1:
                effect_paragraph += " Furthermore, " + " ".join(effect_texts[1:])
            paragraphs.append(effect_paragraph)
            
        # Add general information
        if general_evidence:
            general_texts = [item["text"] for item in general_evidence]
            general_paragraph = " ".join(general_texts)
            paragraphs.append(general_paragraph)
            
        # Combine into final answer
        answer_text = "\n\n".join(paragraphs)
        
        # For concise format, limit length
        if options.get("format") == "concise":
            sentences = sent_tokenize(answer_text)
            if len(sentences) > 5:
                answer_text = " ".join(sentences[:5])
                
        # Add sources
        sources = [item["document_id"] for item in top_evidence]
        
        return {
            "answer": answer_text,
            "confidence": evidence["confidence"],
            "sources": sources
        }
    
    def _generate_no_answer_response(self) -> Dict[str, Any]:
        """
        Generate a response for when no relevant information is found.
        
        Returns:
            Answer data dictionary
        """
        no_info_response = np.random.choice(self.templates["no_information"])
        
        return {
            "answer": no_info_response,
            "confidence": 0.0,
            "sources": []
        }
    
    def _generate_alternative_suggestion(self, query: str, query_info: Dict[str, Any]) -> str:
        """
        Generate an alternative query suggestion.
        
        Args:
            query: Original query
            query_info: Analysis of the query
            
        Returns:
            Alternative suggestion or empty string
        """
        # Extract entities and intents from query
        entities = [entity[0] for entity in query_info.get("entities", [])]
        keywords = query_info.get("keywords", [])
        
        # Generate alternative based on what we have
        if entities:
            return f"You might try asking about {entities[0]} in a different way, or check if this entity exists in the knowledge base."
        elif keywords:
            most_important = keywords[0] if keywords else ""
            return f"You could try a different query related to {most_important}, or be more specific about what you're looking for."
        else:
            return "You might try rephrasing your query to be more specific."
    
    def _add_to_answer_history(self, query: str, answer: str, evidence: Dict[str, Any]) -> None:
        """
        Add an answer to the history for debugging.
        
        Args:
            query: Original query
            answer: Generated answer
            evidence: Evidence used
        """
        # Create history item
        history_item = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "answer": answer,
            "evidence_count": len(evidence["evidence_items"]),
            "confidence": evidence["confidence"]
        }
        
        # Add to history
        self.answer_history.append(history_item)
        
        # Trim history if needed
        if len(self.answer_history) > self.max_history:
            self.answer_history = self.answer_history[-self.max_history:]

    def validate_against_evidence(self, answer: str, evidence: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate an answer against evidence to prevent hallucination.
        
        Args:
            answer: Generated answer
            evidence: Evidence dictionary
            
        Returns:
            Validation results
        """
        # Split answer into sentences
        answer_sentences = sent_tokenize(answer)
        
        # Track validation results
        validation = {
            "supported": [],
            "unsupported": [],
            "modified": [],
            "overall_support": 0.0
        }
        
        # Check each sentence against evidence
        for sentence in answer_sentences:
            supported, score, supporting_evidence = self._check_sentence_support(
                sentence, evidence["evidence_items"]
            )
            
            if supported:
                validation["supported"].append({
                    "sentence": sentence,
                    "score": score,
                    "evidence": supporting_evidence
                })
            else:
                # For unsupported sentences, recommend modification
                validation["unsupported"].append({
                    "sentence": sentence,
                    "score": score
                })
                
                # Generate a modified version based on evidence
                if supporting_evidence:
                    # Use closest evidence instead
                    modified = supporting_evidence[0]["text"]
                    validation["modified"].append({
                        "original": sentence,
                        "modified": modified
                    })
                else:
                    # Remove entirely
                    validation["modified"].append({
                        "original": sentence,
                        "modified": None
                    })
                    
        # Calculate overall support score
        if answer_sentences:
            validation["overall_support"] = len(validation["supported"]) / len(answer_sentences)
            
        return validation
    
    def _check_sentence_support(self, sentence: str, evidence: List[Dict]) -> Tuple[bool, float, List[Dict]]:
        """
        Check if a sentence is supported by evidence.
        
        Args:
            sentence: Sentence to validate
            evidence: List of evidence items
            
        Returns:
            Tuple of (is_supported, support_score, supporting_evidence)
        """
        # Extract keywords from sentence
        keywords = self._extract_keywords(sentence)
        
        if not keywords:
            return False, 0.0, []
            
        # Track evidence scores and items
        evidence_scores = []
        
        for item in evidence:
            # Calculate similarity
            similarity = self._calculate_semantic_similarity(sentence, item["text"])
            evidence_scores.append((item, similarity))
            
        # Sort by similarity
        evidence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Check if any evidence sufficiently supports the sentence
        if evidence_scores and evidence_scores[0][1] >= self.citation_needed_threshold:
            # Get supporting evidence
            supporting_evidence = [item for item, score in evidence_scores if score >= self.citation_needed_threshold]
            
            # Return supported with the score and evidence
            return True, evidence_scores[0][1], supporting_evidence
        else:
            # Return unsupported, but include closest evidence for modification
            closest_evidence = [item for item, _ in evidence_scores[:3]] if evidence_scores else []
            return False, evidence_scores[0][1] if evidence_scores else 0.0, closest_evidence
















Default Application Settings (config/settings.py)

"""
Default application settings for the RAG system.
These settings can be overridden by environment variables or configuration files.
"""

# Application settings
APP_NAME = "Advanced Enterprise RAG System"
APP_VERSION = "1.0.0"
DEBUG = False
HOST = "0.0.0.0"
PORT = 8000
LOG_LEVEL = "INFO"
CACHE_DIR = "cache"
TEMP_DIR = "tmp"
MAX_UPLOAD_SIZE = 100 * 1024 * 1024  # 100 MB

# Document processing settings
CHUNK_SIZE = 300
CHUNK_OVERLAP = 50
SUPPORTED_DOCUMENT_TYPES = [".pdf", ".docx", ".txt", ".csv", ".xlsx", ".json", ".md", ".html", ".xml"]
SUPPORTED_IMAGE_TYPES = [".jpg", ".jpeg", ".png", ".gif", ".bmp"]
USE_OCR = True
USE_ADVANCED_OCR = True
MAX_DOCUMENT_SIZE = 50 * 1024 * 1024  # 50 MB
TABLE_EXTRACTION_ENABLED = True
IMAGE_EXTRACTION_ENABLED = True
FORMULA_EXTRACTION_ENABLED = True
CODE_EXTRACTION_ENABLED = True

# Retrieval settings
TOP_K_RETRIEVAL = 10
HYBRID_SEARCH_WEIGHTS = {
    "bm25": 0.3,
    "vector": 0.7
}
KNOWLEDGE_GRAPH_ENABLED = True
RANKING_THRESHOLD = 0.7
MINIMUM_RELEVANCE_SCORE = 0.5
RERANKING_ENABLED = True
CROSS_ENCODER_RERANKING = False  # Requires additional models

# NLP settings
EMBEDDINGS_MODEL = "all-MiniLM-L6-v2"  # Sentence-transformers model
EMBEDDINGS_DIMENSION = 384
EMBEDDING_BATCH_SIZE = 32
USE_SENTENCE_TRANSFORMERS = True
FALLBACK_TO_TFIDF = True
NER_ENABLED = True
NER_MODEL = "en_core_web_md"  # spaCy model name
SUMMARIZATION_MAX_LENGTH = 500

# Source settings (for Confluence and Remedy integration)
SOURCE_CONTROLLER_ENABLED = True
DEFAULT_SOURCES = ["confluence", "remedy"]  # Default sources to search

# Confluence settings
CONFLUENCE_URL = ""
CONFLUENCE_USERNAME = ""
CONFLUENCE_PASSWORD = ""
CONFLUENCE_API_TOKEN = ""
CONFLUENCE_SPACE_KEY = ""
CONFLUENCE_MAX_RESULTS = 100
CONFLUENCE_INCLUDE_ATTACHMENTS = True
CONFLUENCE_CACHE_TTL = 3600  # 1 hour

# Remedy settings
REMEDY_URL = ""
REMEDY_USERNAME = ""
REMEDY_PASSWORD = ""
REMEDY_SERVER_NAME = ""
REMEDY_MAX_RESULTS = 50
REMEDY_CACHE_TTL = 3600  # 1 hour

# Web interface settings
WEB_STATIC_PATH = "web/static"
WEB_TEMPLATE_PATH = "web/templates"
WEB_TITLE = "Enterprise Knowledge Assistant"
WEB_DESCRIPTION = "Intelligent RAG system for enterprise knowledge"
WEB_FAVICON = "web/static/images/favicon.ico"
MAX_CONVERSATION_HISTORY = 10

# Security settings
ENABLE_AUTHENTICATION = False
JWT_SECRET_KEY = ""  # Should be set in environment variables
JWT_EXPIRY = 86400  # 24 hours
ENCRYPTION_KEY = ""  # Should be set in environment variables
API_RATE_LIMIT = 100  # Requests per minute











Config Package Initialization (config/__init__.py)

"""
Configuration package for the RAG system.
Initializes the global configuration object.
"""
from config.config import Config, config

__all__ = ['Config', 'config']












Utilities Package Initialization (utils/__init__.py)

"""
Utilities package for the RAG system.
Contains common utility functions and classes.
"""
from utils.logger import setup_logger, get_logger
from utils.helpers import (
    html_to_text, 
    extract_table_from_html, 
    sanitize_text,
    detect_language,
    calculate_similarity,
    extract_date_from_string,
    format_date
)
from utils.cache import Cache, DiskCache, MemoryCache
from utils.metrics import (
    calculate_precision, 
    calculate_recall, 
    calculate_f1,
    calculate_latency, 
    measure_memory_usage,
    timeit
)

__all__ = [
    'setup_logger', 
    'get_logger',
    'html_to_text',
    'extract_table_from_html',
    'sanitize_text',
    'detect_language',
    'calculate_similarity',
    'extract_date_from_string',
    'format_date',
    'Cache',
    'DiskCache',
    'MemoryCache',
    'calculate_precision',
    'calculate_recall',
    'calculate_f1',
    'calculate_latency',
    'measure_memory_usage',
    'timeit'
]













Logging Utilities (utils/logger.py)

"""
Logging utilities for the RAG system.
Configures logging with customizable formats and outputs.
"""
import logging
import os
import sys
import time
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
from pathlib import Path
from typing import Dict, Optional, Union, List

# Default log format
DEFAULT_LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
DEFAULT_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

# ANSI color codes for colored terminal output
COLORS = {
    'RESET': '\033[0m',
    'BLACK': '\033[30m',
    'RED': '\033[31m',
    'GREEN': '\033[32m',
    'YELLOW': '\033[33m',
    'BLUE': '\033[34m',
    'MAGENTA': '\033[35m',
    'CYAN': '\033[36m',
    'WHITE': '\033[37m',
    'BOLD': '\033[1m',
    'UNDERLINE': '\033[4m'
}

# Map log levels to colors
LEVEL_COLORS = {
    logging.DEBUG: COLORS['BLUE'],
    logging.INFO: COLORS['GREEN'],
    logging.WARNING: COLORS['YELLOW'],
    logging.ERROR: COLORS['RED'],
    logging.CRITICAL: COLORS['BOLD'] + COLORS['RED']
}

class ColoredFormatter(logging.Formatter):
    """Formatter that adds colors to log messages based on level."""
    
    def format(self, record):
        """Format the log record with colors."""
        # Get original formatting
        log_message = super().format(record)
        
        # Add color based on level
        if record.levelno in LEVEL_COLORS:
            color = LEVEL_COLORS[record.levelno]
            return f"{color}{log_message}{COLORS['RESET']}"
        
        return log_message

class ContextFilter(logging.Filter):
    """Adds contextual information to log records."""
    
    def __init__(self, context: Optional[Dict] = None):
        """Initialize with optional context dictionary."""
        super().__init__()
        self.context = context or {}
    
    def filter(self, record):
        """Add context values to the record."""
        for key, value in self.context.items():
            setattr(record, key, value)
        return True

class RequestIdFilter(logging.Filter):
    """Adds request_id to log records."""
    
    def __init__(self):
        """Initialize the filter."""
        super().__init__()
        self._request_id = None
    
    @property
    def request_id(self):
        """Get current request ID."""
        return self._request_id
    
    @request_id.setter
    def request_id(self, value):
        """Set current request ID."""
        self._request_id = value
    
    def filter(self, record):
        """Add request_id to the record."""
        record.request_id = self._request_id or '-'
        return True

class TimingFilter(logging.Filter):
    """Adds timing information to log records."""
    
    def __init__(self):
        """Initialize the filter."""
        super().__init__()
        self.start_time = time.time()
    
    def filter(self, record):
        """Add timing information to the record."""
        elapsed = time.time() - self.start_time
        record.elapsed = f"{elapsed:.6f}s"
        return True

def setup_logger(
    name: str = None,
    level: Union[int, str] = logging.INFO,
    log_file: Optional[str] = None,
    console: bool = True,
    log_format: str = DEFAULT_LOG_FORMAT,
    date_format: str = DEFAULT_DATE_FORMAT,
    max_bytes: int = 10 * 1024 * 1024,  # 10 MB
    backup_count: int = 5,
    rotating_when: str = 'midnight',
    use_colors: bool = True,
    context: Optional[Dict] = None,
    propagate: bool = False
) -> logging.Logger:
    """
    Set up a logger with customizable configuration.
    
    Args:
        name: Logger name (uses root logger if None)
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Path to log file (No file logging if None)
        console: Whether to log to console
        log_format: Log message format
        date_format: Date format for log messages
        max_bytes: Maximum size in bytes for rotating file handlers
        backup_count: Number of backup files to keep
        rotating_when: When to rotate logs ('midnight', 'h', 'd', etc.)
        use_colors: Whether to use colored output in console
        context: Additional context to add to log records
        propagate: Whether to propagate to parent loggers
        
    Returns:
        Configured logger instance
    """
    # Get logger
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.propagate = propagate
    
    # Clear existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Create formatters
    basic_formatter = logging.Formatter(log_format, date_format)
    
    if use_colors and console:
        console_formatter = ColoredFormatter(log_format, date_format)
    else:
        console_formatter = basic_formatter
    
    # Add console handler if requested
    if console:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(console_formatter)
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    
    # Add file handler if requested
    if log_file:
        # Create directory if it doesn't exist
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        
        # Use rotating file handler
        if rotating_when:
            # Time-based rotation
            file_handler = TimedRotatingFileHandler(
                log_file,
                when=rotating_when,
                backupCount=backup_count
            )
        else:
            # Size-based rotation
            file_handler = RotatingFileHandler(
                log_file,
                maxBytes=max_bytes,
                backupCount=backup_count
            )
        
        file_handler.setFormatter(basic_formatter)
        file_handler.setLevel(level)
        logger.addHandler(file_handler)
    
    # Add context filter if provided
    if context:
        context_filter = ContextFilter(context)
        logger.addFilter(context_filter)
    
    return logger

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger instance by name.
    
    Args:
        name: Logger name
        
    Returns:
        Logger instance
    """
    return logging.getLogger(name)

# Initialize a global request ID filter
request_id_filter = RequestIdFilter()

def set_request_id(request_id: str) -> None:
    """
    Set the current request ID for logging.
    
    Args:
        request_id: Request ID to set
    """
    request_id_filter.request_id = request_id

def reset_request_id() -> None:
    """Reset the current request ID."""
    request_id_filter.request_id = None

def get_request_id() -> Optional[str]:
    """Get the current request ID."""
    return request_id_filter.request_id

# Create a default root logger
root_logger = setup_logger(
    name=None,
    level=logging.INFO,
    console=True,
    use_colors=True
)











Helper Functions (utils/helpers.py)


"""
Helper functions for the RAG system.
Provides common utility functions for text processing, HTML parsing, etc.
"""
import re
import os
import hashlib
import unicodedata
import datetime
import json
import uuid
import string
from typing import Dict, List, Optional, Tuple, Any, Union
from pathlib import Path
import pandas as pd

from bs4 import BeautifulSoup

# Try to import language detection
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0  # For consistent results
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

# Try to import sentence transformers for similarity
try:
    from sentence_transformers import SentenceTransformer, util
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Try to import scikit-learn for fallback similarity calculation
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

def html_to_text(html_content: str) -> str:
    """
    Convert HTML content to plain text.
    
    Args:
        html_content: HTML content to convert
        
    Returns:
        Plain text extracted from HTML
    """
    if not html_content:
        return ""
        
    try:
        # Parse HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for element in soup(["script", "style", "head", "title", "meta", "[document]"]):
            element.extract()
            
        # Get text and normalize whitespace
        text = soup.get_text(separator=' ')
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        # If parsing fails, try simpler methods
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', ' ', html_content)
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

def extract_table_from_html(html_table: str) -> Tuple[List[str], List[List[str]]]:
    """
    Extract a table structure from HTML.
    
    Args:
        html_table: HTML table content
        
    Returns:
        Tuple of (headers, rows)
    """
    try:
        soup = BeautifulSoup(html_table, 'html.parser')
        table = soup.find('table')
        
        if not table:
            return [], []
            
        # Extract headers
        headers = []
        header_row = table.find('thead')
        if header_row:
            headers = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]
        else:
            # Try to get headers from first row
            first_row = table.find('tr')
            if first_row:
                headers = [th.get_text().strip() for th in first_row.find_all(['th', 'td'])]
        
        # Extract rows
        rows = []
        for tr in table.find_all('tr')[1:] if headers else table.find_all('tr'):
            row = [td.get_text().strip() for td in tr.find_all(['td', 'th'])]
            if row:  # Skip empty rows
                rows.append(row)
        
        return headers, rows
    except Exception as e:
        return [], []

def html_table_to_dataframe(html_table: str) -> Optional[pd.DataFrame]:
    """
    Convert HTML table to pandas DataFrame.
    
    Args:
        html_table: HTML table content
        
    Returns:
        DataFrame or None if conversion fails
    """
    try:
        headers, rows = extract_table_from_html(html_table)
        if not rows:
            return None
            
        # Create DataFrame
        if headers:
            df = pd.DataFrame(rows, columns=headers)
        else:
            df = pd.DataFrame(rows)
            
        return df
    except Exception as e:
        return None

def extract_html_tables(html_content: str) -> List[pd.DataFrame]:
    """
    Extract all tables from HTML content as DataFrames.
    
    Args:
        html_content: HTML content
        
    Returns:
        List of DataFrames for each table
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = []
        
        for table_tag in soup.find_all('table'):
            df = html_table_to_dataframe(str(table_tag))
            if df is not None and not df.empty:
                tables.append(df)
                
        return tables
    except Exception as e:
        return []

def extract_html_images(html_content: str) -> List[Dict[str, str]]:
    """
    Extract all images from HTML content.
    
    Args:
        html_content: HTML content
        
    Returns:
        List of dictionaries with image information
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        images = []
        
        for img_tag in soup.find_all('img'):
            img = {
                'src': img_tag.get('src', ''),
                'alt': img_tag.get('alt', ''),
                'title': img_tag.get('title', '')
            }
            
            # Try to get caption from figure element
            figure = img_tag.find_parent('figure')
            if figure:
                figcaption = figure.find('figcaption')
                if figcaption:
                    img['caption'] = figcaption.get_text().strip()
                    
            if img['src']:  # Only add if src is present
                images.append(img)
                
        return images
    except Exception as e:
        return []

def extract_html_headings(html_content: str) -> List[Dict[str, Any]]:
    """
    Extract all headings from HTML content with their hierarchy.
    
    Args:
        html_content: HTML content
        
    Returns:
        List of dictionaries with heading information
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        headings = []
        
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            heading = {
                'level': int(tag.name[1]),
                'text': tag.get_text().strip(),
                'id': tag.get('id', '')
            }
            headings.append(heading)
                
        return headings
    except Exception as e:
        return []

def sanitize_text(text: str) -> str:
    """
    Sanitize text by removing excessive whitespace, control characters, etc.
    
    Args:
        text: Text to sanitize
        
    Returns:
        Sanitized text
    """
    if not text:
        return ""
        
    # Normalize unicode
    text = unicodedata.normalize('NFKC', text)
    
    # Replace control characters with space
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', text)
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def detect_language(text: str) -> str:
    """
    Detect the language of text.
    
    Args:
        text: Text to analyze
        
    Returns:
        ISO language code (e.g., 'en', 'fr', 'de')
    """
    if not text or len(text.strip()) < 10:
        return "en"  # Default to English for very short texts
        
    if LANGDETECT_AVAILABLE:
        try:
            return detect(text)
        except:
            return "en"  # Default to English if detection fails
    else:
        # Simple fallback using common words in different languages
        common_words = {
            "en": ["the", "and", "is", "in", "to", "of", "a"],
            "es": ["el", "la", "es", "en", "de", "y", "que"],
            "fr": ["le", "la", "est", "en", "de", "et", "un"],
            "de": ["der", "die", "das", "ist", "und", "in", "von"],
            "it": ["il", "la", "", "e", "in", "di", "un"],
            "pt": ["o", "a", "", "e", "em", "de", "que"],
            "nl": ["de", "het", "is", "en", "in", "van", "een"],
            "ru": ["", "", "", "", "", "", ""],
            "ja": ["", "", "", "", "", "", ""],
            "zh": ["", "", "", "", "", "", ""]
        }
        
        text_lower = text.lower()
        scores = {}
        
        for lang, words in common_words.items():
            count = sum(1 for word in words if f" {word} " in f" {text_lower} ")
            scores[lang] = count / len(words)
        
        return max(scores.items(), key=lambda x: x[1])[0]

def calculate_similarity(text1: str, text2: str) -> float:
    """
    Calculate semantic similarity between two texts.
    
    Args:
        text1: First text
        text2: Second text
        
    Returns:
        Similarity score between 0 and 1
    """
    if not text1 or not text2:
        return 0.0
        
    # Use sentence transformers if available
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            # Load model on first use
            global sentence_model
            if 'sentence_model' not in globals():
                sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
                
            # Encode texts
            embedding1 = sentence_model.encode(text1, convert_to_tensor=True)
            embedding2 = sentence_model.encode(text2, convert_to_tensor=True)
            
            # Calculate cosine similarity
            similarity = float(util.pytorch_cos_sim(embedding1, embedding2).item())
            
            return max(0.0, min(1.0, similarity))  # Ensure result is between 0 and 1
        except Exception as e:
            # Fall back to TF-IDF if sentence transformers fails
            pass
    
    # Fall back to scikit-learn TF-IDF
    if SKLEARN_AVAILABLE:
        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([text1, text2])
            similarity = float(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0, 0])
            
            return max(0.0, min(1.0, similarity))  # Ensure result is between 0 and 1
        except Exception as e:
            pass
    
    # Last resort: simple word overlap
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    if not words1 or not words2:
        return 0.0
        
    overlap = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    return overlap / union if union > 0 else 0.0

def create_directory_if_not_exists(directory_path: str) -> bool:
    """
    Create a directory if it doesn't exist.
    
    Args:
        directory_path: Path of directory to create
        
    Returns:
        True if directory was created, False if it already existed
    """
    if not os.path.exists(directory_path):
        os.makedirs(directory_path, exist_ok=True)
        return True
    return False

def get_file_hash(file_path: str) -> str:
    """
    Generate a hash for a file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Hexadecimal hash string
    """
    hasher = hashlib.md5()
    with open(file_path, 'rb') as f:
        buffer = f.read(65536)  # Read in 64kb chunks
        while buffer:
            hasher.update(buffer)
            buffer = f.read(65536)
    return hasher.hexdigest()

def sanitize_filename(filename: str) -> str:
    """
    Convert a string to a safe filename.
    
    Args:
        filename: Filename to sanitize
        
    Returns:
        Sanitized filename
    """
    # Replace spaces and unwanted characters
    safe_name = re.sub(r'[^\w\-\.]', '_', filename)
    return safe_name

def extract_date_from_string(date_string: str) -> Optional[datetime.datetime]:
    """
    Extract a date from various string formats.
    
    Args:
        date_string: String containing a date
        
    Returns:
        Datetime object or None if extraction fails
    """
    if not date_string:
        return None
        
    # Try various date formats
    date_formats = [
        '%Y-%m-%d',          # 2023-01-15
        '%Y/%m/%d',          # 2023/01/15
        '%d-%m-%Y',          # 15-01-2023
        '%d/%m/%Y',          # 15/01/2023
        '%Y-%m-%d %H:%M:%S', # 2023-01-15 14:30:00
        '%Y/%m/%d %H:%M:%S', # 2023/01/15 14:30:00
        '%d-%m-%Y %H:%M:%S', # 15-01-2023 14:30:00
        '%d/%m/%Y %H:%M:%S', # 15/01/2023 14:30:00
        '%B %d, %Y',         # January 15, 2023
        '%d %B %Y',          # 15 January 2023
        '%Y-%m-%dT%H:%M:%S', # 2023-01-15T14:30:00
        '%Y-%m-%dT%H:%M:%SZ' # 2023-01-15T14:30:00Z
    ]
    
    for fmt in date_formats:
        try:
            return datetime.datetime.strptime(date_string, fmt)
        except ValueError:
            continue
            
    # Try natural language parsing with regex
    patterns = [
        r'(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{4})',  # 15 Jan 2023
        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{1,2}),?\s+(\d{4})'  # Jan 15 2023
    ]
    
    for pattern in patterns:
        match = re.search(pattern, date_string, re.IGNORECASE)
        if match:
            month_map = {
                'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 
                'may': 5, 'jun': 6, 'jul': 7, 'aug': 8, 
                'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
            }
            
            groups = match.groups()
            if len(groups) == 3:
                # First format: day month year
                if groups[0].isdigit() and groups[2].isdigit():
                    day = int(groups[0])
                    month = month_map.get(groups[1].lower()[:3])
                    year = int(groups[2])
                    if month:
                        return datetime.datetime(year, month, day)
                # Second format: month day year
                elif groups[1].isdigit() and groups[2].isdigit():
                    month = month_map.get(groups[0].lower()[:3])
                    day = int(groups[1])
                    year = int(groups[2])
                    if month:
                        return datetime.datetime(year, month, day)
    
    # If all attempts fail
    return None

def format_date(date_obj: datetime.datetime, format_str: str = '%Y-%m-%d') -> str:
    """
    Format a datetime object as a string.
    
    Args:
        date_obj: Datetime object to format
        format_str: Format string
        
    Returns:
        Formatted date string
    """
    if not date_obj:
        return ""
        
    return date_obj.strftime(format_str)

def truncate_text(text: str, max_length: int = 100, ellipsis: str = '...') -> str:
    """
    Truncate text to a maximum length.
    
    Args:
        text: Text to truncate
        max_length: Maximum length
        ellipsis: String to add at the end if truncated
        
    Returns:
        Truncated text
    """
    if not text:
        return ""
        
    if len(text) <= max_length:
        return text
        
    # Try to truncate at word boundary
    truncated = text[:max_length]
    last_space = truncated.rfind(' ')
    
    if last_space > max_length * 0.8:  # Only truncate at word if it's not too short
        truncated = truncated[:last_space]
        
    return truncated + ellipsis

def generate_uuid() -> str:
    """
    Generate a UUID.
    
    Returns:
        UUID string
    """
    return str(uuid.uuid4())

def split_text(text: str, max_chunk_size: int = 300, overlap: int = 50) -> List[str]:
    """
    Split text into chunks with overlap.
    
    Args:
        text: Text to split
        max_chunk_size: Maximum chunk size in characters
        overlap: Overlap between chunks in characters
        
    Returns:
        List of text chunks
    """
    if not text:
        return []
        
    # If text is shorter than max size, return as is
    if len(text) <= max_chunk_size:
        return [text]
        
    chunks = []
    start = 0
    
    while start < len(text):
        # Get chunk of max_chunk_size or remainder
        end = min(start + max_chunk_size, len(text))
        
        # If this is not the end of the text, try to break at sentence or word boundary
        if end < len(text):
            # Try to find sentence end (., !, ?)
            sentence_end = max(
                text.rfind('. ', start, end),
                text.rfind('! ', start, end),
                text.rfind('? ', start, end)
            )
            
            if sentence_end > start + max_chunk_size * 0.7:  # Only use if not too short
                end = sentence_end + 1  # Include the period
            else:
                # Fall back to word boundary
                space = text.rfind(' ', start, end)
                if space > start + max_chunk_size * 0.7:  # Only use if not too short
                    end = space
        
        # Add chunk
        chunks.append(text[start:end].strip())
        
        # Move start position for next chunk, accounting for overlap
        start = end - overlap if end < len(text) else len(text)
        
    return chunks

def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    """
    Extract important keywords from text.
    
    Args:
        text: Text to analyze
        max_keywords: Maximum number of keywords to extract
        
    Returns:
        List of keywords
    """
    if not text:
        return []
        
    # Simple implementation based on word frequency
    # Remove punctuation
    text = text.lower()
    for char in string.punctuation:
        text = text.replace(char, ' ')
    
    # Split into words
    words = text.split()
    
    # Remove common stopwords
    stopwords = {
        'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 
        'be', 'been', 'being', 'to', 'of', 'for', 'with', 'by', 'about', 
        'against', 'between', 'into', 'through', 'during', 'before', 'after', 
        'above', 'below', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 
        'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 
        'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 
        'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 
        'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 
        'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',
        'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven',
        'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn',
        'weren', 'won', 'wouldn', 'that', 'this', 'these', 'those', 'am',
        'what', 'which', 'who', 'whom', 'have', 'had', 'has', 'as', 'if'
    }
    
    filtered_words = [word for word in words if word not in stopwords and len(word) > 2]
    
    # Count word frequency
    word_freq = {}
    for word in filtered_words:
        if word not in word_freq:
            word_freq[word] = 0
        word_freq[word] += 1
    
    # Sort by frequency
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    
    # Return top keywords
    return [word for word, _ in sorted_words[:max_keywords]]






Caching Mechanism (utils/cache.py)

"""
Caching mechanism for the RAG system.
Provides in-memory and disk-based caching with TTL support.
"""
import os
import json
import pickle
import time
import hashlib
import logging
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable, Tuple

logger = logging.getLogger(__name__)

class Cache:
    """
    Base cache class with common functionality.
    """
    
    def __init__(self, default_ttl: int = 3600):
        """
        Initialize the cache.
        
        Args:
            default_ttl: Default time-to-live in seconds (1 hour)
        """
        self.default_ttl = default_ttl
    
    def _get_cache_key(self, key: Union[str, Tuple]) -> str:
        """
        Generate a cache key.
        
        Args:
            key: Cache key (string or tuple)
            
        Returns:
            String key
        """
        if isinstance(key, str):
            return key
            
        # Convert tuples to strings
        if isinstance(key, tuple):
            return '_'.join(str(k) for k in key)
            
        # Hash other types
        return hashlib.md5(str(key).encode()).hexdigest()
    
    def get(self, key: Any, default: Any = None) -> Any:
        """
        Get a value from the cache.
        
        Args:
            key: Cache key
            default: Default value if key not found
            
        Returns:
            Cached value or default
        """
        raise NotImplementedError("Subclasses must implement get()")
    
    def set(self, key: Any, value: Any, ttl: Optional[int] = None) -> None:
        """
        Set a value in the cache.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time-to-live in seconds (None = default_ttl)
        """
        raise NotImplementedError("Subclasses must implement set()")
    
    def delete(self, key: Any) -> bool:
        """
        Delete a value from the cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if key was deleted, False if not found
        """
        raise NotImplementedError("Subclasses must implement delete()")
    
    def has(self, key: Any) -> bool:
        """
        Check if key exists in the cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if key exists, False otherwise
        """
        raise NotImplementedError("Subclasses must implement has()")
    
    def clear(self) -> None:
        """Clear all cache entries."""
        raise NotImplementedError("Subclasses must implement clear()")
    
    def get_or_set(self, key: Any, callable_fn: Callable, ttl: Optional[int] = None) -> Any:
        """
        Get a value from cache, or set it using a callable if not found.
        
        Args:
            key: Cache key
            callable_fn: Function to call to get value if not in cache
            ttl: Time-to-live in seconds
            
        Returns:
            Cached or computed value
        """
        value = self.get(key)
        if value is None:
            value = callable_fn()
            if value is not None:
                self.set(key, value, ttl)
        return value

class MemoryCache(Cache):
    """
    In-memory cache implementation with TTL support.
    """
    
    def __init__(self, default_ttl: int = 3600, max_size: int = 1000, cleanup_interval: int = 60):
        """
        Initialize memory cache.
        
        Args:
            default_ttl: Default time-to-live in seconds
            max_size: Maximum number of items in cache
            cleanup_interval: Interval in seconds to clean expired items
        """
        super().__init__(default_ttl)
        self.cache = {}  # {key: (value, expiry_time)}
        self.max_size = max_size
        self.cleanup_interval = cleanup_interval
        self.last_cleanup = time.time()
        self.lock = threading.RLock()  # Reentrant lock for thread safety
    
    def get(self, key: Any, default: Any = None) -> Any:
        """
        Get a value from the cache.
        
        Args:
            key: Cache key
            default: Default value if key not found
            
        Returns:
            Cached value or default
        """
        cache_key = self._get_cache_key(key)
        
        with self.lock:
            # Clean up expired items if needed
            self._cleanup_if_needed()
            
            if cache_key not in self.cache:
                return default
                
            value, expiry_time = self.cache[cache_key]
            
            # Check if expired
            if expiry_time < time.time():
                del self.cache[cache_key]
                return default
                
            return value
    
    def set(self, key: Any, value: Any, ttl: Optional[int] = None) -> None:
        """
        Set a value in the cache.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time-to-live in seconds (None = default_ttl)
        """
        cache_key = self._get_cache_key(key)
        
        with self.lock:
            # Clean up expired items if needed
            self._cleanup_if_needed()
            
            # Calculate expiry time
            ttl = ttl if ttl is not None else self.default_ttl
            expiry_time = time.time() + ttl
            
            # Evict items if cache is full
            if len(self.cache) >= self.max_size and cache_key not in self.cache:
                self._evict_items()
                
            # Store in cache
            self.cache[cache_key] = (value, expiry_time)
    
    def delete(self, key: Any) -> bool:
        """
        Delete a value from the cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if key was deleted, False if not found
        """
        cache_key = self._get_cache_key(key)
        
        with self.lock:
            if cache_key in self.cache:
                del self.cache[cache_key]
                return True
            return False
    
    def has(self, key: Any) -> bool:
        """
        Check if key exists in the cache and is not expired.
        
        Args:
            key: Cache key
            
        Returns:
            True if key exists and not expired, False otherwise
        """
        cache_key = self._get_cache_key(key)
        
        with self.lock:
            if cache_key not in self.cache:
                return False
                
            _, expiry_time = self.cache[cache_key]
            return expiry_time >= time.time()
    
    def clear(self) -> None:
        """Clear all cache entries."""
        with self.lock:
            self.cache.clear()
    
    def _cleanup_if_needed(self) -> None:
        """Clean up expired items if cleanup interval has passed."""
        now = time.time()
        if now - self.last_cleanup > self.cleanup_interval:
            self._cleanup_expired()
            self.last_cleanup = now
    
    def _cleanup_expired(self) -> None:
        """Remove all expired items from cache."""
        now = time.time()
        keys_to_delete = [k for k, (_, exp) in self.cache.items() if exp < now]
        for key in keys_to_delete:
            del self.cache[key]
    
    def _evict_items(self) -> None:
        """Evict items when cache is full."""
        # First, remove expired items
        self._cleanup_expired()
        
        # If still over max size, remove oldest items
        if len(self.cache) >= self.max_size:
            # Sort by expiry time
            sorted_items = sorted(self.cache.items(), key=lambda x: x[1][1])
            
            # Remove oldest 10% of items
            num_to_remove = max(1, len(self.cache) // 10)
            for i in range(num_to_remove):
                if i < len(sorted_items):
                    del self.cache[sorted_items[i][0]]

class DiskCache(Cache):
    """
    Disk-based cache implementation with TTL support.
    """
    
    def __init__(self, cache_dir: str, default_ttl: int = 3600, 
                 max_size_mb: int = 100, cleanup_interval: int = 3600,
                 use_json: bool = True):
        """
        Initialize disk cache.
        
        Args:
            cache_dir: Directory to store cache files
            default_ttl: Default time-to-live in seconds
            max_size_mb: Maximum cache size in MB
            cleanup_interval: Interval in seconds to clean expired items
            use_json: Use JSON serialization (True) or pickle (False)
        """
        super().__init__(default_ttl)
        self.cache_dir = Path(cache_dir)
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cleanup_interval = cleanup_interval
        self.use_json = use_json
        self.last_cleanup = time.time()
        self.lock = threading.RLock()  # Reentrant lock for thread safety
        
        # Create cache directory if it doesn't exist
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Create metadata file if it doesn't exist
        self.metadata_file = self.cache_dir / 'metadata.json'
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict[str, Dict[str, Union[str, int, float]]]:
        """Load cache metadata from disk."""
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.warning(f"Error loading cache metadata: {str(e)}")
            return {}
    
    def _save_metadata(self) -> None:
        """Save cache metadata to disk."""
        try:
            with open(self.metadata_file, 'w') as f:
                json.dump(self.metadata, f)
        except Exception as e:
            logger.warning(f"Error saving cache metadata: {str(e)}")
    
    def _get_cache_filename(self, key: str) -> str:
        """
        Get filename for a cache key.
        
        Args:
            key: Cache key
            
        Returns:
            Cache filename
        """
        key_hash = hashlib.md5(key.encode()).hexdigest()
        extension = '.json' if self.use_json else '.pickle'
        return key_hash + extension
    
    def _get_cache_filepath(self, key: str) -> Path:
        """
        Get filepath for a cache key.
        
        Args:
            key: Cache key
            
        Returns:
            Path to cache file
        """
        filename = self._get_cache_filename(key)
        return self.cache_dir / filename
    
    def get(self, key: Any, default: Any = None) -> Any:
        """
        Get a value from the cache.
        
        Args:
            key: Cache key
            default: Default value if key not found
            
        Returns:
            Cached value or default
        """
        cache_key = self._get_cache_key(key)
        file_path = self._get_cache_filepath(cache_key)
        
        with self.lock:
            # Clean up expired items if needed
            self._cleanup_if_needed()
            
            # Check if file exists
            if not file_path.exists():
                return default
                
            # Check metadata for expiry
            if cache_key in self.metadata:
                expiry_time = self.metadata[cache_key].get('expiry_time', 0)
                if expiry_time < time.time():
                    # Expired - remove file and metadata
                    file_path.unlink(missing_ok=True)
                    del self.metadata[cache_key]
                    self._save_metadata()
                    return default
            
            # Read from file
            try:
                if self.use_json:
                    with open(file_path, 'r') as f:
                        return json.load(f)
                else:
                    with open(file_path, 'rb') as f:
                        return pickle.load(f)
            except Exception as e:
                logger.warning(f"Error reading cache file {file_path}: {str(e)}")
                return default
    
    def set(self, key: Any, value: Any, ttl: Optional[int] = None) -> None:
        """
        Set a value in the cache.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time-to-live in seconds (None = default_ttl)
        """
        cache_key = self._get_cache_key(key)
        file_path = self._get_cache_filepath(cache_key)
        
        with self.lock:
            # Clean up expired items if needed
            self._cleanup_if_needed()
            
            # Calculate expiry time
            ttl = ttl if ttl is not None else self.default_ttl
            expiry_time = time.time() + ttl
            
            # Check if disk cache is full
            self._check_disk_space()
            
            # Write to file
            try:
                if self.use_json:
                    with open(file_path, 'w') as f:
                        json.dump(value, f)
                else:
                    with open(file_path, 'wb') as f:
                        pickle.dump(value, f)
                        
                # Update metadata
                file_size = file_path.stat().st_size
                self.metadata[cache_key] = {
                    'expiry_time': expiry_time,
                    'size': file_size,
                    'file_path': str(file_path),
                    'created_at': time.time()
                }
                self._save_metadata()
                
            except Exception as e:
                logger.warning(f"Error writing cache file {file_path}: {str(e)}")
    
    def delete(self, key: Any) -> bool:
        """
        Delete a value from the cache.
        
        Args:
            key: Cache key
            
        Returns:
            True if key was deleted, False if not found
        """
        cache_key = self._get_cache_key(key)
        file_path = self._get_cache_filepath(cache_key)
        
        with self.lock:
            if file_path.exists():
                try:
                    file_path.unlink()
                    if cache_key in self.metadata:
                        del self.metadata[cache_key]
                        self._save_metadata()
                    return True
                except Exception as e:
                    logger.warning(f"Error deleting cache file {file_path}: {str(e)}")
            
            return False
    
    def has(self, key: Any) -> bool:
        """
        Check if key exists in the cache and is not expired.
        
        Args:
            key: Cache key
            
        Returns:
            True if key exists and not expired, False otherwise
        """
        cache_key = self._get_cache_key(key)
        file_path = self._get_cache_filepath(cache_key)
        
        with self.lock:
            if not file_path.exists():
                return False
                
            # Check metadata for expiry
            if cache_key in self.metadata:
                expiry_time = self.metadata[cache_key].get('expiry_time', 0)
                return expiry_time >= time.time()
                
            return False
    
    def clear(self) -> None:
        """Clear all cache entries."""
        with self.lock:
            for file_path in self.cache_dir.glob('*.*'):
                if file_path.name != 'metadata.json':
                    try:
                        file_path.unlink()
                    except Exception as e:
                        logger.warning(f"Error deleting cache file {file_path}: {str(e)}")
            
            self.metadata = {}
            self._save_metadata()
    
    def _cleanup_if_needed(self) -> None:
        """Clean up expired items if cleanup interval has passed."""
        now = time.time()
        if now - self.last_cleanup > self.cleanup_interval:
            self._cleanup_expired()
            self.last_cleanup = now
    
    def _cleanup_expired(self) -> None:
        """Remove all expired items from cache."""
        now = time.time()
        keys_to_delete = []
        
        for key, meta in self.metadata.items():
            if meta.get('expiry_time', 0) < now:
                keys_to_delete.append(key)
                try:
                    file_path = Path(meta.get('file_path', ''))
                    if file_path.exists():
                        file_path.unlink()
                except Exception as e:
                    logger.warning(f"Error deleting expired cache file: {str(e)}")
        
        # Update metadata
        for key in keys_to_delete:
            if key in self.metadata:
                del self.metadata[key]
                
        self._save_metadata()
    
    def _check_disk_space(self) -> None:
        """Check if disk cache is full and evict items if needed."""
        # Calculate total size
        total_size = sum(meta.get('size', 0) for meta in self.metadata.values())
        
        # If over limit, remove oldest items
        if total_size > self.max_size_bytes:
            # Sort by creation time
            sorted_items = sorted(
                self.metadata.items(), 
                key=lambda x: x[1].get('created_at', 0)
            )
            
            # Remove oldest items until under limit
            for key, meta in sorted_items:
                file_path = Path(meta.get('file_path', ''))
                try:
                    if file_path.exists():
                        file_path.unlink()
                except Exception as e:
                    logger.warning(f"Error deleting cache file during eviction: {str(e)}")
                    
                # Update total size
                total_size -= meta.get('size', 0)
                del self.metadata[key]
                
                # Stop if under limit
                if total_size <= self.max_size_bytes * 0.9:  # 10% margin
                    break
                    
            # Save updated metadata
            self._save_metadata()

class CacheFactory:
    """
    Factory for creating cache instances.
    """
    
    @staticmethod
    def create(cache_type: str, **kwargs) -> Cache:
        """
        Create a cache instance.
        
        Args:
            cache_type: Type of cache ('memory' or 'disk')
            **kwargs: Additional arguments for the cache
            
        Returns:
            Cache instance
        """
        if cache_type.lower() == 'memory':
            return MemoryCache(**kwargs)
        elif cache_type.lower() == 'disk':
            return DiskCache(**kwargs)
        else:
            raise ValueError(f"Unknown cache type: {cache_type}")













Performance Metrics (utils/metrics.py)


"""
Performance metrics for the RAG system.
Provides functions for measuring and evaluating system performance.
"""
import time
import functools
import logging
import inspect
import os
import psutil
from typing import Dict, List, Any, Callable, Optional, Union, Tuple
from datetime import datetime

# Try to import numpy and matplotlib for stats
try:
    import numpy as np
    import matplotlib.pyplot as plt
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

class PerformanceTracker:
    """Track execution time and resource usage for functions."""
    
    def __init__(self, history_size: int = 100):
        """
        Initialize the performance tracker.
        
        Args:
            history_size: Number of historical measurements to store
        """
        self.history_size = history_size
        self.measurements = {}  # {function_name: [measurements]}
        self.process = psutil.Process(os.getpid())
    
    def start_measurement(self, name: str) -> Dict[str, Any]:
        """
        Start a new measurement.
        
        Args:
            name: Name of the measurement
            
        Returns:
            Measurement dictionary with start time
        """
        measurement = {
            'name': name,
            'start_time': time.time(),
            'start_memory': self.process.memory_info().rss,
            'start_cpu': self.process.cpu_percent()
        }
        return measurement
    
    def end_measurement(self, measurement: Dict[str, Any]) -> Dict[str, Any]:
        """
        End a measurement and calculate metrics.
        
        Args:
            measurement: Measurement dictionary from start_measurement
            
        Returns:
            Updated measurement dictionary with metrics
        """
        name = measurement['name']
        end_time = time.time()
        end_memory = self.process.memory_info().rss
        
        # Calculate metrics
        measurement['end_time'] = end_time
        measurement['end_memory'] = end_memory
        measurement['duration'] = end_time - measurement['start_time']
        measurement['memory_delta'] = end_memory - measurement['start_memory']
        measurement['end_cpu'] = self.process.cpu_percent()
        measurement['timestamp'] = datetime.now().isoformat()
        
        # Add to history
        if name not in self.measurements:
            self.measurements[name] = []
            
        self.measurements[name].append(measurement)
        
        # Limit history size
        if len(self.measurements[name]) > self.history_size:
            self.measurements[name].pop(0)
            
        return measurement
    
    def get_stats(self, name: str) -> Dict[str, Any]:
        """
        Get statistics for a measurement type.
        
        Args:
            name: Name of the measurement
            
        Returns:
            Dictionary with statistics or empty dict if no measurements
        """
        if name not in self.measurements or not self.measurements[name]:
            return {}
            
        measurements = self.measurements[name]
        durations = [m['duration'] for m in measurements]
        memory_deltas = [m['memory_delta'] for m in measurements]
        
        stats = {
            'count': len(measurements),
            'avg_duration': sum(durations) / len(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'avg_memory_delta': sum(memory_deltas) / len(memory_deltas),
            'last_measurement': measurements[-1]
        }
        
        if NUMPY_AVAILABLE and len(durations) > 1:
            stats['duration_stddev'] = float(np.std(durations))
            stats['duration_percentiles'] = {
                '50': float(np.percentile(durations, 50)),
                '90': float(np.percentile(durations, 90)),
                '95': float(np.percentile(durations, 95)),
                '99': float(np.percentile(durations, 99))
            }
            
        return stats
    
    def print_stats(self, name: str) -> None:
        """
        Print statistics for a measurement type.
        
        Args:
            name: Name of the measurement
        """
        stats = self.get_stats(name)
        if not stats:
            print(f"No measurements for '{name}'")
            return
            
        print(f"Statistics for '{name}':")
        print(f"  Count: {stats['count']}")
        print(f"  Average duration: {stats['avg_duration']:.6f}s")
        print(f"  Min duration: {stats['min_duration']:.6f}s")
        print(f"  Max duration: {stats['max_duration']:.6f}s")
        
        if 'duration_stddev' in stats:
            print(f"  Standard deviation: {stats['duration_stddev']:.6f}s")
            print(f"  P50: {stats['duration_percentiles']['50']:.6f}s")
            print(f"  P90: {stats['duration_percentiles']['90']:.6f}s")
            print(f"  P95: {stats['duration_percentiles']['95']:.6f}s")
            
        print(f"  Average memory delta: {stats['avg_memory_delta'] / (1024 * 1024):.2f} MB")
    
    def plot_history(self, name: str, metric: str = 'duration', 
                    save_path: Optional[str] = None) -> None:
        """
        Plot measurement history.
        
        Args:
            name: Name of the measurement
            metric: Metric to plot ('duration' or 'memory_delta')
            save_path: Path to save plot (None to display)
        """
        if not NUMPY_AVAILABLE:
            logger.warning("Cannot plot history: numpy or matplotlib not available")
            return
            
        if name not in self.measurements or not self.measurements[name]:
            logger.warning(f"No measurements for '{name}'")
            return
            
        measurements = self.measurements[name]
        
        if metric == 'duration':
            values = [m['duration'] for m in measurements]
            title = f"{name} Duration History"
            ylabel = "Duration (s)"
        elif metric == 'memory_delta':
            values = [m['memory_delta'] / (1024 * 1024) for m in measurements]  # Convert to MB
            title = f"{name} Memory Usage History"
            ylabel = "Memory Delta (MB)"
        else:
            logger.warning(f"Unknown metric: {metric}")
            return
            
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(values)), values, marker='o')
        plt.title(title)
        plt.xlabel("Measurement Number")
        plt.ylabel(ylabel)
        plt.grid(True)
        
        if save_path:
            plt.savefig(save_path)
        else:
            plt.show()


# Create a global performance tracker
performance_tracker = PerformanceTracker()

def timeit(func: Optional[Callable] = None, name: Optional[str] = None, 
          log_level: Optional[int] = logging.DEBUG) -> Callable:
    """
    Decorator to measure and log execution time.
    
    Args:
        func: Function to decorate
        name: Name for the measurement (defaults to function name)
        log_level: Logging level for the output
        
    Returns:
        Decorated function
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            func_name = name if name else func.__name__
            logger_name = func.__module__ if hasattr(func, '__module__') else 'metrics'
            func_logger = logging.getLogger(logger_name)
            
            # Start measurement
            start_time = time.time()
            measurement = performance_tracker.start_measurement(func_name)
            
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                # End measurement
                end_time = time.time()
                duration = end_time - start_time
                
                performance_tracker.end_measurement(measurement)
                
                # Log result
                if log_level is not None:
                    func_logger.log(log_level, f"{func_name} executed in {duration:.6f} seconds")
                    
        return wrapper
        
    if func is None:
        return decorator
    return decorator(func)

def measure_memory_usage(func: Optional[Callable] = None, name: Optional[str] = None, 
                        log_level: Optional[int] = logging.DEBUG) -> Callable:
    """
    Decorator to measure and log memory usage.
    
    Args:
        func: Function to decorate
        name: Name for the measurement (defaults to function name)
        log_level: Logging level for the output
        
    Returns:
        Decorated function
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            func_name = name if name else func.__name__
            logger_name = func.__module__ if hasattr(func, '__module__') else 'metrics'
            func_logger = logging.getLogger(logger_name)
            
            # Start measurement
            process = psutil.Process(os.getpid())
            start_memory = process.memory_info().rss
            
            result = func(*args, **kwargs)
            
            # End measurement
            end_memory = process.memory_info().rss
            memory_delta = end_memory - start_memory
            
            # Log result
            if log_level is not None:
                func_logger.log(
                    log_level, 
                    f"{func_name} memory usage: {memory_delta / (1024 * 1024):.2f} MB"
                )
                
            return result
            
        return wrapper
        
    if func is None:
        return decorator
    return decorator(func)

def calculate_precision(relevant_items: int, retrieved_items: int) -> float:
    """
    Calculate precision metric.
    
    Args:
        relevant_items: Number of relevant items retrieved
        retrieved_items: Total number of items retrieved
        
    Returns:
        Precision value (0-1)
    """
    if retrieved_items == 0:
        return 0.0
    return relevant_items / retrieved_items

def calculate_recall(relevant_items: int, total_relevant_items: int) -> float:
    """
    Calculate recall metric.
    
    Args:
        relevant_items: Number of relevant items retrieved
        total_relevant_items: Total number of relevant items
        
    Returns:
        Recall value (0-1)
    """
    if total_relevant_items == 0:
        return 0.0
    return relevant_items / total_relevant_items

def calculate_f1(precision: float, recall: float) -> float:
    """
    Calculate F1 score from precision and recall.
    
    Args:
        precision: Precision value
        recall: Recall value
        
    Returns:
        F1 score (0-1)
    """
    if precision + recall == 0:
        return 0.0
    return 2 * (precision * recall) / (precision + recall)

def calculate_latency(times: List[float], percentile: float = 95) -> float:
    """
    Calculate latency at a specific percentile.
    
    Args:
        times: List of execution times
        percentile: Percentile to calculate (0-100)
        
    Returns:
        Latency value at the specified percentile
    """
    if not times:
        return 0.0
        
    if NUMPY_AVAILABLE:
        return float(np.percentile(times, percentile))
    else:
        # Manual calculation
        sorted_times = sorted(times)
        idx = int(len(sorted_times) * percentile / 100)
        return sorted_times[idx]

class QueryMetrics:
    """
    Collect and analyze query processing metrics.
    """
    
    def __init__(self, max_history: int = 1000):
        """
        Initialize query metrics.
        
        Args:
            max_history: Maximum number of queries to track
        """
        self.max_history = max_history
        self.queries = []
        self.query_types = {}  # {query_type: count}
        self.query_intents = {}  # {intent: count}
        self.sources_used = {}  # {source: count}
        self.latencies = []
        
    def record_query(self, query: str, processing_time: float, 
                    query_type: str, intent: str, sources: List[str],
                    result_count: int, user_id: Optional[str] = None) -> None:
        """
        Record a query for metrics tracking.
        
        Args:
            query: Query string
            processing_time: Processing time in seconds
            query_type: Type of query
            intent: Query intent
            sources: Sources used for this query
            result_count: Number of results returned
            user_id: Optional user identifier
        """
        # Create query record
        timestamp = datetime.now().isoformat()
        query_record = {
            'query': query,
            'timestamp': timestamp,
            'processing_time': processing_time,
            'query_type': query_type,
            'intent': intent,
            'sources': sources,
            'result_count': result_count,
            'user_id': user_id
        }
        
        # Add to history (limit size)
        self.queries.append(query_record)
        if len(self.queries) > self.max_history:
            self.queries.pop(0)
            
        # Update latencies
        self.latencies.append(processing_time)
        if len(self.latencies) > self.max_history:
            self.latencies.pop(0)
            
        # Update query types
        if query_type not in self.query_types:
            self.query_types[query_type] = 0
        self.query_types[query_type] += 1
        
        # Update query intents
        if intent not in self.query_intents:
            self.query_intents[intent] = 0
        self.query_intents[intent] += 1
        
        # Update sources used
        for source in sources:
            if source not in self.sources_used:
                self.sources_used[source] = 0
            self.sources_used[source] += 1
            
    def get_summary(self) -> Dict[str, Any]:
        """
        Get a summary of query metrics.
        
        Returns:
            Dictionary with metric summary
        """
        if not self.queries:
            return {
                'total_queries': 0,
                'avg_processing_time': 0,
                'query_types': {},
                'query_intents': {},
                'sources_used': {}
            }
            
        # Calculate metrics
        total_queries = len(self.queries)
        avg_processing_time = sum(q['processing_time'] for q in self.queries) / total_queries
        
        metrics = {
            'total_queries': total_queries,
            'avg_processing_time': avg_processing_time,
            'min_processing_time': min(q['processing_time'] for q in self.queries),
            'max_processing_time': max(q['processing_time'] for q in self.queries),
            'query_types': self.query_types,
            'query_intents': self.query_intents,
            'sources_used': self.sources_used,
        }
        
        # Add percentiles if numpy is available
        if NUMPY_AVAILABLE and self.latencies:
            metrics['latency_p50'] = float(np.percentile(self.latencies, 50))
            metrics['latency_p90'] = float(np.percentile(self.latencies, 90))
            metrics['latency_p95'] = float(np.percentile(self.latencies, 95))
            metrics['latency_p99'] = float(np.percentile(self.latencies, 99))
            
        return metrics
    
    def get_recent_queries(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get most recent queries.
        
        Args:
            limit: Maximum number of queries to return
            
        Returns:
            List of recent query records
        """
        return self.queries[-limit:]
    
    def plot_latency_distribution(self, save_path: Optional[str] = None) -> None:
        """
        Plot the distribution of query latencies.
        
        Args:
            save_path: Path to save plot (None to display)
        """
        if not NUMPY_AVAILABLE:
            logger.warning("Cannot plot latency: numpy or matplotlib not available")
            return
            
        if not self.latencies:
            logger.warning("No latency data available")
            return
            
        plt.figure(figsize=(10, 6))
        plt.hist(self.latencies, bins=20, alpha=0.7)
        plt.axvline(np.percentile(self.latencies, 95), color='r', linestyle='dashed', 
                   linewidth=2, label='P95')
        plt.axvline(np.percentile(self.latencies, 50), color='g', linestyle='dashed', 
                   linewidth=2, label='P50')
        
        plt.title("Query Latency Distribution")
        plt.xlabel("Latency (s)")
        plt.ylabel("Frequency")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path)
        else:
            plt.show()

class SystemMetrics:
    """
    Collect and analyze system-wide metrics.
    """
    
    def __init__(self, update_interval: int = 60):
        """
        Initialize system metrics.
        
        Args:
            update_interval: Interval in seconds for metric updates
        """
        self.update_interval = update_interval
        self.last_update = 0
        self.cpu_usage = []
        self.memory_usage = []
        self.disk_usage = []
        self.timestamps = []
        self.max_history_points = 1000
        
    def update(self) -> Dict[str, Any]:
        """
        Update system metrics if interval has passed.
        
        Returns:
            Current metrics
        """
        current_time = time.time()
        
        # Check if update is needed
        if current_time - self.last_update < self.update_interval:
            # If we have data, return the latest
            if self.cpu_usage:
                return {
                    'timestamp': self.timestamps[-1],
                    'cpu': self.cpu_usage[-1],
                    'memory': self.memory_usage[-1],
                    'disk': self.disk_usage[-1]
                }
            # Otherwise, force an update
        
        # Update metrics
        timestamp = datetime.now().isoformat()
        
        # CPU usage (percentage)
        cpu = psutil.cpu_percent(interval=0.1)
        
        # Memory usage
        memory = psutil.virtual_memory()
        memory_used_percent = memory.percent
        
        # Disk usage
        disk = psutil.disk_usage('/')
        disk_used_percent = disk.percent
        
        # Store in history
        self.timestamps.append(timestamp)
        self.cpu_usage.append(cpu)
        self.memory_usage.append(memory_used_percent)
        self.disk_usage.append(disk_used_percent)
        
        # Limit history size
        if len(self.timestamps) > self.max_history_points:
            self.timestamps.pop(0)
            self.cpu_usage.pop(0)
            self.memory_usage.pop(0)
            self.disk_usage.pop(0)
            
        self.last_update = current_time
        
        return {
            'timestamp': timestamp,
            'cpu': cpu,
            'memory': memory_used_percent,
            'disk': disk_used_percent
        }
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Get a summary of system metrics.
        
        Returns:
            Dictionary with metric summary
        """
        if not self.cpu_usage:
            self.update()  # Force an update if no data
            
        # Calculate averages
        avg_cpu = sum(self.cpu_usage) / len(self.cpu_usage) if self.cpu_usage else 0
        avg_memory = sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0
        avg_disk = sum(self.disk_usage) / len(self.disk_usage) if self.disk_usage else 0
        
        metrics = {
            'avg_cpu': avg_cpu,
            'avg_memory': avg_memory,
            'avg_disk': avg_disk,
            'current': self.update(),  # Get latest metrics
            'process_count': len(psutil.pids())
        }
        
        # Add detailed memory info
        memory = psutil.virtual_memory()
        metrics['memory_details'] = {
            'total': memory.total,
            'available': memory.available,
            'used': memory.used,
            'free': memory.free
        }
        
        # Add detailed disk info
        disk = psutil.disk_usage('/')
        metrics['disk_details'] = {
            'total': disk.total,
            'used': disk.used,
            'free': disk.free
        }
        
        return metrics
    
    def plot_history(self, metric: str = 'cpu', save_path: Optional[str] = None) -> None:
        """
        Plot historical system metrics.
        
        Args:
            metric: Metric to plot ('cpu', 'memory', or 'disk')
            save_path: Path to save plot (None to display)
        """
        if not NUMPY_AVAILABLE:
            logger.warning("Cannot plot history: numpy or matplotlib not available")
            return
            
        # Ensure we have data
        if not self.timestamps:
            logger.warning("No metric data available")
            return
            
        # Select data based on metric
        if metric == 'cpu':
            data = self.cpu_usage
            title = "CPU Usage History"
            ylabel = "CPU Usage (%)"
        elif metric == 'memory':
            data = self.memory_usage
            title = "Memory Usage History"
            ylabel = "Memory Usage (%)"
        elif metric == 'disk':
            data = self.disk_usage
            title = "Disk Usage History"
            ylabel = "Disk Usage (%)"
        else:
            logger.warning(f"Unknown metric: {metric}")
            return
            
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(data)), data)
        plt.title(title)
        plt.xlabel("Time")
        plt.ylabel(ylabel)
        plt.grid(True, alpha=0.3)
        
        # Add recent timestamps as x-ticks
        if len(self.timestamps) > 10:
            # Show 10 evenly spaced timestamps
            step = len(self.timestamps) // 10
            indices = range(0, len(self.timestamps), step)
            labels = [self.timestamps[i].split('T')[1].split('.')[0] for i in indices]
            plt.xticks(indices, labels, rotation=45)
            
        if save_path:
            plt.savefig(save_path)
        else:
            plt.show()


# Initialize global metrics objects
query_metrics = QueryMetrics()
system_metrics = SystemMetrics()








NLP Package Initialization (nlp/__init__.py)



"""
Natural Language Processing package for the RAG system.
Contains modules for embeddings, entity extraction, summarization, and more.
"""
from nlp.embeddings import (
    get_embedding_model,
    encode_text,
    encode_query,
    calculate_similarity,
    EmbeddingModel,
    SentenceTransformerModel,
    SpacyModel,
    TfidfModel
)
from nlp.entity_extraction import (
    extract_entities,
    extract_keywords,
    extract_relations,
    EntityExtractor,
    SpacyEntityExtractor,
    RegexEntityExtractor
)
from nlp.summarization import (
    generate_summary,
    TextRankSummarizer,
    LexRankSummarizer,
    TfidfSummarizer,
    ExtractiveSummarizer
)
from nlp.query_analysis import (
    analyze_query,
    detect_query_intent,
    detect_query_language,
    QueryAnalyzer
)
from nlp.answer_generation import (
    generate_answer,
    format_answer,
    AnswerGenerator,
    FactVerifier
)

__all__ = [
    'get_embedding_model',
    'encode_text',
    'encode_query',
    'calculate_similarity',
    'EmbeddingModel',
    'SentenceTransformerModel',
    'SpacyModel',
    'TfidfModel',
    'extract_entities',
    'extract_keywords',
    'extract_relations',
    'EntityExtractor',
    'SpacyEntityExtractor',
    'RegexEntityExtractor',
    'generate_summary',
    'TextRankSummarizer',
    'LexRankSummarizer',
    'TfidfSummarizer',
    'ExtractiveSummarizer',
    'analyze_query',
    'detect_query_intent',
    'detect_query_language',
    'QueryAnalyzer',
    'generate_answer',
    'format_answer',
    'AnswerGenerator',
    'FactVerifier'
]














Embedding Models for Text Representation (nlp/embeddings.py)



"""
Embedding models for text representation in the RAG system.
Supports multiple embedding models with automatic fallback mechanisms.
"""
import os
import logging
import pickle
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Union, Any, Tuple

# Try to import sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Try to import spaCy
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

# Try to import scikit-learn for TF-IDF
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)

class EmbeddingModel(ABC):
    """Abstract base class for embedding models."""
    
    @abstractmethod
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """
        Encode text into embeddings.
        
        Args:
            texts: Text or list of texts to encode
            batch_size: Batch size for encoding
            
        Returns:
            Numpy array of embeddings
        """
        pass
    
    @abstractmethod
    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.
        
        Returns:
            Dimension size
        """
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        """
        Get the model name.
        
        Returns:
            Model name
        """
        pass
    
    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            
        Returns:
            Cosine similarity score
        """
        # Ensure embeddings are normalized
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        embedding1_normalized = embedding1 / norm1
        embedding2_normalized = embedding2 / norm2
        
        # Compute cosine similarity
        return float(np.dot(embedding1_normalized, embedding2_normalized))

class SentenceTransformerModel(EmbeddingModel):
    """Embedding model based on sentence-transformers."""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Initialize the sentence transformer model.
        
        Args:
            model_name: Name of the sentence-transformers model
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError("sentence-transformers is not installed. "
                             "Install it with 'pip install sentence-transformers'.")
                             
        self.model_name = model_name
        
        try:
            self.model = SentenceTransformer(model_name)
            logger.info(f"Loaded sentence-transformers model: {model_name}")
        except Exception as e:
            logger.error(f"Error loading sentence-transformers model {model_name}: {str(e)}")
            raise
    
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """
        Encode text into embeddings.
        
        Args:
            texts: Text or list of texts to encode
            batch_size: Batch size for encoding
            
        Returns:
            Numpy array of embeddings
        """
        if isinstance(texts, str):
            texts = [texts]
            
        try:
            embeddings = self.model.encode(texts, batch_size=batch_size, show_progress_bar=False)
            return embeddings
        except Exception as e:
            logger.error(f"Error encoding text with sentence-transformers: {str(e)}")
            # Return zero embeddings as fallback
            return np.zeros((len(texts), self.get_dimension()))
    
    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.
        
        Returns:
            Dimension size
        """
        return self.model.get_sentence_embedding_dimension()
    
    def get_model_name(self) -> str:
        """
        Get the model name.
        
        Returns:
            Model name
        """
        return self.model_name

class SpacyModel(EmbeddingModel):
    """Embedding model based on spaCy."""
    
    def __init__(self, model_name: str = "en_core_web_md"):
        """
        Initialize the spaCy model.
        
        Args:
            model_name: Name of the spaCy model
        """
        if not SPACY_AVAILABLE:
            raise ImportError("spaCy is not installed. "
                             "Install it with 'pip install spacy' and "
                             "download the model with 'python -m spacy download en_core_web_md'.")
                             
        self.model_name = model_name
        
        try:
            self.nlp = spacy.load(model_name)
            # Check if model has vectors
            if not self.nlp.has_pipe("tok2vec") and not self.nlp.has_pipe("transformer"):
                logger.warning(f"spaCy model {model_name} does not have word vectors. "
                              "Embeddings will be based on simple averaging of token vectors.")
            logger.info(f"Loaded spaCy model: {model_name}")
        except Exception as e:
            logger.error(f"Error loading spaCy model {model_name}: {str(e)}")
            raise
    
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """
        Encode text into embeddings.
        
        Args:
            texts: Text or list of texts to encode
            batch_size: Batch size for encoding
            
        Returns:
            Numpy array of embeddings
        """
        if isinstance(texts, str):
            texts = [texts]
            
        # Process texts
        embeddings = []
        for text in texts:
            try:
                doc = self.nlp(text)
                if doc.vector.size > 0:
                    embeddings.append(doc.vector)
                else:
                    # Fall back to average of token vectors
                    tokens = [token for token in doc if not token.is_stop and not token.is_punct]
                    if tokens:
                        token_vectors = np.array([token.vector for token in tokens])
                        embeddings.append(np.mean(token_vectors, axis=0))
                    else:
                        # Empty or only stop words/punctuation
                        embeddings.append(np.zeros(self.get_dimension()))
            except Exception as e:
                logger.error(f"Error encoding text with spaCy: {str(e)}")
                embeddings.append(np.zeros(self.get_dimension()))
                
        return np.array(embeddings)
    
    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.
        
        Returns:
            Dimension size
        """
        return self.nlp.vocab.vectors_length
    
    def get_model_name(self) -> str:
        """
        Get the model name.
        
        Returns:
            Model name
        """
        return self.model_name

class TfidfModel(EmbeddingModel):
    """Embedding model based on TF-IDF."""
    
    def __init__(self, max_features: int = 5000):
        """
        Initialize the TF-IDF model.
        
        Args:
            max_features: Maximum number of features
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learn is not installed. "
                             "Install it with 'pip install scikit-learn'.")
                             
        self.max_features = max_features
        self.model_name = f"tfidf-{max_features}"
        self.vectorizer = TfidfVectorizer(
            max_features=max_features, 
            stop_words='english', 
            ngram_range=(1, 2)
        )
        self.is_fitted = False
        self.dimension = max_features
    
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """
        Encode text into embeddings.
        
        Args:
            texts: Text or list of texts to encode
            batch_size: Batch size for encoding
            
        Returns:
            Numpy array of embeddings
        """
        if isinstance(texts, str):
            texts = [texts]
            
        try:
            if not self.is_fitted:
                # First time: fit and transform
                embeddings = self.vectorizer.fit_transform(texts).toarray()
                self.is_fitted = True
                self.dimension = embeddings.shape[1]
                return embeddings
            else:
                # Already fitted: just transform
                embeddings = self.vectorizer.transform(texts).toarray()
                return embeddings
        except Exception as e:
            logger.error(f"Error encoding text with TF-IDF: {str(e)}")
            # Return zero embeddings as fallback
            return np.zeros((len(texts), self.get_dimension()))
    
    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.
        
        Returns:
            Dimension size
        """
        return self.dimension
    
    def get_model_name(self) -> str:
        """
        Get the model name.
        
        Returns:
            Model name
        """
        return self.model_name

class EmbeddingModelFactory:
    """Factory for creating embedding models with caching."""
    
    _instances = {}
    
    @classmethod
    def get_model(cls, model_type: str, model_name: str = None, 
                 cache_dir: Optional[str] = None) -> EmbeddingModel:
        """
        Get an embedding model instance, reusing existing instances.
        
        Args:
            model_type: Type of model ('sentence-transformer', 'spacy', 'tfidf')
            model_name: Name of the model
            cache_dir: Directory to cache models
            
        Returns:
            Embedding model instance
        """
        # Set default model names
        if model_name is None:
            if model_type == 'sentence-transformer':
                model_name = 'all-MiniLM-L6-v2'
            elif model_type == 'spacy':
                model_name = 'en_core_web_md'
            elif model_type == 'tfidf':
                model_name = 'tfidf-5000'
        
        # Create cache key
        cache_key = f"{model_type}_{model_name}"
        
        # Check if instance already exists
        if cache_key in cls._instances:
            return cls._instances[cache_key]
            
        # Create new instance
        if model_type == 'sentence-transformer':
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                logger.warning("sentence-transformers not available, falling back to alternative model")
                return cls.get_fallback_model(cache_dir)
            try:
                model = SentenceTransformerModel(model_name)
                cls._instances[cache_key] = model
                return model
            except Exception as e:
                logger.error(f"Error creating sentence-transformer model: {str(e)}")
                return cls.get_fallback_model(cache_dir)
                
        elif model_type == 'spacy':
            if not SPACY_AVAILABLE:
                logger.warning("spaCy not available, falling back to alternative model")
                return cls.get_fallback_model(cache_dir)
            try:
                model = SpacyModel(model_name)
                cls._instances[cache_key] = model
                return model
            except Exception as e:
                logger.error(f"Error creating spaCy model: {str(e)}")
                return cls.get_fallback_model(cache_dir)
                
        elif model_type == 'tfidf':
            if not SKLEARN_AVAILABLE:
                logger.warning("scikit-learn not available, falling back to alternative model")
                return cls.get_fallback_model(cache_dir)
            try:
                max_features = int(model_name.split('-')[1]) if '-' in model_name else 5000
                model = TfidfModel(max_features)
                cls._instances[cache_key] = model
                return model
            except Exception as e:
                logger.error(f"Error creating TF-IDF model: {str(e)}")
                return cls.get_fallback_model(cache_dir)
                
        else:
            logger.warning(f"Unknown model type '{model_type}', falling back to alternative model")
            return cls.get_fallback_model(cache_dir)
    
    @classmethod
    def get_fallback_model(cls, cache_dir: Optional[str] = None) -> EmbeddingModel:
        """
        Get a fallback embedding model when preferred model is not available.
        
        Args:
            cache_dir: Directory to cache models
            
        Returns:
            Fallback embedding model
        """
        # Try models in order of preference
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                model = SentenceTransformerModel()
                cls._instances['fallback_sentence_transformer'] = model
                return model
            except Exception:
                pass
                
        if SPACY_AVAILABLE:
            try:
                model = SpacyModel()
                cls._instances['fallback_spacy'] = model
                return model
            except Exception:
                pass
                
        if SKLEARN_AVAILABLE:
            try:
                model = TfidfModel()
                cls._instances['fallback_tfidf'] = model
                return model
            except Exception:
                pass
                
        # Last resort: in-memory TF-IDF-like model
        logger.warning("No embedding models available, using basic in-memory model")
        return BasicEmbeddingModel()

class BasicEmbeddingModel(EmbeddingModel):
    """
    Basic embedding model that works without dependencies.
    Uses a simple bag-of-words approach with TF-IDF-like weighting.
    """
    
    def __init__(self):
        """Initialize the basic embedding model."""
        self.word_to_idx = {}
        self.idf = {}
        self.dimension = 0
        self.fitted = False
    
    def _preprocess(self, text: str) -> List[str]:
        """
        Preprocess text into tokens.
        
        Args:
            text: Input text
            
        Returns:
            List of tokens
        """
        # Simple tokenization
        text = text.lower()
        # Remove punctuation
        for char in ".,!?;:()[]{}<>\"'`~@#$%^&*-+=|\\":
            text = text.replace(char, ' ')
        # Split into tokens and filter short ones
        tokens = [token for token in text.split() if len(token) > 2]
        return tokens
    
    def _fit(self, texts: List[str]):
        """
        Fit the model on texts.
        
        Args:
            texts: List of texts to fit on
        """
        # Build vocabulary
        token_docs = {}
        doc_count = len(texts)
        
        for i, text in enumerate(texts):
            tokens = self._preprocess(text)
            for token in set(tokens):  # Use set to count each token once per document
                if token not in token_docs:
                    token_docs[token] = 0
                token_docs[token] += 1
        
        # Build word-to-index mapping
        self.word_to_idx = {word: idx for idx, word in enumerate(sorted(token_docs.keys()))}
        self.dimension = len(self.word_to_idx)
        
        # Calculate IDF
        self.idf = {word: np.log(doc_count / (count + 1)) + 1 
                   for word, count in token_docs.items()}
        
        self.fitted = True
    
    def encode(self, texts: Union[str, List[str]], batch_size: int = 32) -> np.ndarray:
        """
        Encode text into embeddings.
        
        Args:
            texts: Text or list of texts to encode
            batch_size: Batch size for encoding (not used)
            
        Returns:
            Numpy array of embeddings
        """
        if isinstance(texts, str):
            texts = [texts]
            
        # Fit on first use
        if not self.fitted:
            self._fit(texts)
            
        # Encode texts
        embeddings = np.zeros((len(texts), self.dimension))
        
        for i, text in enumerate(texts):
            tokens = self._preprocess(text)
            
            # Count token frequencies
            token_counts = {}
            for token in tokens:
                if token in self.word_to_idx:
                    if token not in token_counts:
                        token_counts[token] = 0
                    token_counts[token] += 1
            
            # Calculate TF-IDF
            for token, count in token_counts.items():
                idx = self.word_to_idx.get(token)
                if idx is not None:
                    tf = count / len(tokens)
                    idf = self.idf.get(token, 1.0)
                    embeddings[i, idx] = tf * idf
            
            # Normalize
            norm = np.linalg.norm(embeddings[i])
            if norm > 0:
                embeddings[i] = embeddings[i] / norm
                
        return embeddings
    
    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.
        
        Returns:
            Dimension size
        """
        return self.dimension
    
    def get_model_name(self) -> str:
        """
        Get the model name.
        
        Returns:
            Model name
        """
        return "basic-bow-tfidf"

# Global embedding model instance
_global_embedding_model = None

def get_embedding_model(model_type: str = 'sentence-transformer', 
                      model_name: str = None,
                      cache_dir: Optional[str] = None) -> EmbeddingModel:
    """
    Get the global embedding model, initializing if necessary.
    
    Args:
        model_type: Type of model ('sentence-transformer', 'spacy', 'tfidf')
        model_name: Name of the model
        cache_dir: Directory to cache models
        
    Returns:
        Embedding model instance
    """
    global _global_embedding_model
    
    if _global_embedding_model is None:
        _global_embedding_model = EmbeddingModelFactory.get_model(
            model_type=model_type,
            model_name=model_name,
            cache_dir=cache_dir
        )
        
    return _global_embedding_model

def encode_text(texts: Union[str, List[str]], model: Optional[EmbeddingModel] = None, 
               batch_size: int = 32) -> np.ndarray:
    """
    Encode text using the embedding model.
    
    Args:
        texts: Text or list of texts to encode
        model: Embedding model to use (uses global model if None)
        batch_size: Batch size for encoding
        
    Returns:
        Numpy array of embeddings
    """
    if model is None:
        model = get_embedding_model()
        
    return model.encode(texts, batch_size=batch_size)

def encode_query(query: str, model: Optional[EmbeddingModel] = None) -> np.ndarray:
    """
    Encode a query using the embedding model.
    
    Args:
        query: Query text to encode
        model: Embedding model to use (uses global model if None)
        
    Returns:
        Query embedding
    """
    return encode_text(query, model=model)

def calculate_similarity(text1: str, text2: str, model: Optional[EmbeddingModel] = None) -> float:
    """
    Calculate semantic similarity between two texts.
    
    Args:
        text1: First text
        text2: Second text
        model: Embedding model to use (uses global model if None)
        
    Returns:
        Similarity score (0-1)
    """
    if not text1 or not text2:
        return 0.0
        
    if model is None:
        model = get_embedding_model()
        
    # Encode texts
    embeddings = model.encode([text1, text2])
    
    # Calculate similarity
    if len(embeddings) == 2:
        return model.similarity(embeddings[0], embeddings[1])
    else:
        return 0.0

def batch_encode_texts(texts: List[str], model: Optional[EmbeddingModel] = None, 
                      batch_size: int = 32, cache_dir: Optional[str] = None) -> np.ndarray:
    """
    Encode a large batch of texts with optional caching.
    
    Args:
        texts: List of texts to encode
        model: Embedding model to use (uses global model if None)
        batch_size: Batch size for encoding
        cache_dir: Directory to cache embeddings
        
    Returns:
        Numpy array of embeddings
    """
    if model is None:
        model = get_embedding_model()
        
    # If caching is enabled
    if cache_dir is not None:
        # Create cache directory if it doesn't exist
        os.makedirs(cache_dir, exist_ok=True)
        
        # Create cache file path
        model_name = model.get_model_name()
        cache_file = os.path.join(cache_dir, f"embeddings_{model_name}.pkl")
        
        # Check if cache exists
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cached_embeddings = pickle.load(f)
                logger.info(f"Loaded {len(cached_embeddings)} cached embeddings")
                return cached_embeddings
            except Exception as e:
                logger.warning(f"Error loading cached embeddings: {str(e)}")
    
    # Encode texts
    embeddings = model.encode(texts, batch_size=batch_size)
    
    # Cache embeddings if cache_dir is specified
    if cache_dir is not None:
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embeddings, f)
            logger.info(f"Cached {len(embeddings)} embeddings")
        except Exception as e:
            logger.warning(f"Error caching embeddings: {str(e)}")
    
    return embeddings

















nlp/entity_extraction.py - Advanced Entity Recognition


"""
Advanced entity extraction with multiple approaches:
1. Rule-based pattern matching with gazetteers
2. ML-based NER with fallbacks to simpler approaches
3. Domain-specific entity extraction rules
4. Confidence scoring and entity linking
"""
import re
import logging
import json
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import string
from collections import defaultdict

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    logging.warning("spaCy not installed. Advanced NER will be limited.")

try:
    from flair.data import Sentence
    from flair.models import SequenceTagger
    FLAIR_AVAILABLE = True
except ImportError:
    FLAIR_AVAILABLE = False
    logging.warning("flair not installed. Advanced NER will be limited.")

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("transformers not installed. Advanced NER will be limited.")

logger = logging.getLogger(__name__)

class Entity:
    """Class representing an extracted entity with metadata."""
    
    def __init__(self, text: str, entity_type: str, 
                 start_pos: int, end_pos: int,
                 confidence: float = 1.0,
                 source: str = "unknown",
                 metadata: Optional[Dict[str, Any]] = None):
        """
        Initialize an entity.
        
        Args:
            text: Entity text
            entity_type: Entity type (e.g., PERSON, ORG, etc.)
            start_pos: Start position in source text
            end_pos: End position in source text
            confidence: Confidence score (0-1)
            source: Source of the entity (which extraction method)
            metadata: Additional metadata
        """
        self.text = text
        self.entity_type = entity_type
        self.start_pos = start_pos
        self.end_pos = end_pos
        self.confidence = confidence
        self.source = source
        self.metadata = metadata or {}
        self.normalized_text = self._normalize_text()
        
    def _normalize_text(self) -> str:
        """Normalize entity text for comparison."""
        text = self.text.lower()
        # Remove punctuation
        text = "".join(c for c in text if c not in string.punctuation)
        return text.strip()
        
    def overlaps_with(self, other: 'Entity') -> bool:
        """Check if this entity overlaps with another."""
        return (
            (self.start_pos <= other.start_pos < self.end_pos) or
            (self.start_pos < other.end_pos <= self.end_pos) or
            (other.start_pos <= self.start_pos < other.end_pos) or
            (other.start_pos < self.end_pos <= other.end_pos)
        )
        
    def exact_match(self, other: 'Entity') -> bool:
        """Check if this entity exactly matches another."""
        return (
            self.start_pos == other.start_pos and
            self.end_pos == other.end_pos
        )
        
    def __str__(self) -> str:
        return f"{self.text} ({self.entity_type}, {self.confidence:.2f})"
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "text": self.text,
            "normalized_text": self.normalized_text,
            "entity_type": self.entity_type,
            "start_pos": self.start_pos,
            "end_pos": self.end_pos,
            "confidence": self.confidence,
            "source": self.source,
            "metadata": self.metadata
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Entity':
        """Create entity from dictionary."""
        entity = cls(
            text=data["text"],
            entity_type=data["entity_type"],
            start_pos=data["start_pos"],
            end_pos=data["end_pos"],
            confidence=data["confidence"],
            source=data["source"],
            metadata=data["metadata"]
        )
        if "normalized_text" in data:
            entity.normalized_text = data["normalized_text"]
        return entity

class EntityExtractor:
    """Advanced entity extractor with multiple extraction methods."""
    
    def __init__(self, use_spacy: bool = True, use_flair: bool = False, 
                 use_transformers: bool = False, gazetteer_dir: Optional[str] = None,
                 custom_rules: Optional[Dict[str, List[str]]] = None,
                 confidence_threshold: float = 0.5):
        """
        Initialize the entity extractor with desired extraction methods.
        
        Args:
            use_spacy: Whether to use spaCy NER
            use_flair: Whether to use Flair NER
            use_transformers: Whether to use Transformers NER
            gazetteer_dir: Directory with gazetteer files
            custom_rules: Dictionary of custom rules for entity types
            confidence_threshold: Minimum confidence threshold for entities
        """
        self.confidence_threshold = confidence_threshold
        self.models = {}
        
        # Initialize spaCy model if requested and available
        if use_spacy and SPACY_AVAILABLE:
            try:
                self.models["spacy"] = spacy.load("en_core_web_md")
                logger.info("Loaded spaCy NER model")
            except Exception as e:
                logger.error(f"Error loading spaCy model: {str(e)}")
                try:
                    # Fall back to sm model
                    self.models["spacy"] = spacy.load("en_core_web_sm")
                    logger.info("Loaded spaCy small NER model as fallback")
                except Exception as e:
                    logger.error(f"Error loading spaCy sm model: {str(e)}")
                    
        # Initialize Flair model if requested and available
        if use_flair and FLAIR_AVAILABLE:
            try:
                self.models["flair"] = SequenceTagger.load("ner")
                logger.info("Loaded Flair NER model")
            except Exception as e:
                logger.error(f"Error loading Flair model: {str(e)}")
                
        # Initialize Transformers model if requested and available
        if use_transformers and TRANSFORMERS_AVAILABLE:
            try:
                self.models["transformers"] = pipeline(
                    "ner", 
                    model="dslim/bert-base-NER", 
                    aggregation_strategy="simple"
                )
                logger.info("Loaded Transformers NER model")
            except Exception as e:
                logger.error(f"Error loading Transformers model: {str(e)}")
                
        # Initialize NLTK for basic NER fallback
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        try:
            nltk.data.find('taggers/averaged_perceptron_tagger')
        except LookupError:
            nltk.download('averaged_perceptron_tagger')
        try:
            nltk.data.find('chunkers/maxent_ne_chunker')
        except LookupError:
            nltk.download('maxent_ne_chunker')
        try:
            nltk.data.find('corpora/words')
        except LookupError:
            nltk.download('words')
            
        # Load gazetteers if provided
        self.gazetteers = self._load_gazetteers(gazetteer_dir)
        
        # Set up custom rules
        self.custom_rules = custom_rules or {}
        
        # Set up regex patterns for rule-based extraction
        self.patterns = self._setup_regex_patterns()
        
    def _load_gazetteers(self, gazetteer_dir: Optional[str]) -> Dict[str, Set[str]]:
        """
        Load gazetteer files from directory.
        
        Gazetteers are text files with entity names, one per line.
        Filename determines entity type (e.g., organizations.txt).
        """
        gazetteers = {}
        
        if not gazetteer_dir:
            return gazetteers
            
        try:
            gazetteer_path = Path(gazetteer_dir)
            if not gazetteer_path.exists():
                logger.warning(f"Gazetteer directory not found: {gazetteer_dir}")
                return gazetteers
                
            for file_path in gazetteer_path.glob("*.txt"):
                entity_type = file_path.stem.upper()
                entities = set()
                
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        entity = line.strip()
                        if entity and not entity.startswith("#"):
                            entities.add(entity.lower())
                            
                gazetteers[entity_type] = entities
                logger.info(f"Loaded {len(entities)} entities for {entity_type} from {file_path.name}")
                
        except Exception as e:
            logger.error(f"Error loading gazetteers: {str(e)}")
            
        return gazetteers
        
    def _setup_regex_patterns(self) -> Dict[str, List[re.Pattern]]:
        """Set up regex patterns for rule-based entity extraction."""
        patterns = {}
        
        # Email addresses
        patterns["EMAIL"] = [
            re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        ]
        
        # URLs
        patterns["URL"] = [
            re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w\.-]*\??[\w=&\-%]*')
        ]
        
        # Phone numbers (various formats)
        patterns["PHONE"] = [
            re.compile(r'\b(?:\+\d{1,2}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b'),
            re.compile(r'\b\d{5}[\s.-]?\d{5}\b')
        ]
        
        # IP addresses
        patterns["IP_ADDRESS"] = [
            re.compile(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b')
        ]
        
        # Dates (various formats)
        patterns["DATE"] = [
            re.compile(r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b'),
            re.compile(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b'),
            re.compile(r'\b\d{4}-\d{2}-\d{2}\b')  # ISO format
        ]
        
        # Credit card numbers (masked for safety)
        patterns["CREDIT_CARD"] = [
            re.compile(r'\b(?:\d{4}[- ]){3}\d{4}\b'),
            re.compile(r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b')
        ]
        
        # Social security numbers (masked for safety)
        patterns["SSN"] = [
            re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
        ]
        
        # Version numbers
        patterns["VERSION"] = [
            re.compile(r'\bv\d+\.\d+(?:\.\d+)?(?:-[a-zA-Z0-9]+)?\b'),
            re.compile(r'\b\d+\.\d+(?:\.\d+)?(?:-[a-zA-Z0-9]+)?\b')
        ]
        
        # Product codes
        patterns["PRODUCT_CODE"] = [
            re.compile(r'\b[A-Z]{2,}-\d{3,}\b'),
            re.compile(r'\b[A-Z]{2,}\d{3,}\b')
        ]
        
        # Add custom patterns from rules
        for entity_type, rules in self.custom_rules.items():
            if entity_type not in patterns:
                patterns[entity_type] = []
                
            for rule in rules:
                try:
                    pattern = re.compile(rule)
                    patterns[entity_type].append(pattern)
                except re.error:
                    logger.error(f"Invalid regex pattern for {entity_type}: {rule}")
                    
        return patterns
        
    def extract_entities(self, text: str) -> List[Entity]:
        """
        Extract entities from text using multiple methods.
        
        Args:
            text: Input text
            
        Returns:
            List of Entity objects
        """
        if not text:
            return []
            
        # Extract entities using available methods
        all_entities = []
        
        # 1. ML-based methods
        spacy_entities = self._extract_spacy(text) if "spacy" in self.models else []
        flair_entities = self._extract_flair(text) if "flair" in self.models else []
        transformer_entities = self._extract_transformers(text) if "transformers" in self.models else []
        
        # 2. Rule-based methods
        regex_entities = self._extract_regex(text)
        gazetteer_entities = self._extract_gazetteer(text)
        
        # 3. Basic NLTK fallback if no other methods available
        nltk_entities = self._extract_nltk(text) if not (spacy_entities or flair_entities or transformer_entities) else []
        
        # Combine all entities
        all_entities = (
            spacy_entities + 
            flair_entities + 
            transformer_entities + 
            regex_entities + 
            gazetteer_entities + 
            nltk_entities
        )
        
        # Resolve overlapping entities
        resolved_entities = self._resolve_overlapping_entities(all_entities)
        
        # Filter low-confidence entities
        final_entities = [
            entity for entity in resolved_entities 
            if entity.confidence >= self.confidence_threshold
        ]
        
        return final_entities
        
    def _extract_spacy(self, text: str) -> List[Entity]:
        """Extract entities using spaCy."""
        entities = []
        
        if "spacy" not in self.models:
            return entities
            
        try:
            doc = self.models["spacy"](text)
            
            for ent in doc.ents:
                # Map spaCy entity labels to standard types
                entity_type = self._map_spacy_entity_type(ent.label_)
                
                entities.append(Entity(
                    text=ent.text,
                    entity_type=entity_type,
                    start_pos=ent.start_char,
                    end_pos=ent.end_char,
                    confidence=0.85,  # spaCy doesn't provide confidence, use fixed value
                    source="spacy"
                ))
                
        except Exception as e:
            logger.error(f"Error extracting entities with spaCy: {str(e)}")
            
        return entities
        
    def _map_spacy_entity_type(self, spacy_type: str) -> str:
        """Map spaCy entity types to standard types."""
        mapping = {
            "PERSON": "PERSON",
            "ORG": "ORGANIZATION",
            "GPE": "LOCATION",
            "LOC": "LOCATION",
            "PRODUCT": "PRODUCT",
            "DATE": "DATE",
            "TIME": "TIME",
            "MONEY": "MONEY",
            "PERCENT": "PERCENT",
            "CARDINAL": "NUMBER",
            "ORDINAL": "NUMBER",
            "QUANTITY": "QUANTITY",
            "NORP": "GROUP",
            "FAC": "LOCATION",
            "WORK_OF_ART": "WORK_OF_ART",
            "LAW": "LAW",
            "LANGUAGE": "LANGUAGE",
            "EVENT": "EVENT"
        }
        
        return mapping.get(spacy_type, spacy_type)
        
    def _extract_flair(self, text: str) -> List[Entity]:
        """Extract entities using Flair."""
        entities = []
        
        if "flair" not in self.models:
            return entities
            
        try:
            sentence = Sentence(text)
            self.models["flair"].predict(sentence)
            
            for entity in sentence.get_spans('ner'):
                entity_type = entity.tag
                
                entities.append(Entity(
                    text=entity.text,
                    entity_type=entity_type,
                    start_pos=entity.start_position,
                    end_pos=entity.end_position,
                    confidence=entity.score,
                    source="flair"
                ))
                
        except Exception as e:
            logger.error(f"Error extracting entities with Flair: {str(e)}")
            
        return entities
        
    def _extract_transformers(self, text: str) -> List[Entity]:
        """Extract entities using Transformers."""
        entities = []
        
        if "transformers" not in self.models:
            return entities
            
        try:
            # Transformers NER pipeline returns a list of dicts
            results = self.models["transformers"](text)
            
            for entity in results:
                entity_type = entity["entity_group"]
                
                entities.append(Entity(
                    text=entity["word"],
                    entity_type=entity_type,
                    start_pos=entity["start"],
                    end_pos=entity["end"],
                    confidence=entity["score"],
                    source="transformers"
                ))
                
        except Exception as e:
            logger.error(f"Error extracting entities with Transformers: {str(e)}")
            
        return entities
        
    def _extract_regex(self, text: str) -> List[Entity]:
        """Extract entities using regex patterns."""
        entities = []
        
        for entity_type, patterns in self.patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    start_pos = match.start()
                    end_pos = match.end()
                    matched_text = match.group(0)
                    
                    entities.append(Entity(
                        text=matched_text,
                        entity_type=entity_type,
                        start_pos=start_pos,
                        end_pos=end_pos,
                        confidence=0.9,  # High confidence for regex matches
                        source="regex"
                    ))
                    
        return entities
        
    def _extract_gazetteer(self, text: str) -> List[Entity]:
        """Extract entities using gazetteer lists."""
        entities = []
        
        if not self.gazetteers:
            return entities
            
        # Tokenize text into sentences for more accurate position tracking
        sentences = sent_tokenize(text)
        
        current_pos = 0
        for sentence in sentences:
            # For each entity type in gazetteers
            for entity_type, entity_list in self.gazetteers.items():
                # Check for each entity in the current sentence
                for entity in entity_list:
                    # Convert to lowercase for case-insensitive matching
                    sentence_lower = sentence.lower()
                    entity_lower = entity.lower()
                    
                    # Use regex word boundary to match whole words only
                    pattern = r'\b' + re.escape(entity_lower) + r'\b'
                    for match in re.finditer(pattern, sentence_lower):
                        start_pos = current_pos + match.start()
                        end_pos = current_pos + match.end()
                        
                        # Get the actual text from the original case
                        matched_text = text[start_pos:end_pos]
                        
                        entities.append(Entity(
                            text=matched_text,
                            entity_type=entity_type,
                            start_pos=start_pos,
                            end_pos=end_pos,
                            confidence=0.85,  # Good confidence for gazetteer matches
                            source="gazetteer"
                        ))
                        
            # Update position counter
            current_pos += len(sentence)
            
        return entities
        
    def _extract_nltk(self, text: str) -> List[Entity]:
        """Extract entities using NLTK as fallback."""
        entities = []
        
        try:
            sentences = nltk.sent_tokenize(text)
            
            current_pos = 0
            for sentence in sentences:
                tokens = nltk.word_tokenize(sentence)
                pos_tags = nltk.pos_tag(tokens)
                chunks = nltk.ne_chunk(pos_tags)
                
                # Track token positions
                token_positions = []
                pos = 0
                for token in tokens:
                    start = sentence.find(token, pos)
                    if start >= 0:
                        end = start + len(token)
                        token_positions.append((start, end))
                        pos = end
                    else:
                        # Fallback: just use the last end position
                        if token_positions:
                            last_end = token_positions[-1][1]
                            token_positions.append((last_end, last_end + len(token)))
                        else:
                            token_positions.append((0, len(token)))
                
                # Extract named entities
                entity_tokens = []
                current_entity_type = None
                
                for i, chunk in enumerate(chunks):
                    if hasattr(chunk, 'label'):
                        # Start of a named entity
                        if not entity_tokens or chunk.label() != current_entity_type:
                            # If we have a previous entity, add it
                            if entity_tokens:
                                self._add_nltk_entity(entity_tokens, current_entity_type, token_positions, current_pos, entities)
                                entity_tokens = []
                                
                            current_entity_type = chunk.label()
                            
                        # Add all tokens in this chunk
                        for token in chunk:
                            token_idx = token[1]
                            entity_tokens.append(token_idx)
                    elif entity_tokens:
                        # End of a named entity
                        self._add_nltk_entity(entity_tokens, current_entity_type, token_positions, current_pos, entities)
                        entity_tokens = []
                        current_entity_type = None
                        
                # Add any remaining entity
                if entity_tokens:
                    self._add_nltk_entity(entity_tokens, current_entity_type, token_positions, current_pos, entities)
                    
                # Update position counter
                current_pos += len(sentence)
                
        except Exception as e:
            logger.error(f"Error extracting entities with NLTK: {str(e)}")
            
        return entities
        
    def _add_nltk_entity(self, token_indices, entity_type, token_positions, current_pos, entities):
        """Helper to add an NLTK entity to the results."""
        if not token_indices or not entity_type:
            return
            
        try:
            # Calculate start and end positions
            start_idx = token_indices[0]
            end_idx = token_indices[-1]
            
            if start_idx < len(token_positions) and end_idx < len(token_positions):
                start_pos = token_positions[start_idx][0] + current_pos
                end_pos = token_positions[end_idx][1] + current_pos
                
                entity_text = ""
                for idx in token_indices:
                    if idx < len(token_positions):
                        start = token_positions[idx][0] + current_pos
                        end = token_positions[idx][1] + current_pos
                        entity_text += " " + self.text[start:end]
                
                entity_text = entity_text.strip()
                
                entities.append(Entity(
                    text=entity_text,
                    entity_type=entity_type,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    confidence=0.7,  # Lower confidence for NLTK fallback
                    source="nltk"
                ))
        except Exception as e:
            logger.error(f"Error adding NLTK entity: {str(e)}")
        
    def _resolve_overlapping_entities(self, entities: List[Entity]) -> List[Entity]:
        """
        Resolve overlapping entities by keeping the highest confidence ones.
        
        Args:
            entities: List of potentially overlapping entities
            
        Returns:
            List of non-overlapping entities
        """
        if not entities:
            return []
            
        # Sort by confidence (highest first)
        sorted_entities = sorted(entities, key=lambda e: e.confidence, reverse=True)
        
        # Track which spans are already covered
        resolved_entities = []
        covered_spans = set()
        
        for entity in sorted_entities:
            # Create a set of positions covered by this entity
            entity_span = set(range(entity.start_pos, entity.end_pos))
            
            # Check if this entity overlaps with any already covered span
            if not entity_span.intersection(covered_spans):
                # No overlap, add this entity
                resolved_entities.append(entity)
                covered_spans.update(entity_span)
            # else:
            #     # This is an alternative approach that merges overlapping entities
            #     # Instead of adding the whole entity, we could be more granular and
            #     # only add the positions that aren't already covered
            #     uncovered = entity_span - covered_spans
            #     if uncovered:
            #         # Create a new entity with just the uncovered span
            #         # This requires adjusting the text and positions
            #         ...
                
        return resolved_entities
        
    def extract_relationships(self, text: str, entities: List[Entity]) -> List[Dict[str, Any]]:
        """
        Extract relationships between entities in the text.
        
        Args:
            text: Input text
            entities: List of entities
            
        Returns:
            List of relationship dictionaries
        """
        if not text or not entities:
            return []
            
        relationships = []
        
        # Sort entities by position
        sorted_entities = sorted(entities, key=lambda e: e.start_pos)
        
        # Look for relationships in the same sentence
        sentences = sent_tokenize(text)
        
        current_pos = 0
        for sentence in sentences:
            # Find entities in this sentence
            sentence_entities = []
            for entity in sorted_entities:
                if (entity.start_pos >= current_pos and 
                    entity.end_pos <= current_pos + len(sentence)):
                    # Adjust positions to be relative to the sentence
                    entity_copy = Entity(
                        text=entity.text,
                        entity_type=entity.entity_type,
                        start_pos=entity.start_pos - current_pos,
                        end_pos=entity.end_pos - current_pos,
                        confidence=entity.confidence,
                        source=entity.source,
                        metadata=entity.metadata.copy()
                    )
                    sentence_entities.append(entity_copy)
                    
            # Extract relationships between entities in this sentence
            sentence_relationships = self._extract_sentence_relationships(sentence, sentence_entities)
            
            # Adjust relationship positions to be absolute
            for rel in sentence_relationships:
                rel["subject_start"] += current_pos
                rel["subject_end"] += current_pos
                rel["object_start"] += current_pos
                rel["object_end"] += current_pos
                
            relationships.extend(sentence_relationships)
            
            # Update position counter
            current_pos += len(sentence)
            
        return relationships
        
    def _extract_sentence_relationships(self, sentence: str, entities: List[Entity]) -> List[Dict[str, Any]]:
        """Extract relationships between entities in a sentence."""
        if len(entities) < 2:
            return []
            
        relationships = []
        
        # For each pair of entities
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities):
                if i == j:
                    continue
                    
                # Get the text between the entities
                if entity1.end_pos < entity2.start_pos:
                    # entity1 comes before entity2
                    between_text = sentence[entity1.end_pos:entity2.start_pos]
                    
                    # Look for relationship indicators
                    relationship = self._identify_relationship(
                        between_text, entity1, entity2
                    )
                    
                    if relationship:
                        relationships.append({
                            "subject": entity1.text,
                            "subject_type": entity1.entity_type,
                            "subject_start": entity1.start_pos,
                            "subject_end": entity1.end_pos,
                            "predicate": relationship["predicate"],
                            "object": entity2.text,
                            "object_type": entity2.entity_type,
                            "object_start": entity2.start_pos,
                            "object_end": entity2.end_pos,
                            "confidence": relationship["confidence"]
                        })
                        
        return relationships
        
    def _identify_relationship(self, between_text: str, entity1: Entity, entity2: Entity) -> Optional[Dict[str, Any]]:
        """Identify a relationship between two entities based on the text between them."""
        if not between_text.strip():
            return None
            
        # Potential relationships based on entity types
        entity_pair = (entity1.entity_type, entity2.entity_type)
        
        # Only process short connecting text (likely to contain relationship indicators)
        if len(between_text.split()) > 8:
            return None
            
        # Simple verb-based relationship extraction
        if "spacy" in self.models:
            try:
                doc = self.models["spacy"](between_text)
                
                # Look for verbs
                verbs = []
                for token in doc:
                    if token.pos_ == "VERB":
                        verbs.append(token.lemma_)
                        
                if verbs:
                    return {
                        "predicate": " ".join(verbs),
                        "confidence": 0.7
                    }
            except:
                pass
                
        # Fall back to simple text pattern matching
        # Common relationship patterns for different entity pairs
        patterns = {
            ("PERSON", "ORGANIZATION"): [
                (r'works for', 'works for', 0.8),
                (r'employed by', 'employed by', 0.8),
                (r'founded', 'founded', 0.85),
                (r'leads', 'leads', 0.8),
                (r'owns', 'owns', 0.8)
            ],
            ("PERSON", "LOCATION"): [
                (r'lives in', 'lives in', 0.75),
                (r'from', 'from', 0.7),
                (r'visited', 'visited', 0.75),
                (r'traveled to', 'traveled to', 0.8)
            ],
            ("ORGANIZATION", "LOCATION"): [
                (r'located in', 'located in', 0.8),
                (r'based in', 'based in', 0.85),
                (r'headquartered in', 'headquartered in', 0.9),
                (r'operates in', 'operates in', 0.8)
            ],
            ("ORGANIZATION", "ORGANIZATION"): [
                (r'acquired', 'acquired', 0.85),
                (r'merged with', 'merged with', 0.9),
                (r'partnered with', 'partnered with', 0.85),
                (r'competes with', 'competes with', 0.8)
            ]
        }
        
        # Check for relationship patterns
        relationship_patterns = patterns.get(entity_pair, [])
        
        for pattern, predicate, confidence in relationship_patterns:
            if re.search(pattern, between_text.lower()):
                return {
                    "predicate": predicate,
                    "confidence": confidence
                }
                
        # Generic relationship extraction for unknown entity pairs
        # Just look for any verb/preposition with surrounding words
        if len(between_text.split()) <= 5:
            return {
                "predicate": between_text.strip(),
                "confidence": 0.6
            }
            
        return None
                
    def get_entity_metadata(self, entity_text: str, entity_type: str) -> Dict[str, Any]:
        """
        Get additional metadata for an entity (e.g., from a knowledge base).
        
        Args:
            entity_text: Entity text
            entity_type: Entity type
            
        Returns:
            Dictionary with metadata
        """
        # This is a placeholder - in a real implementation, this would
        # look up the entity in a knowledge base or database
        metadata = {}
        
        # Some example metadata for common entity types
        if entity_type == "PERSON":
            # For a person, we might include information like:
            metadata["gender"] = "unknown"
            metadata["title"] = None
            
        elif entity_type == "ORGANIZATION":
            # For an organization:
            metadata["industry"] = "unknown"
            metadata["size"] = "unknown"
            
        elif entity_type == "LOCATION":
            # For a location:
            metadata["country"] = "unknown"
            metadata["region"] = "unknown"
            
        elif entity_type == "DATE":
            # For a date, try to parse it
            try:
                import dateutil.parser
                parsed_date = dateutil.parser.parse(entity_text)
                metadata["year"] = parsed_date.year
                metadata["month"] = parsed_date.month
                metadata["day"] = parsed_date.day
                metadata["iso_format"] = parsed_date.strftime("%Y-%m-%d")
            except:
                pass
                
        return metadata

def extract_entities_from_text(text: str, use_ml: bool = True) -> List[Dict[str, Any]]:
    """
    Convenience function to extract entities from text.
    
    Args:
        text: Input text
        use_ml: Whether to use ML models if available
        
    Returns:
        List of entity dictionaries
    """
    extractor = EntityExtractor(
        use_spacy=use_ml and SPACY_AVAILABLE,
        use_flair=use_ml and FLAIR_AVAILABLE,
        use_transformers=use_ml and TRANSFORMERS_AVAILABLE
    )
    
    entities = extractor.extract_entities(text)
    return [entity.to_dict() for entity in entities]















nlp/summarization.py - Advanced Text Summarization

"""
Advanced text summarization module with various approaches:
1. Extractive summarization with TextRank and LexRank
2. Graph-based summarization with semantic similarity
3. Query-focused summarization
4. Multi-document summarization
5. Domain-specific summarization strategies

All algorithms use purely classical NLP techniques to avoid hallucination.
"""
import re
import logging
import math
import string
import random
from collections import defaultdict, Counter
from typing import Dict, Any, List, Optional, Tuple, Union, Set

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logging.warning("NetworkX not installed. Advanced graph-based summarization will be limited.")
    
try:
    from sklearn.cluster import KMeans
    KMEANS_AVAILABLE = True
except ImportError:
    KMEANS_AVAILABLE = False
    logging.warning("sklearn KMeans not installed. Clustering-based summarization will be limited.")

try:
    from sumy.parsers.plaintext import PlaintextParser
    from sumy.nlp.tokenizers import Tokenizer
    from sumy.summarizers.lex_rank import LexRankSummarizer
    from sumy.summarizers.text_rank import TextRankSummarizer
    from sumy.summarizers.lsa import LsaSummarizer
    from sumy.nlp.stemmers import Stemmer
    from sumy.utils import get_stop_words
    SUMY_AVAILABLE = True
except ImportError:
    SUMY_AVAILABLE = False
    logging.warning("sumy not installed. Alternative summarization algorithms will be used.")

# Download necessary NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

logger = logging.getLogger(__name__)

class SummarizationConfig:
    """Configuration for summarization algorithms."""
    
    def __init__(self, 
                 ratio: float = 0.2,
                 max_sentences: int = 10,
                 min_length: int = 40,
                 language: str = "english",
                 use_query_weighting: bool = True,
                 cluster_sentences: bool = False,
                 num_clusters: Optional[int] = None,
                 diversity_factor: float = 0.5,
                 importance_factor: float = 0.5,
                 domain_keywords: Optional[List[str]] = None):
        """
        Initialize summarization configuration.
        
        Args:
            ratio: Percentage of original text to retain (0.0-1.0)
            max_sentences: Maximum number of sentences in summary
            min_length: Minimum length of sentences to include (in characters)
            language: Language code for stopwords and tokenization
            use_query_weighting: Whether to use query-based weighting
            cluster_sentences: Whether to cluster sentences for diversity
            num_clusters: Number of clusters for sentence clustering
            diversity_factor: Factor for diversity (0.0-1.0)
            importance_factor: Factor for sentence importance (0.0-1.0)
            domain_keywords: List of domain-specific keywords to weight higher
        """
        self.ratio = ratio
        self.max_sentences = max_sentences
        self.min_length = min_length
        self.language = language
        self.use_query_weighting = use_query_weighting
        self.cluster_sentences = cluster_sentences and KMEANS_AVAILABLE
        self.num_clusters = num_clusters
        self.diversity_factor = diversity_factor
        self.importance_factor = importance_factor
        self.domain_keywords = domain_keywords or []
        
        # Validate parameters
        if not 0.0 <= ratio <= 1.0:
            logger.warning(f"Invalid ratio {ratio}. Using default 0.2")
            self.ratio = 0.2
            
        if self.diversity_factor + self.importance_factor != 1.0:
            # Normalize factors
            total = self.diversity_factor + self.importance_factor
            self.diversity_factor /= total
            self.importance_factor /= total
            
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary."""
        return {
            "ratio": self.ratio,
            "max_sentences": self.max_sentences,
            "min_length": self.min_length,
            "language": self.language,
            "use_query_weighting": self.use_query_weighting,
            "cluster_sentences": self.cluster_sentences,
            "num_clusters": self.num_clusters,
            "diversity_factor": self.diversity_factor,
            "importance_factor": self.importance_factor,
            "domain_keywords": self.domain_keywords
        }

class SentenceInfo:
    """Information about a sentence for summarization."""
    
    def __init__(self, 
                 sentence: str,
                 position: int,
                 original_index: int,
                 score: float = 0.0,
                 cluster: Optional[int] = None,
                 source_document: Optional[str] = None):
        """
        Initialize sentence information.
        
        Args:
            sentence: The sentence text
            position: Position in the document (0 = start)
            original_index: Original index in the document
            score: Importance score
            cluster: Cluster ID if clustering is used
            source_document: Source document identifier for multi-document summarization
        """
        self.sentence = sentence
        self.position = position
        self.original_index = original_index
        self.score = score
        self.cluster = cluster
        self.source_document = source_document
        self.tokens = []
        self.vector = None
        
    def __repr__(self) -> str:
        return f"SentenceInfo(position={self.position}, score={self.score:.4f}, cluster={self.cluster})"

class TextRankSummarizer:
    """
    Implementation of TextRank algorithm for extractive summarization.
    """
    
    def __init__(self, config: Optional[SummarizationConfig] = None):
        """
        Initialize TextRank summarizer.
        
        Args:
            config: Summarization configuration
        """
        self.config = config or SummarizationConfig()
        self.stop_words = set(stopwords.words(self.config.language))
    
    def _preprocess_text(self, text: str) -> List[SentenceInfo]:
        """
        Preprocess text for summarization.
        
        Args:
            text: Input text
            
        Returns:
            List of SentenceInfo objects
        """
        # Split text into sentences
        sentences = sent_tokenize(text, language=self.config.language)
        
        # Create SentenceInfo objects
        sentence_info = []
        for i, sentence in enumerate(sentences):
            # Skip very short sentences
            if len(sentence) < self.config.min_length:
                continue
                
            info = SentenceInfo(
                sentence=sentence,
                position=i,
                original_index=i
            )
            
            # Tokenize and remove stopwords
            tokens = word_tokenize(sentence.lower(), language=self.config.language)
            info.tokens = [token for token in tokens if token not in self.stop_words and token not in string.punctuation]
            
            sentence_info.append(info)
            
        return sentence_info
    
    def _build_similarity_matrix(self, sentences: List[SentenceInfo]) -> np.ndarray:
        """
        Build similarity matrix for sentences.
        
        Args:
            sentences: List of SentenceInfo objects
            
        Returns:
            Similarity matrix as numpy array
        """
        # Create TF-IDF vectors for all sentences
        docs = [" ".join(s.tokens) for s in sentences]
        
        if not docs:
            return np.array([])
            
        # Create TF-IDF vectorizer
        tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)
        
        # Handle the case with a single sentence
        if len(docs) == 1:
            return np.array([[1.0]])
            
        try:
            # Create TF-IDF matrix
            tfidf_matrix = tfidf_vectorizer.fit_transform(docs)
            
            # Compute cosine similarity
            similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
            
            # Save vectors for later use
            for i, sentence in enumerate(sentences):
                sentence.vector = tfidf_matrix[i]
                
            return similarity_matrix
        except:
            # Fallback for very short texts or other errors
            logger.warning("Error computing TF-IDF matrix, using fallback approach")
            n = len(sentences)
            similarity_matrix = np.zeros((n, n))
            
            # Simple word overlap similarity
            for i in range(n):
                for j in range(n):
                    if i == j:
                        similarity_matrix[i][j] = 1.0
                    else:
                        tokens_i = set(sentences[i].tokens)
                        tokens_j = set(sentences[j].tokens)
                        
                        if not tokens_i or not tokens_j:
                            similarity_matrix[i][j] = 0.0
                        else:
                            overlap = len(tokens_i.intersection(tokens_j))
                            similarity_matrix[i][j] = overlap / (len(tokens_i) + len(tokens_j) - overlap)
                            
            return similarity_matrix
    
    def _apply_text_rank(self, sentences: List[SentenceInfo], similarity_matrix: np.ndarray) -> None:
        """
        Apply TextRank algorithm to score sentences.
        
        Args:
            sentences: List of SentenceInfo objects
            similarity_matrix: Similarity matrix
        """
        if not NETWORKX_AVAILABLE:
            # Fallback approach without NetworkX
            self._apply_text_rank_fallback(sentences, similarity_matrix)
            return
            
        try:
            # Create a graph from the similarity matrix
            graph = nx.from_numpy_array(similarity_matrix)
            
            # Apply PageRank
            scores = nx.pagerank(graph)
            
            # Assign scores to sentences
            for i, sentence in enumerate(sentences):
                sentence.score = scores[i]
        except:
            # Fallback if NetworkX fails
            logger.warning("NetworkX PageRank failed, using fallback approach")
            self._apply_text_rank_fallback(sentences, similarity_matrix)
    
    def _apply_text_rank_fallback(self, sentences: List[SentenceInfo], similarity_matrix: np.ndarray) -> None:
        """Fallback TextRank implementation without NetworkX."""
        n = len(sentences)
        if n == 0:
            return
            
        # Initialize scores
        scores = np.ones(n) / n
        
        # Parameters for convergence
        damping = 0.85
        epsilon = 1e-4
        max_iter = 100
        
        # Power iteration
        for _ in range(max_iter):
            prev_scores = scores.copy()
            
            for i in range(n):
                summation = 0
                for j in range(n):
                    if i != j and similarity_matrix[j, i] > 0:
                        outgoing_sum = sum(similarity_matrix[j, k] for k in range(n) if j != k)
                        if outgoing_sum > 0:
                            summation += similarity_matrix[j, i] / outgoing_sum * prev_scores[j]
                            
                scores[i] = (1 - damping) + damping * summation
                
            # Check convergence
            if np.sqrt(np.sum((scores - prev_scores) ** 2)) < epsilon:
                break
                
        # Normalize scores
        if np.sum(scores) > 0:
            scores = scores / np.sum(scores)
            
        # Assign scores to sentences
        for i, sentence in enumerate(sentences):
            sentence.score = scores[i]
    
    def _apply_position_boost(self, sentences: List[SentenceInfo]) -> None:
        """
        Apply position boost to sentence scores.
        
        Args:
            sentences: List of SentenceInfo objects
        """
        n = len(sentences)
        if n <= 1:
            return
            
        # Higher boost for sentences at the beginning and end
        for sentence in sentences:
            pos = sentence.position
            
            # Position boost formula (decreasing from start and end)
            if pos < n / 2:
                # Front half gets higher boost
                boost = 1.0 - (pos / n) * 0.8
            else:
                # Back half gets lower boost
                boost = 0.3 * (pos / n)
                
            # Apply boost as a small adjustment to scores
            sentence.score = 0.9 * sentence.score + 0.1 * boost
    
    def _apply_domain_boost(self, sentences: List[SentenceInfo]) -> None:
        """
        Apply domain-specific keyword boost to sentence scores.
        
        Args:
            sentences: List of SentenceInfo objects
        """
        if not self.config.domain_keywords:
            return
            
        domain_keywords = set(kw.lower() for kw in self.config.domain_keywords)
        
        for sentence in sentences:
            # Count matches with domain keywords
            matches = sum(1 for token in sentence.tokens if token.lower() in domain_keywords)
            
            if matches > 0:
                # Calculate boost based on keyword matches
                boost = min(0.3, 0.1 * matches)
                
                # Apply boost
                sentence.score = sentence.score * (1 + boost)
    
    def _apply_query_weighting(self, sentences: List[SentenceInfo], query: Optional[str]) -> None:
        """
        Apply query-based weighting to sentence scores.
        
        Args:
            sentences: List of SentenceInfo objects
            query: Optional query for focused summarization
        """
        if not query or not self.config.use_query_weighting:
            return
            
        # Tokenize and clean query
        query_tokens = word_tokenize(query.lower(), language=self.config.language)
        query_tokens = [token for token in query_tokens if token not in self.stop_words and token not in string.punctuation]
        
        if not query_tokens:
            return
            
        # Create a simple query TF-IDF vector
        query_counter = Counter(query_tokens)
        query_idf = {}
        
        # Calculate how many sentences contain each query token
        for token in query_tokens:
            doc_freq = sum(1 for s in sentences if token in s.tokens)
            if doc_freq > 0:
                # IDF formula
                query_idf[token] = math.log(len(sentences) / doc_freq)
            else:
                query_idf[token] = 0
        
        # Calculate similarity between each sentence and the query
        for sentence in sentences:
            similarity = 0
            for token in query_tokens:
                if token in sentence.tokens:
                    # TF-IDF score for this token
                    tf = sentence.tokens.count(token) / len(sentence.tokens)
                    similarity += tf * query_idf.get(token, 0)
            
            # Normalize by query length
            if query_tokens:
                similarity /= len(query_tokens)
                
            # Apply similarity as a boost to sentence score
            if similarity > 0:
                sentence.score = 0.7 * sentence.score + 0.3 * similarity
    
    def _cluster_sentences(self, sentences: List[SentenceInfo]) -> None:
        """
        Cluster sentences for diversity-focused summarization.
        
        Args:
            sentences: List of SentenceInfo objects
        """
        if not self.config.cluster_sentences or not KMEANS_AVAILABLE or len(sentences) <= 1:
            return
            
        try:
            # Ensure all sentences have vectors
            vectors = np.array([s.vector.toarray()[0] if hasattr(s.vector, 'toarray') else s.vector for s in sentences])
            
            # Determine number of clusters
            if self.config.num_clusters:
                k = min(self.config.num_clusters, len(sentences))
            else:
                # Auto-determine number of clusters
                k = min(max(2, int(len(sentences) * self.config.ratio)), len(sentences))
                
            # Apply KMeans clustering
            kmeans = KMeans(n_clusters=k, random_state=42)
            clusters = kmeans.fit_predict(vectors)
            
            # Assign clusters to sentences
            for i, sentence in enumerate(sentences):
                sentence.cluster = int(clusters[i])
        except Exception as e:
            logger.warning(f"Error clustering sentences: {str(e)}")
    
    def _select_sentences(self, sentences: List[SentenceInfo]) -> List[SentenceInfo]:
        """
        Select sentences for the summary.
        
        Args:
            sentences: List of scored SentenceInfo objects
            
        Returns:
            List of selected sentences
        """
        if not sentences:
            return []
            
        # Determine number of sentences to include
        n = len(sentences)
        selection_count = min(
            self.config.max_sentences,
            max(1, int(n * self.config.ratio))
        )
        
        if selection_count >= n:
            # If we're selecting all sentences, return them in original order
            return sorted(sentences, key=lambda s: s.original_index)
            
        # Apply cluster-based selection if enabled
        if self.config.cluster_sentences and any(s.cluster is not None for s in sentences):
            return self._select_sentences_with_clustering(sentences, selection_count)
        else:
            # Simple selection based on scores
            top_sentences = sorted(sentences, key=lambda s: s.score, reverse=True)
            selected = top_sentences[:selection_count]
            
            # Return in original document order
            return sorted(selected, key=lambda s: s.original_index)
    
    def _select_sentences_with_clustering(self, sentences: List[SentenceInfo], selection_count: int) -> List[SentenceInfo]:
        """Select sentences using cluster-based approach."""
        # Group sentences by cluster
        clusters = defaultdict(list)
        for sentence in sentences:
            clusters[sentence.cluster].append(sentence)
            
        # Sort clusters by size (descending)
        sorted_clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)
        
        selected = []
        remaining = selection_count
        
        # First round: select top sentence from each cluster
        for cluster_id, cluster_sentences in sorted_clusters:
            if remaining <= 0:
                break
                
            # Get highest scoring sentence from this cluster
            top_sentence = max(cluster_sentences, key=lambda s: s.score)
            selected.append(top_sentence)
            remaining -= 1
            
        # Second round: fill remaining slots by score
        if remaining > 0:
            # Get all unselected sentences
            unselected = [s for s in sentences if s not in selected]
            
            # Sort by score (descending)
            unselected.sort(key=lambda s: s.score, reverse=True)
            
            # Add top remaining sentences
            selected.extend(unselected[:remaining])
            
        # Return in original document order
        return sorted(selected, key=lambda s: s.original_index)
    
    def summarize(self, text: str, query: Optional[str] = None) -> str:
        """
        Generate a summary of the text.
        
        Args:
            text: Input text
            query: Optional query for focused summarization
            
        Returns:
            Generated summary text
        """
        # Edge cases
        if not text:
            return ""
            
        # If text is shorter than max_sentences, just return it
        sentence_count = len(sent_tokenize(text, language=self.config.language))
        if sentence_count <= self.config.max_sentences:
            return text
            
        # Preprocess text
        sentences = self._preprocess_text(text)
        
        if not sentences:
            return ""
            
        # Build similarity matrix
        similarity_matrix = self._build_similarity_matrix(sentences)
        
        # Apply TextRank algorithm
        self._apply_text_rank(sentences, similarity_matrix)
        
        # Apply position boost
        self._apply_position_boost(sentences)
        
        # Apply domain-specific boost
        self._apply_domain_boost(sentences)
        
        # Apply query weighting if provided
        if query:
            self._apply_query_weighting(sentences, query)
            
        # Cluster sentences if enabled
        self._cluster_sentences(sentences)
        
        # Select sentences for summary
        selected = self._select_sentences(sentences)
        
        # Build summary from selected sentences
        summary_sentences = [s.sentence for s in selected]
        
        return " ".join(summary_sentences)

class LexRankSummarizer:
    """
    Implementation of LexRank algorithm for extractive summarization.
    
    This is an alternative to TextRank that often performs well on factual content.
    """
    
    def __init__(self, config: Optional[SummarizationConfig] = None):
        """
        Initialize LexRank summarizer.
        
        Args:
            config: Summarization configuration
        """
        self.config = config or SummarizationConfig()
        self.stop_words = set(stopwords.words(self.config.language))
        
        # If sumy is available, use its LexRank implementation
        self.use_sumy = SUMY_AVAILABLE
        if self.use_sumy:
            self.sumy_summarizer = LexRankSummarizer(Stemmer(self.config.language))
            self.sumy_summarizer.stop_words = get_stop_words(self.config.language)
    
    def _compute_tf_idf_matrix(self, sentences: List[str]) -> Tuple[np.ndarray, List[List[str]]]:
        """
        Compute TF-IDF matrix for sentences.
        
        Args:
            sentences: List of sentences
            
        Returns:
            Tuple of (TF-IDF matrix, tokenized sentences)
        """
        # Tokenize sentences
        tokenized_sentences = []
        for sentence in sentences:
            tokens = word_tokenize(sentence.lower(), language=self.config.language)
            filtered_tokens = [token for token in tokens if token not in self.stop_words and token not in string.punctuation]
            tokenized_sentences.append(filtered_tokens)
            
        # Compute inverse document frequency (IDF)
        word_counts = Counter()
        for tokens in tokenized_sentences:
            word_counts.update(set(tokens))
            
        num_docs = len(sentences)
        idf = {}
        for word, count in word_counts.items():
            idf[word] = math.log(num_docs / count)
            
        # Compute TF-IDF matrix
        tfidf_matrix = np.zeros((num_docs, num_docs))
        
        for i, tokens_i in enumerate(tokenized_sentences):
            if not tokens_i:
                continue
                
            # Compute term frequencies
            tf_i = Counter(tokens_i)
            doc_len_i = len(tokens_i)
            
            # Normalize term frequencies by document length
            for word in tf_i:
                tf_i[word] /= doc_len_i
                
            # Compute TF-IDF vector
            tfidf_i = {word: tf * idf.get(word, 0) for word, tf in tf_i.items()}
            
            for j, tokens_j in enumerate(tokenized_sentences):
                if not tokens_j:
                    continue
                    
                # Compute cosine similarity
                overlap = set(tokens_i) & set(tokens_j)
                if not overlap:
                    continue
                    
                # Compute dot product
                dot_product = sum(tfidf_i.get(word, 0) * tf_i.get(word, 0) for word in overlap)
                
                # Compute magnitudes
                magnitude_i = math.sqrt(sum(tfidf_i.get(word, 0) ** 2 for word in tokens_i))
                magnitude_j = math.sqrt(sum(tfidf_i.get(word, 0) ** 2 for word in tokens_j))
                
                if magnitude_i * magnitude_j == 0:
                    continue
                    
                # Compute cosine similarity
                similarity = dot_product / (magnitude_i * magnitude_j)
                tfidf_matrix[i, j] = similarity
                
        return tfidf_matrix, tokenized_sentences
    
    def _power_method(self, matrix: np.ndarray, epsilon: float = 1e-4, max_iter: int = 100) -> np.ndarray:
        """
        Apply power method to compute stationary distribution.
        
        Args:
            matrix: Transition matrix
            epsilon: Convergence threshold
            max_iter: Maximum number of iterations
            
        Returns:
            Stationary distribution
        """
        n = matrix.shape[0]
        
        # Initialize uniform distribution
        p = np.ones(n) / n
        
        # Power method iterations
        for _ in range(max_iter):
            p_next = np.dot(matrix, p)
            
            # Check convergence
            delta = np.linalg.norm(p_next - p, 1)
            if delta < epsilon:
                break
                
            p = p_next
            
        return p
    
    def _compute_lexrank_scores(self, similarity_matrix: np.ndarray, threshold: float = 0.1, damping: float = 0.85) -> np.ndarray:
        """
        Compute LexRank scores from similarity matrix.
        
        Args:
            similarity_matrix: Sentence similarity matrix
            threshold: Similarity threshold for pruning edges
            damping: Damping factor for LexRank
            
        Returns:
            LexRank scores
        """
        n = similarity_matrix.shape[0]
        
        # Apply threshold to create binary matrix
        binary_matrix = (similarity_matrix > threshold).astype(float)
        
        # Row normalization to create Markov transition matrix
        row_sums = binary_matrix.sum(axis=1, keepdims=True)
        
        # Avoid division by zero
        row_sums[row_sums == 0] = 1
        
        transition_matrix = binary_matrix / row_sums
        
        # Apply damping factor
        transition_matrix = damping * transition_matrix + (1 - damping) / n
        
        # Compute stationary distribution
        scores = self._power_method(transition_matrix)
        
        return scores
    
    def summarize(self, text: str, query: Optional[str] = None) -> str:
        """
        Generate a summary of the text using LexRank.
        
        Args:
            text: Input text
            query: Optional query for focused summarization
            
        Returns:
            Generated summary text
        """
        # Edge cases
        if not text:
            return ""
            
        # If sumy is available, use its implementation
        if self.use_sumy:
            try:
                return self._summarize_with_sumy(text, query)
            except Exception as e:
                logger.warning(f"Sumy summarization failed: {str(e)}")
                # Fall back to custom implementation
        
        # Split text into sentences
        sentences = sent_tokenize(text, language=self.config.language)
        
        # Skip very short sentences
        filtered_sentences = [s for s in sentences if len(s) >= self.config.min_length]
        
        # Edge case: very few sentences remaining
        if len(filtered_sentences) <= self.config.max_sentences:
            return " ".join(filtered_sentences)
            
        # Compute similarity matrix
        similarity_matrix, tokenized_sentences = self._compute_tf_idf_matrix(filtered_sentences)
        
        # Compute LexRank scores
        scores = self._compute_lexrank_scores(similarity_matrix)
        
        # Sort sentences by score
        ranked_sentences = [(i, score) for i, score in enumerate(scores)]
        ranked_sentences.sort(key=lambda x: x[1], reverse=True)
        
        # Select top sentences
        num_sentences = min(self.config.max_sentences, max(1, int(len(filtered_sentences) * self.config.ratio)))
        top_indices = [i for i, _ in ranked_sentences[:num_sentences]]
        
        # Return sentences in original order
        top_indices.sort()
        summary_sentences = [filtered_sentences[i] for i in top_indices]
        
        return " ".join(summary_sentences)
    
    def _summarize_with_sumy(self, text: str, query: Optional[str] = None) -> str:
        """
        Generate a summary using sumy's LexRank implementation.
        
        Args:
            text: Input text
            query: Optional query for focused summarization
            
        Returns:
            Generated summary text
        """
        # Create sumy parser
        parser = PlaintextParser.from_string(text, Tokenizer(self.config.language))
        
        # Determine number of sentences
        sentences = sent_tokenize(text, language=self.config.language)
        num_sentences = min(self.config.max_sentences, max(1, int(len(sentences) * self.config.ratio)))
        
        # Generate summary
        summary_sentences = self.sumy_summarizer(parser.document, num_sentences)
        
        return " ".join(str(sentence) for sentence in summary_sentences)

class MultiDocumentSummarizer:
    """
    Summarizer for multiple documents with redundancy reduction.
    """
    
    def __init__(self, config: Optional[SummarizationConfig] = None):
        """
        Initialize multi-document summarizer.
        
        Args:
            config: Summarization configuration
        """
        self.config = config or SummarizationConfig()
        self.text_rank = TextRankSummarizer(config)
        
        # Increase diversity factor for multi-document summarization
        if self.config.diversity_factor < 0.6:
            self.config.diversity_factor = 0.6
            self.config.importance_factor = 0.4
            
    def _detect_redundancy(self, candidates: List[str], selected: List[str], threshold: float = 0.5) -> bool:
        """
        Detect if a candidate sentence is redundant given selected sentences.
        
        Args:
            candidate: Candidate sentence
            selected: Already selected sentences
            threshold: Similarity threshold
            
        Returns:
            True if redundant, False otherwise
        """
        if not selected:
            return False
            
        # Create TF-IDF vectors
        all_sentences = selected + candidates
        
        # Create TF-IDF vectorizer
        vectorizer = TfidfVectorizer()
        try:
            tfidf_matrix = vectorizer.fit_transform(all_sentences)
            
            # Calculate similarity between candidate and selected sentences
            candidate_idx = len(selected)
            
            # Get candidate vector
            candidate_vector = tfidf_matrix[candidate_idx:candidate_idx+len(candidates)]
            
            # Get selected vectors
            selected_vectors = tfidf_matrix[:candidate_idx]
            
            # Compute cosine similarity
            similarities = cosine_similarity(candidate_vector, selected_vectors)
            
            # Check if any similarity exceeds threshold
            return (similarities > threshold).any()
            
        except:
            # Fallback for very short texts
            # Simple word overlap
            candidate_words = set(" ".join(candidates).lower().split())
            selected_words = set(" ".join(selected).lower().split())
            
            if not candidate_words or not selected_words:
                return False
                
            # Jaccard similarity
            overlap = len(candidate_words.intersection(selected_words))
            union = len(candidate_words.union(selected_words))
            
            return overlap / union > threshold
    
    def summarize(self, documents: List[str], query: Optional[str] = None) -> str:
        """
        Generate a summary from multiple documents.
        
        Args:
            documents: List of document texts
            query: Optional query for focused summarization
            
        Returns:
            Generated summary text
        """
        if not documents:
            return ""
            
        # For a single document, use TextRank
        if len(documents) == 1:
            return self.text_rank.summarize(documents[0], query)
            
        all_sentences = []
        doc_sentences = []
        
        # Extract sentences from each document
        for i, doc in enumerate(documents):
            sentences = sent_tokenize(doc, language=self.config.language)
            filtered_sentences = [s for s in sentences if len(s) >= self.config.min_length]
            
            # Add to document-specific list
            doc_sentences.append(filtered_sentences)
            
            # Add to all sentences with document marker
            for j, sentence in enumerate(filtered_sentences):
                all_sentences.append(SentenceInfo(
                    sentence=sentence,
                    position=j,
                    original_index=j,
                    source_document=i
                ))
                
        if not all_sentences:
            return ""
            
        # Define sentence similarity measure
        docs = [s.sentence for s in all_sentences]
        
        # Create TF-IDF vectorizer
        tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.95)
        
        try:
            # Create TF-IDF matrix
            tfidf_matrix = tfidf_vectorizer.fit_transform(docs)
            
            # Compute cosine similarity
            similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
            
            # Apply TextRank algorithm
            self.text_rank._apply_text_rank(all_sentences, similarity_matrix)
            
            # Save vectors for later use
            for i, sentence in enumerate(all_sentences):
                sentence.vector = tfidf_matrix[i]
                
        except:
            # Fallback for very short texts
            logger.warning("TF-IDF matrix calculation failed, using fallback approach")
            for sentence in all_sentences:
                # Random score (not ideal but prevents errors)
                sentence.score = random.random()
        
        # Apply query weighting if provided
        if query:
            self.text_rank._apply_query_weighting(all_sentences, query)
            
        # Cluster sentences if enabled
        self.text_rank._cluster_sentences(all_sentences)
        
        # MMR-based selection to reduce redundancy
        selected_sentences = []
        selected_texts = []
        candidates = sorted(all_sentences, key=lambda s: s.score, reverse=True)
        
        # Determine number of sentences to include
        selection_count = min(
            self.config.max_sentences,
            max(1, int(len(all_sentences) * self.config.ratio))
        )
        
        while candidates and len(selected_sentences) < selection_count:
            # Find best candidate that is not redundant
            best_idx = None
            
            for i, candidate in enumerate(candidates):
                # Check if this candidate is redundant
                if not self._detect_redundancy([candidate.sentence], selected_texts, threshold=0.6):
                    best_idx = i
                    break
                    
            if best_idx is None:
                # All remaining candidates are redundant
                # Lower the threshold and retry
                for i, candidate in enumerate(candidates):
                    if not self._detect_redundancy([candidate.sentence], selected_texts, threshold=0.4):
                        best_idx = i
                        break
                        
            if best_idx is None:
                # Still all redundant, just take the highest scoring one
                best_idx = 0
                
            # Add best candidate to selected
            best_candidate = candidates.pop(best_idx)
            selected_sentences.append(best_candidate)
            selected_texts.append(best_candidate.sentence)
        
        # Sort selected sentences by document order
        selected_sentences.sort(key=lambda s: (s.source_document, s.original_index))
        
        # Join sentences into summary
        summary = " ".join(s.sentence for s in selected_sentences)
        
        return summary

class QueryFocusedSummarizer:
    """
    Summarizer that focuses on content relevant to a specific query.
    """
    
    def __init__(self, config: Optional[SummarizationConfig] = None):
        """
        Initialize query-focused summarizer.
        
        Args:
            config: Summarization configuration
        """
        self.config = config or SummarizationConfig()
        self.config.use_query_weighting = True
        self.text_rank = TextRankSummarizer(self.config)
    
    def summarize(self, text: str, query: str) -> str:
        """
        Generate a query-focused summary.
        
        Args:
            text: Input text
            query: Query for focused summarization
            
        Returns:
            Generated summary text
        """
        if not text or not query:
            return ""
            
        # Apply TextRank with query weighting
        return self.text_rank.summarize(text, query)

def summarize_text(text: str, ratio: float = 0.2, max_sentences: int = 10, 
                  query: Optional[str] = None, method: str = "textrank") -> str:
    """
    Convenience function to summarize text with default settings.
    
    Args:
        text: Input text
        ratio: Percentage of original text to retain (0.0-1.0)
        max_sentences: Maximum number of sentences in summary
        query: Optional query for focused summarization
        method: Summarization method ('textrank', 'lexrank', 'lsa')
        
    Returns:
        Generated summary text
    """
    config = SummarizationConfig(ratio=ratio, max_sentences=max_sentences)
    
    if method.lower() == "textrank":
        summarizer = TextRankSummarizer(config)
    elif method.lower() == "lexrank":
        summarizer = LexRankSummarizer(config)
    elif method.lower() == "lsa" and SUMY_AVAILABLE:
        # Use sumy's LSA summarizer
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        stemmer = Stemmer("english")
        summarizer = LsaSummarizer(stemmer)
        summarizer.stop_words = get_stop_words("english")
        
        sentences = sent_tokenize(text)
        num_sentences = min(max_sentences, max(1, int(len(sentences) * ratio)))
        
        summary_sentences = summarizer(parser.document, num_sentences)
        return " ".join(str(sentence) for sentence in summary_sentences)
    else:
        # Default to TextRank
        summarizer = TextRankSummarizer(config)
        
    return summarizer.summarize(text, query)

def summarize_multiple_documents(documents: List[str], ratio: float = 0.2, 
                                max_sentences: int = 10, query: Optional[str] = None) -> str:
    """
    Convenience function to summarize multiple documents.
    
    Args:
        documents: List of document texts
        ratio: Percentage of original text to retain (0.0-1.0)
        max_sentences: Maximum number of sentences in summary
        query: Optional query for focused summarization
        
    Returns:
        Generated summary text
    """
    config = SummarizationConfig(ratio=ratio, max_sentences=max_sentences, diversity_factor=0.6)
    summarizer = MultiDocumentSummarizer(config)
    return summarizer.summarize(documents, query)












retrieval/vector_store.py - Advanced Vector Database

"""
Advanced vector database implementation with multiple backend support:
1. In-memory numpy/scipy implementation
2. FAISS integration when available
3. Hnswlib integration when available
4. Disk-based persistence and memory-mapping for large datasets
5. Support for sharding and partitioning
6. Async and batch operations for efficiency
"""
import os
import logging
import pickle
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union, Callable, Generator
import uuid
import threading
import tempfile
import shutil
import warnings
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

import numpy as np
from scipy.spatial.distance import cdist

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("FAISS not installed. FAISS-based vector indices will not be available.")

try:
    import hnswlib
    HNSWLIB_AVAILABLE = True
except ImportError:
    HNSWLIB_AVAILABLE = False
    logging.warning("HNSWLib not installed. HNSW-based vector indices will not be available.")

try:
    import annoy
    ANNOY_AVAILABLE = True
except ImportError:
    ANNOY_AVAILABLE = False
    logging.warning("Annoy not installed. Annoy-based vector indices will not be available.")

try:
    import milvus
    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False
    logging.warning("Milvus not installed. Milvus-based vector indices will not be available.")

logger = logging.getLogger(__name__)

class DistanceMetric(Enum):
    """Distance metrics for vector search."""
    COSINE = "cosine"
    EUCLIDEAN = "euclidean"
    DOT_PRODUCT = "dot_product"
    
    @classmethod
    def from_string(cls, metric_str: str) -> 'DistanceMetric':
        """Convert string to DistanceMetric."""
        if metric_str.lower() == "cosine":
            return cls.COSINE
        elif metric_str.lower() in ["euclidean", "l2"]:
            return cls.EUCLIDEAN
        elif metric_str.lower() in ["dot_product", "dot", "inner_product"]:
            return cls.DOT_PRODUCT
        else:
            raise ValueError(f"Unknown distance metric: {metric_str}")

class IndexType(Enum):
    """Types of vector indices."""
    FLAT = "flat"
    HNSW = "hnsw"
    IVF_FLAT = "ivf_flat"
    IVF_PQ = "ivf_pq"
    ANNOY = "annoy"
    MILVUS = "milvus"
    
    @classmethod
    def from_string(cls, type_str: str) -> 'IndexType':
        """Convert string to IndexType."""
        if type_str.lower() == "flat":
            return cls.FLAT
        elif type_str.lower() == "hnsw":
            return cls.HNSW
        elif type_str.lower() == "ivf_flat":
            return cls.IVF_FLAT
        elif type_str.lower() == "ivf_pq":
            return cls.IVF_PQ
        elif type_str.lower() == "annoy":
            return cls.ANNOY
        elif type_str.lower() == "milvus":
            return cls.MILVUS
        else:
            raise ValueError(f"Unknown index type: {type_str}")

class VectorIndex:
    """Base class for vector indices."""
    
    def __init__(self, dim: int, metric: DistanceMetric):
        """
        Initialize the vector index.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
        """
        self.dim = dim
        self.metric = metric
        self.is_ready = False
    
    def add(self, vectors: np.ndarray, ids: Optional[List[Any]] = None) -> List[Any]:
        """
        Add vectors to the index.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        raise NotImplementedError
    
    def search(self, query: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            
        Returns:
            Tuple of (distances, indices)
        """
        raise NotImplementedError
    
    def delete(self, ids: List[Any]) -> None:
        """
        Delete vectors from the index.
        
        Args:
            ids: IDs of vectors to delete
        """
        raise NotImplementedError
    
    def save(self, path: str) -> None:
        """
        Save the index to disk.
        
        Args:
            path: Path to save the index
        """
        raise NotImplementedError
    
    @classmethod
    def load(cls, path: str) -> 'VectorIndex':
        """
        Load the index from disk.
        
        Args:
            path: Path to load the index from
            
        Returns:
            Loaded index
        """
        raise NotImplementedError
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the index.
        
        Returns:
            Number of vectors
        """
        raise NotImplementedError

class NumpyVectorIndex(VectorIndex):
    """In-memory vector index using numpy."""
    
    def __init__(self, dim: int, metric: DistanceMetric):
        """
        Initialize numpy vector index.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
        """
        super().__init__(dim, metric)
        self.vectors = None
        self.ids = []
        self.id_to_index = {}
        self.is_ready = True
    
    def add(self, vectors: np.ndarray, ids: Optional[List[Any]] = None) -> List[Any]:
        """
        Add vectors to the index.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        # Ensure vectors have correct shape
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
            
        n_vectors = vectors.shape[0]
        
        # Create IDs if not provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(n_vectors)]
        elif len(ids) != n_vectors:
            raise ValueError(f"Number of IDs ({len(ids)}) doesn't match number of vectors ({n_vectors})")
            
        # Handle first addition
        if self.vectors is None:
            self.vectors = vectors.copy()
            self.ids = ids.copy()
            self.id_to_index = {id: i for i, id in enumerate(ids)}
            return ids
            
        # Handle subsequent additions
        old_count = len(self.ids)
        new_indices = list(range(old_count, old_count + n_vectors))
        
        # Update ID to index mapping
        for i, id in zip(new_indices, ids):
            self.id_to_index[id] = i
            
        # Update vectors and IDs
        self.vectors = np.vstack((self.vectors, vectors))
        self.ids.extend(ids)
        
        return ids
    
    def search(self, query: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            
        Returns:
            Tuple of (distances, indices)
        """
        if self.vectors is None or len(self.ids) == 0:
            if len(query.shape) == 1:
                return np.array([]), np.array([])
            else:
                return np.array([[] for _ in range(query.shape[0])]), np.array([[] for _ in range(query.shape[0])])
                
        # Ensure query has correct shape
        if len(query.shape) == 1:
            query = query.reshape(1, -1)
            
        # Calculate distances
        if self.metric == DistanceMetric.COSINE:
            # Normalize vectors for cosine similarity
            query_norm = np.linalg.norm(query, axis=1, keepdims=True)
            vectors_norm = np.linalg.norm(self.vectors, axis=1, keepdims=True)
            
            # Avoid division by zero
            query_norm[query_norm == 0] = 1
            vectors_norm[vectors_norm == 0] = 1
            
            query_normalized = query / query_norm
            vectors_normalized = self.vectors / vectors_norm
            
            # Cosine similarity = dot product of normalized vectors
            similarities = np.dot(query_normalized, vectors_normalized.T)
            
            # Convert to distances (1 - similarity)
            distances = 1 - similarities
            
        elif self.metric == DistanceMetric.DOT_PRODUCT:
            # Dot product (negative for nearest neighbor search)
            similarities = np.dot(query, self.vectors.T)
            distances = -similarities
            
        else:  # DistanceMetric.EUCLIDEAN
            distances = cdist(query, self.vectors, 'euclidean')
            
        # Limit k to the number of vectors
        k = min(k, len(self.ids))
        
        # Find k nearest neighbors
        indices = np.argsort(distances, axis=1)[:, :k]
        
        # Get corresponding distances
        distances = np.take_along_axis(distances, indices, axis=1)
        
        # If query is a single vector, return 1D arrays
        if query.shape[0] == 1:
            indices = indices[0]
            distances = distances[0]
            
        return distances, indices
    
    def delete(self, ids: List[Any]) -> None:
        """
        Delete vectors from the index.
        
        Args:
            ids: IDs of vectors to delete
        """
        if self.vectors is None or len(self.ids) == 0:
            return
            
        # Find indices to delete
        indices_to_delete = []
        for id in ids:
            if id in self.id_to_index:
                indices_to_delete.append(self.id_to_index[id])
                
        if not indices_to_delete:
            return
            
        # Create mask for vectors to keep
        keep_mask = np.ones(len(self.ids), dtype=bool)
        keep_mask[indices_to_delete] = False
        
        # Filter vectors and IDs
        self.vectors = self.vectors[keep_mask]
        kept_ids = [id for i, id in enumerate(self.ids) if keep_mask[i]]
        
        # Update ID to index mapping
        self.id_to_index = {id: i for i, id in enumerate(kept_ids)}
        self.ids = kept_ids
    
    def save(self, path: str) -> None:
        """
        Save the index to disk.
        
        Args:
            path: Path to save the index
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        data = {
            "dim": self.dim,
            "metric": self.metric.value,
            "ids": self.ids,
            "vectors": self.vectors,
            "type": "numpy"
        }
        
        with open(path, "wb") as f:
            pickle.dump(data, f)
    
    @classmethod
    def load(cls, path: str) -> 'NumpyVectorIndex':
        """
        Load the index from disk.
        
        Args:
            path: Path to load the index from
            
        Returns:
            Loaded NumpyVectorIndex
        """
        with open(path, "rb") as f:
            data = pickle.load(f)
            
        metric = DistanceMetric.from_string(data["metric"])
        index = cls(data["dim"], metric)
        index.ids = data["ids"]
        index.vectors = data["vectors"]
        index.id_to_index = {id: i for i, id in enumerate(index.ids)}
        
        return index
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the index.
        
        Returns:
            Number of vectors
        """
        return len(self.ids) if self.ids else 0

class FaissVectorIndex(VectorIndex):
    """Vector index using FAISS."""
    
    def __init__(self, dim: int, metric: DistanceMetric, index_type: IndexType = IndexType.FLAT, params: Optional[Dict[str, Any]] = None):
        """
        Initialize FAISS vector index.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
            index_type: Type of FAISS index
            params: Additional index parameters
        """
        super().__init__(dim, metric)
        
        if not FAISS_AVAILABLE:
            raise ImportError("FAISS is not available. Please install FAISS to use FaissVectorIndex.")
            
        self.index_type = index_type
        self.params = params or {}
        self.index = None
        self.ids = []
        self.id_to_idx = {}
        
        # Initialize index
        self._create_index()
    
    def _create_index(self) -> None:
        """Create the FAISS index."""
        # Configure metric
        if self.metric == DistanceMetric.COSINE:
            metric_type = faiss.METRIC_INNER_PRODUCT
            self.normalize = True
        elif self.metric == DistanceMetric.DOT_PRODUCT:
            metric_type = faiss.METRIC_INNER_PRODUCT
            self.normalize = False
        else:  # DistanceMetric.EUCLIDEAN
            metric_type = faiss.METRIC_L2
            self.normalize = False
            
        # Create index based on type
        if self.index_type == IndexType.FLAT:
            self.index = faiss.IndexFlatL2(self.dim) if metric_type == faiss.METRIC_L2 else faiss.IndexFlatIP(self.dim)
            
        elif self.index_type == IndexType.HNSW:
            # Get parameters
            M = self.params.get("M", 16)  # Number of connections per layer
            ef_construction = self.params.get("ef_construction", 200)  # Construction-time neighbors to consider
            
            # Create HNSW index
            self.index = faiss.IndexHNSWFlat(self.dim, M, metric_type)
            self.index.hnsw.efConstruction = ef_construction
            self.index.hnsw.efSearch = self.params.get("ef_search", 64)
            
        elif self.index_type == IndexType.IVF_FLAT:
            # Get parameters
            nlist = self.params.get("nlist", 100)  # Number of clusters
            
            # Create quantizer
            quantizer = faiss.IndexFlatL2(self.dim) if metric_type == faiss.METRIC_L2 else faiss.IndexFlatIP(self.dim)
            
            # Create IVF index
            self.index = faiss.IndexIVFFlat(quantizer, self.dim, nlist, metric_type)
            self.index.nprobe = self.params.get("nprobe", 10)  # Number of clusters to visit during search
            
        elif self.index_type == IndexType.IVF_PQ:
            # Get parameters
            nlist = self.params.get("nlist", 100)  # Number of clusters
            M = self.params.get("M", 8)  # Number of subquantizers
            nbits = self.params.get("nbits", 8)  # Number of bits per subquantizer
            
            # Create quantizer
            quantizer = faiss.IndexFlatL2(self.dim) if metric_type == faiss.METRIC_L2 else faiss.IndexFlatIP(self.dim)
            
            # Create IVF-PQ index
            self.index = faiss.IndexIVFPQ(quantizer, self.dim, nlist, M, nbits, metric_type)
            self.index.nprobe = self.params.get("nprobe", 10)  # Number of clusters to visit during search
            
        else:
            raise ValueError(f"Unsupported FAISS index type: {self.index_type}")
            
        # Mark as not trained for indices that require training
        self.is_trained = isinstance(self.index, faiss.IndexFlat)
        self.is_ready = self.is_trained
    
    def add(self, vectors: np.ndarray, ids: Optional[List[Any]] = None) -> List[Any]:
        """
        Add vectors to the index.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        # Ensure vectors have correct shape
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
            
        # Handle empty vectors
        n_vectors = vectors.shape[0]
        if n_vectors == 0:
            return []
            
        # Create IDs if not provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(n_vectors)]
        elif len(ids) != n_vectors:
            raise ValueError(f"Number of IDs ({len(ids)}) doesn't match number of vectors ({n_vectors})")
            
        # Ensure vectors are the right type
        vectors = vectors.astype(np.float32)
        
        # Normalize vectors if using cosine similarity
        if self.normalize:
            faiss.normalize_L2(vectors)
            
        # Train index if needed
        if not self.is_trained:
            self.index.train(vectors)
            self.is_trained = True
            
        # Add vectors and update ID mapping
        start_idx = len(self.ids)
        self.index.add(vectors)
        
        for i, id in enumerate(ids):
            self.id_to_idx[id] = start_idx + i
            
        self.ids.extend(ids)
        self.is_ready = True
        
        return ids
    
    def search(self, query: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            
        Returns:
            Tuple of (distances, indices)
        """
        if not self.is_ready or len(self.ids) == 0:
            if len(query.shape) == 1:
                return np.array([]), np.array([])
            else:
                n_queries = query.shape[0]
                return np.empty((n_queries, 0)), np.empty((n_queries, 0))
                
        # Ensure query has correct shape
        if len(query.shape) == 1:
            query = query.reshape(1, -1)
            
        # Ensure query is the right type
        query = query.astype(np.float32)
        
        # Normalize query if using cosine similarity
        if self.normalize:
            faiss.normalize_L2(query)
            
        # Limit k to the number of vectors
        k = min(k, len(self.ids))
        
        # Perform search
        distances, indices = self.index.search(query, k)
        
        # If query is a single vector, return 1D arrays
        if query.shape[0] == 1:
            distances = distances[0]
            indices = indices[0]
            
        return distances, indices
    
    def delete(self, ids: List[Any]) -> None:
        """
        Delete vectors from the index.
        
        Args:
            ids: IDs of vectors to delete
        """
        if not self.is_ready or len(self.ids) == 0:
            return
            
        # Find indices to delete
        indices_to_delete = []
        for id in ids:
            if id in self.id_to_idx:
                indices_to_delete.append(self.id_to_idx[id])
                
        if not indices_to_delete:
            return
            
        # Check if index supports direct removal
        if hasattr(self.index, 'remove_ids'):
            # Convert indices to an IDSelector for FAISS
            id_array = np.array(indices_to_delete, dtype=np.int64)
            sel = faiss.IDSelectorBatch(len(id_array), faiss.swig_ptr(id_array))
            
            # Remove from FAISS index
            self.index.remove_ids(sel)
            
            # Update ID mapping
            for id in ids:
                if id in self.id_to_idx:
                    del self.id_to_idx[id]
                    self.ids.remove(id)
        else:
            # For indices that don't support direct removal, we need to rebuild
            logger.warning("FAISS index type doesn't support direct removal. Rebuilding index...")
            
            # Keep track of which indices to keep
            keep_mask = np.ones(len(self.ids), dtype=bool)
            for idx in indices_to_delete:
                if 0 <= idx < len(self.ids):
                    keep_mask[idx] = False
                    
            # Create list of kept IDs
            kept_ids = [id for i, id in enumerate(self.ids) if keep_mask[i]]
            
            # TODO: Extract vectors for kept IDs and rebuild index
            # This is complex and depends on the FAISS index type
            # For now, this is a placeholder
            
            logger.warning("Deletion from FAISS index not fully implemented for this index type.")
    
    def save(self, path: str) -> None:
        """
        Save the index to disk.
        
        Args:
            path: Path to save the index
        """
        if not self.is_ready:
            raise ValueError("Cannot save uninitialized FAISS index")
            
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Save FAISS index
        faiss.write_index(self.index, f"{path}.faiss")
        
        # Save metadata
        metadata = {
            "dim": self.dim,
            "metric": self.metric.value,
            "index_type": self.index_type.value,
            "params": self.params,
            "ids": self.ids,
            "normalize": self.normalize,
            "type": "faiss"
        }
        
        with open(f"{path}.meta", "wb") as f:
            pickle.dump(metadata, f)
    
    @classmethod
    def load(cls, path: str) -> 'FaissVectorIndex':
        """
        Load the index from disk.
        
        Args:
            path: Path to load the index from
            
        Returns:
            Loaded FaissVectorIndex
        """
        if not FAISS_AVAILABLE:
            raise ImportError("FAISS is not available. Please install FAISS to use FaissVectorIndex.")
            
        # Load metadata
        with open(f"{path}.meta", "rb") as f:
            metadata = pickle.load(f)
            
        # Create index
        metric = DistanceMetric.from_string(metadata["metric"])
        index_type = IndexType.from_string(metadata["index_type"])
        
        index = cls(metadata["dim"], metric, index_type, metadata["params"])
        
        # Load FAISS index
        index.index = faiss.read_index(f"{path}.faiss")
        index.normalize = metadata["normalize"]
        index.ids = metadata["ids"]
        index.id_to_idx = {id: i for i, id in enumerate(index.ids)}
        index.is_trained = True
        index.is_ready = True
        
        return index
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the index.
        
        Returns:
            Number of vectors
        """
        return len(self.ids) if self.ids else 0

class HnswVectorIndex(VectorIndex):
    """Vector index using HNSWLib."""
    
    def __init__(self, dim: int, metric: DistanceMetric, params: Optional[Dict[str, Any]] = None):
        """
        Initialize HNSW vector index.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
            params: Additional index parameters
        """
        super().__init__(dim, metric)
        
        if not HNSWLIB_AVAILABLE:
            raise ImportError("HNSWLib is not available. Please install hnswlib to use HnswVectorIndex.")
            
        self.params = params or {}
        self.index = None
        self.ids = []
        self.id_to_idx = {}
        self.next_id = 0
        
        # Initialize index
        self._create_index()
    
    def _create_index(self) -> None:
        """Create the HNSW index."""
        space = 'cosine' if self.metric == DistanceMetric.COSINE else \
                'ip' if self.metric == DistanceMetric.DOT_PRODUCT else \
                'l2'
                
        # Get parameters
        ef = self.params.get("ef", 200)  # Query-time neighbors to consider
        M = self.params.get("M", 16)  # Number of bi-directional links
        max_elements = self.params.get("max_elements", 100000)  # Maximum number of elements
        
        # Create index
        self.index = hnswlib.Index(space=space, dim=self.dim)
        self.index.init_index(max_elements=max_elements, ef_construction=ef, M=M)
        self.index.set_ef(ef)  # Set query-time ef
        
        self.is_ready = True
    
    def add(self, vectors: np.ndarray, ids: Optional[List[Any]] = None) -> List[Any]:
        """
        Add vectors to the index.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        # Ensure vectors have correct shape
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
            
        # Handle empty vectors
        n_vectors = vectors.shape[0]
        if n_vectors == 0:
            return []
            
        # Create IDs if not provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(n_vectors)]
        elif len(ids) != n_vectors:
            raise ValueError(f"Number of IDs ({len(ids)}) doesn't match number of vectors ({n_vectors})")
            
        # Ensure vectors are the right type
        vectors = vectors.astype(np.float32)
        
        # Assign integer indices
        indices = list(range(self.next_id, self.next_id + n_vectors))
        self.next_id += n_vectors
        
        # Add vectors
        self.index.add_items(vectors, indices)
        
        # Update ID mapping
        for i, id in enumerate(ids):
            self.id_to_idx[id] = indices[i]
            
        self.ids.extend(ids)
        
        # Resize index if needed
        current_size = len(self.ids)
        max_elements = self.index.get_max_elements()
        
        if current_size > max_elements * 0.8:  # Resize at 80% capacity
            new_size = max(max_elements * 2, current_size * 1.5)
            self.index.resize_index(int(new_size))
            
        return ids
    
    def search(self, query: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            
        Returns:
            Tuple of (distances, indices)
        """
        if not self.is_ready or len(self.ids) == 0:
            if len(query.shape) == 1:
                return np.array([]), np.array([])
            else:
                n_queries = query.shape[0]
                return np.empty((n_queries, 0)), np.empty((n_queries, 0))
                
        # Ensure query has correct shape
        if len(query.shape) == 1:
            query = query.reshape(1, -1)
            
        # Ensure query is the right type
        query = query.astype(np.float32)
        
        # Limit k to the number of vectors
        k = min(k, len(self.ids))
        
        # Perform search
        indices, distances = self.index.knn_query(query, k=k)
        
        # Convert internal indices to original indices
        # Not needed for HNSW as we're using the stored indices directly
        
        # If query is a single vector, return 1D arrays
        if query.shape[0] == 1:
            distances = distances[0]
            indices = indices[0]
            
        # Map indices to IDs
        id_map = {v: k for k, v in self.id_to_idx.items()}
        id_indices = [[id_map.get(idx, idx) for idx in row] for row in indices]
        
        if query.shape[0] == 1:
            id_indices = id_indices[0]
            
        return distances, np.array(id_indices)
    
    def delete(self, ids: List[Any]) -> None:
        """
        Delete vectors from the index.
        
        Args:
            ids: IDs of vectors to delete
        """
        if not self.is_ready or len(self.ids) == 0:
            return
            
        # Find indices to delete
        indices_to_delete = []
        for id in ids:
            if id in self.id_to_idx:
                indices_to_delete.append(self.id_to_idx[id])
                
        if not indices_to_delete:
            return
            
        # Delete from index
        for idx in indices_to_delete:
            self.index.mark_deleted(idx)
            
        # Update ID mapping
        for id in ids:
            if id in self.id_to_idx:
                idx = self.id_to_idx[id]
                del self.id_to_idx[id]
                self.ids.remove(id)
    
    def save(self, path: str) -> None:
        """
        Save the index to disk.
        
        Args:
            path: Path to save the index
        """
        if not self.is_ready:
            raise ValueError("Cannot save uninitialized HNSW index")
            
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Save HNSW index
        self.index.save_index(f"{path}.hnsw")
        
        # Save metadata
        metadata = {
            "dim": self.dim,
            "metric": self.metric.value,
            "params": self.params,
            "ids": self.ids,
            "id_to_idx": self.id_to_idx,
            "next_id": self.next_id,
            "type": "hnsw"
        }
        
        with open(f"{path}.meta", "wb") as f:
            pickle.dump(metadata, f)
    
    @classmethod
    def load(cls, path: str) -> 'HnswVectorIndex':
        """
        Load the index from disk.
        
        Args:
            path: Path to load the index from
            
        Returns:
            Loaded HnswVectorIndex
        """
        if not HNSWLIB_AVAILABLE:
            raise ImportError("HNSWLib is not available. Please install hnswlib to use HnswVectorIndex.")
            
        # Load metadata
        with open(f"{path}.meta", "rb") as f:
            metadata = pickle.load(f)
            
        # Create index
        metric = DistanceMetric.from_string(metadata["metric"])
        
        index = cls(metadata["dim"], metric, metadata["params"])
        
        # Set index properties
        index.ids = metadata["ids"]
        index.id_to_idx = metadata["id_to_idx"]
        index.next_id = metadata["next_id"]
        
        # Load HNSW index
        index.index.load_index(f"{path}.hnsw")
        
        return index
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the index.
        
        Returns:
            Number of vectors
        """
        return len(self.ids) if self.ids else 0

class AnnoyVectorIndex(VectorIndex):
    """Vector index using Annoy."""
    
    def __init__(self, dim: int, metric: DistanceMetric, params: Optional[Dict[str, Any]] = None):
        """
        Initialize Annoy vector index.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
            params: Additional index parameters
        """
        super().__init__(dim, metric)
        
        if not ANNOY_AVAILABLE:
            raise ImportError("Annoy is not available. Please install annoy to use AnnoyVectorIndex.")
            
        self.params = params or {}
        self.index = None
        self.ids = []
        self.id_to_idx = {}
        self.next_id = 0
        self.is_built = False
        
        # Initialize index
        self._create_index()
    
    def _create_index(self) -> None:
        """Create the Annoy index."""
        metric_str = 'angular' if self.metric == DistanceMetric.COSINE else \
                    'dot' if self.metric == DistanceMetric.DOT_PRODUCT else \
                    'euclidean'
                    
        # Get parameters
        n_trees = self.params.get("n_trees", 10)  # Number of trees
        
        # Create index
        self.index = annoy.AnnoyIndex(self.dim, metric_str)
        self.index.set_seed(42)  # For reproducibility
        
        # Store parameters
        self.n_trees = n_trees
        
        self.is_ready = True
    
    def add(self, vectors: np.ndarray, ids: Optional[List[Any]] = None) -> List[Any]:
        """
        Add vectors to the index.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        if self.is_built:
            raise ValueError("Cannot add vectors after index is built")
            
        # Ensure vectors have correct shape
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
            
        # Handle empty vectors
        n_vectors = vectors.shape[0]
        if n_vectors == 0:
            return []
            
        # Create IDs if not provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(n_vectors)]
        elif len(ids) != n_vectors:
            raise ValueError(f"Number of IDs ({len(ids)}) doesn't match number of vectors ({n_vectors})")
            
        # Ensure vectors are the right type
        vectors = vectors.astype(np.float32)
        
        # Add vectors
        for i, vector in enumerate(vectors):
            self.index.add_item(self.next_id, vector)
            self.id_to_idx[ids[i]] = self.next_id
            self.next_id += 1
            
        self.ids.extend(ids)
        
        return ids
    
    def build(self) -> None:
        """Build the index for searching."""
        self.index.build(self.n_trees)
        self.is_built = True
    
    def search(self, query: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            
        Returns:
            Tuple of (distances, indices)
        """
        if not self.is_ready or len(self.ids) == 0:
            if len(query.shape) == 1:
                return np.array([]), np.array([])
            else:
                n_queries = query.shape[0]
                return np.empty((n_queries, 0)), np.empty((n_queries, 0))
                
        # Build the index if not already built
        if not self.is_built:
            self.build()
            
        # Ensure query has correct shape
        if len(query.shape) == 1:
            query = query.reshape(1, -1)
            
        # Ensure query is the right type
        query = query.astype(np.float32)
        
        # Limit k to the number of vectors
        k = min(k, len(self.ids))
        
        # Perform search
        results = []
        for q in query:
            indices, distances = self.index.get_nns_by_vector(q, k, include_distances=True)
            results.append((indices, distances))
            
        # Format results
        if len(query) == 1:
            indices, distances = results[0]
        else:
            indices = np.array([r[0] for r in results])
            distances = np.array([r[1] for r in results])
            
        # Convert internal indices to IDs
        id_map = {v: k for k, v in self.id_to_idx.items()}
        
        if len(query) == 1:
            id_indices = [id_map.get(idx, idx) for idx in indices]
        else:
            id_indices = [[id_map.get(idx, idx) for idx in row] for row in indices]
            
        return np.array(distances), np.array(id_indices)
    
    def delete(self, ids: List[Any]) -> None:
        """
        Delete vectors from the index.
        
        Args:
            ids: IDs of vectors to delete
        """
        if self.is_built:
            raise ValueError("Cannot delete vectors after index is built")
            
        if not self.is_ready or len(self.ids) == 0:
            return
            
        # Find indices to delete
        indices_to_delete = []
        for id in ids:
            if id in self.id_to_idx:
                indices_to_delete.append(self.id_to_idx[id])
                
        if not indices_to_delete:
            return
            
        logger.warning("Delete operation not supported for Annoy index. Need to rebuild.")
        
        # For Annoy, we need to rebuild the index
        # This is a placeholder - actual implementation would require saving vectors
        # and rebuilding the index
    
    def save(self, path: str) -> None:
        """
        Save the index to disk.
        
        Args:
            path: Path to save the index
        """
        if not self.is_ready:
            raise ValueError("Cannot save uninitialized Annoy index")
            
        # Build the index if not already built
        if not self.is_built:
            self.build()
            
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Save Annoy index
        self.index.save(f"{path}.ann")
        
        # Save metadata
        metadata = {
            "dim": self.dim,
            "metric": self.metric.value,
            "params": self.params,
            "ids": self.ids,
            "id_to_idx": self.id_to_idx,
            "next_id": self.next_id,
            "type": "annoy"
        }
        
        with open(f"{path}.meta", "wb") as f:
            pickle.dump(metadata, f)
    
    @classmethod
    def load(cls, path: str) -> 'AnnoyVectorIndex':
        """
        Load the index from disk.
        
        Args:
            path: Path to load the index from
            
        Returns:
            Loaded AnnoyVectorIndex
        """
        if not ANNOY_AVAILABLE:
            raise ImportError("Annoy is not available. Please install annoy to use AnnoyVectorIndex.")
            
        # Load metadata
        with open(f"{path}.meta", "rb") as f:
            metadata = pickle.load(f)
            
        # Create index
        metric = DistanceMetric.from_string(metadata["metric"])
        
        index = cls(metadata["dim"], metric, metadata["params"])
        
        # Set index properties
        index.ids = metadata["ids"]
        index.id_to_idx = metadata["id_to_idx"]
        index.next_id = metadata["next_id"]
        
        # Load Annoy index
        metric_str = 'angular' if index.metric == DistanceMetric.COSINE else \
                   'dot' if index.metric == DistanceMetric.DOT_PRODUCT else \
                   'euclidean'
                   
        index.index = annoy.AnnoyIndex(index.dim, metric_str)
        index.index.load(f"{path}.ann")
        index.is_built = True
        
        return index
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the index.
        
        Returns:
            Number of vectors
        """
        return len(self.ids) if self.ids else 0

class VectorStore:
    """
    Advanced vector database with multiple backends and features.
    """
    
    def __init__(self, dim: int, metric: DistanceMetric = DistanceMetric.COSINE, 
                 index_type: IndexType = IndexType.FLAT, 
                 index_params: Optional[Dict[str, Any]] = None,
                 metadata_index: bool = True,
                 persist_dir: Optional[str] = None):
        """
        Initialize the vector store.
        
        Args:
            dim: Vector dimension
            metric: Distance metric
            index_type: Type of vector index
            index_params: Additional index parameters
            metadata_index: Whether to index metadata for filtering
            persist_dir: Directory for persistence
        """
        self.dim = dim
        self.metric = metric
        self.index_type = index_type
        self.index_params = index_params or {}
        self.metadata_index = metadata_index
        self.persist_dir = persist_dir
        
        # Create index based on type
        self.index = self._create_index()
        
        # Storage for metadata
        self.metadata = {}
        
        # Lock for thread safety
        self.lock = threading.RLock()
        
        # Create metadata indices
        self.metadata_indices = {}
        
        # Track last modified time
        self.last_modified = time.time()
        
        # Load from disk if persist_dir is provided
        if persist_dir and os.path.exists(persist_dir):
            self.load()
    
    def _create_index(self) -> VectorIndex:
        """Create the vector index based on type."""
        if self.index_type == IndexType.FLAT:
            return NumpyVectorIndex(self.dim, self.metric)
            
        elif self.index_type == IndexType.HNSW:
            if HNSWLIB_AVAILABLE:
                return HnswVectorIndex(self.dim, self.metric, self.index_params)
            elif FAISS_AVAILABLE:
                return FaissVectorIndex(self.dim, self.metric, IndexType.HNSW, self.index_params)
            else:
                logger.warning("Neither HNSW nor FAISS is available. Falling back to flat index.")
                return NumpyVectorIndex(self.dim, self.metric)
                
        elif self.index_type in [IndexType.IVF_FLAT, IndexType.IVF_PQ]:
            if FAISS_AVAILABLE:
                return FaissVectorIndex(self.dim, self.metric, self.index_type, self.index_params)
            else:
                logger.warning("FAISS is not available. Falling back to flat index.")
                return NumpyVectorIndex(self.dim, self.metric)
                
        elif self.index_type == IndexType.ANNOY:
            if ANNOY_AVAILABLE:
                return AnnoyVectorIndex(self.dim, self.metric, self.index_params)
            else:
                logger.warning("Annoy is not available. Falling back to flat index.")
                return NumpyVectorIndex(self.dim, self.metric)
                
        else:
            raise ValueError(f"Unsupported index type: {self.index_type}")
    
    def _update_metadata_indices(self, id: str, metadata: Dict[str, Any]) -> None:
        """Update metadata indices for a new item."""
        if not self.metadata_index:
            return
            
        for key, value in metadata.items():
            if key not in self.metadata_indices:
                self.metadata_indices[key] = {}
                
            if value not in self.metadata_indices[key]:
                self.metadata_indices[key][value] = set()
                
            self.metadata_indices[key][value].add(id)
    
    def _remove_from_metadata_indices(self, id: str) -> None:
        """Remove an item from metadata indices."""
        if not self.metadata_index or id not in self.metadata:
            return
            
        metadata = self.metadata[id]
        
        for key, value in metadata.items():
            if key in self.metadata_indices and value in self.metadata_indices[key]:
                self.metadata_indices[key][value].discard(id)
                
                # Clean up empty sets
                if not self.metadata_indices[key][value]:
                    del self.metadata_indices[key][value]
                    
                # Clean up empty indices
                if not self.metadata_indices[key]:
                    del self.metadata_indices[key]
    
    def add(self, vectors: np.ndarray, metadatas: Optional[List[Dict[str, Any]]] = None,
           ids: Optional[List[str]] = None) -> List[str]:
        """
        Add vectors to the store.
        
        Args:
            vectors: Vectors to add, shape (n, dim)
            metadatas: Metadata for each vector
            ids: Optional IDs for the vectors
            
        Returns:
            IDs of added vectors
        """
        # Ensure vectors have correct shape
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
            
        n_vectors = vectors.shape[0]
        
        # Handle empty vectors
        if n_vectors == 0:
            return []
            
        # Ensure dimensions match
        if vectors.shape[1] != self.dim:
            raise ValueError(f"Vector dimension ({vectors.shape[1]}) doesn't match index dimension ({self.dim})")
            
        # Create IDs if not provided
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in range(n_vectors)]
        elif len(ids) != n_vectors:
            raise ValueError(f"Number of IDs ({len(ids)}) doesn't match number of vectors ({n_vectors})")
            
        # Use empty metadata if not provided
        if metadatas is None:
            metadatas = [{} for _ in range(n_vectors)]
        elif len(metadatas) != n_vectors:
            raise ValueError(f"Number of metadata items ({len(metadatas)}) doesn't match number of vectors ({n_vectors})")
            
        with self.lock:
            # Add to index
            self.index.add(vectors, ids)
            
            # Store metadata
            for i, id in enumerate(ids):
                self.metadata[id] = metadatas[i]
                
                # Update metadata indices
                self._update_metadata_indices(id, metadatas[i])
                
            # Update last modified time
            self.last_modified = time.time()
            
            # Persist if needed
            if self.persist_dir:
                self.save()
                
        return ids
    
    def add_documents(self, documents: List[Dict[str, Any]], 
                     vector_key: str = "vector", 
                     metadata_keys: Optional[List[str]] = None,
                     id_key: str = "id") -> List[str]:
        """
        Add documents with vectors to the store.
        
        Args:
            documents: List of documents with vectors
            vector_key: Key for the vector in each document
            metadata_keys: Keys to include in metadata (None = all except vector_key and id_key)
            id_key: Key for the ID in each document
            
        Returns:
            IDs of added documents
        """
        if not documents:
            return []
            
        # Extract vectors, IDs, and metadata
        vectors = []
        ids = []
        metadatas = []
        
        for doc in documents:
            # Extract vector
            if vector_key not in doc:
                raise ValueError(f"Vector key '{vector_key}' not found in document")
                
            vector = doc[vector_key]
            if isinstance(vector, list):
                vector = np.array(vector)
                
            vectors.append(vector)
            
            # Extract ID
            id = doc.get(id_key, str(uuid.uuid4()))
            ids.append(id)
            
            # Extract metadata
            if metadata_keys is None:
                # Include all fields except vector and ID
                metadata = {k: v for k, v in doc.items() if k not in [vector_key, id_key]}
            else:
                # Include only specified fields
                metadata = {k: doc[k] for k in metadata_keys if k in doc}
                
            metadatas.append(metadata)
            
        # Add to store
        vectors_array = np.vstack(vectors)
        return self.add(vectors_array, metadatas, ids)
    
    def search(self, query: np.ndarray, k: int = 10, filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Search for nearest neighbors.
        
        Args:
            query: Query vector, shape (dim,) or (n_queries, dim)
            k: Number of nearest neighbors to return
            filter: Optional metadata filter
            
        Returns:
            List of dictionaries with id, score, and metadata
        """
        # Ensure query has correct shape
        if len(query.shape) == 1:
            query = query.reshape(1, -1)
            
        # Ensure dimensions match
        if query.shape[1] != self.dim:
            raise ValueError(f"Query dimension ({query.shape[1]}) doesn't match index dimension ({self.dim})")
            
        with self.lock:
            # Apply filter if provided
            filtered_ids = None
            if filter:
                filtered_ids = self._apply_filter(filter)
                
                if not filtered_ids:
                    # No matches for filter
                    return [[] for _ in range(query.shape[0])]
                    
            # Search in index
            distances, indices = self.index.search(query, k)
            
            # Convert to list of dictionaries
            results = []
            
            # Handle single query
            if query.shape[0] == 1:
                single_results = []
                
                for i, (dist, idx) in enumerate(zip(distances, indices)):
                    # Check if index is valid (not -1)
                    if isinstance(idx, (int, np.integer)) and idx >= 0:
                        id = idx
                        
                        # Skip if doesn't match filter
                        if filtered_ids is not None and id not in filtered_ids:
                            continue
                            
                        # Add to results
                        single_results.append({
                            "id": id,
                            "score": float(dist),
                            "metadata": self.metadata.get(id, {})
                        })
                
                results = single_results
                
            else:
                # Handle multiple queries
                for q_idx in range(query.shape[0]):
                    q_results = []
                    
                    for i, (dist, idx) in enumerate(zip(distances[q_idx], indices[q_idx])):
                        # Check if index is valid (not -1)
                        if isinstance(idx, (int, np.integer)) and idx >= 0:
                            id = idx
                            
                            # Skip if doesn't match filter
                            if filtered_ids is not None and id not in filtered_ids:
                                continue
                                
                            # Add to results
                            q_results.append({
                                "id": id,
                                "score": float(dist),
                                "metadata": self.metadata.get(id, {})
                            })
                    
                    results.append(q_results)
                    
        return results
    
    def search_by_id(self, id: str, k: int = 10, filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Search for nearest neighbors of a stored vector.
        
        Args:
            id: ID of the vector to search for
            k: Number of nearest neighbors to return
            filter: Optional metadata filter
            
        Returns:
            List of dictionaries with id, score, and metadata
        """
        with self.lock:
            # Check if ID exists
            if not self.contains(id):
                raise ValueError(f"ID '{id}' not found in vector store")
                
            # Get the vector
            vector = self.get_vector(id)
            
            # Search using the vector
            return self.search(vector, k + 1, filter)  # +1 to account for self-match
    
    def search_by_text(self, text: str, embedding_function: Callable[[str], np.ndarray],
                      k: int = 10, filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Search using text input.
        
        Args:
            text: Text to search for
            embedding_function: Function to convert text to vector
            k: Number of nearest neighbors to return
            filter: Optional metadata filter
            
        Returns:
            List of dictionaries with id, score, and metadata
        """
        # Generate vector from text
        vector = embedding_function(text)
        
        # Search using the vector
        return self.search(vector, k, filter)
    
    def _apply_filter(self, filter: Dict[str, Any]) -> Set[str]:
        """
        Apply metadata filter to get matching IDs.
        
        Args:
            filter: Filter dictionary (key-value pairs)
            
        Returns:
            Set of matching IDs
        """
        if not filter:
            return None
            
        if not self.metadata_index:
            # If metadata indexing is disabled, we need to scan all metadata
            matching_ids = set()
            
            for id, metadata in self.metadata.items():
                matches = True
                
                for key, value in filter.items():
                    if key not in metadata or metadata[key] != value:
                        matches = False
                        break
                        
                if matches:
                    matching_ids.add(id)
                    
            return matching_ids
            
        # Use metadata indices for efficient filtering
        matching_ids = None
        
        for key, value in filter.items():
            if key not in self.metadata_indices or value not in self.metadata_indices[key]:
                # No matches for this key-value pair
                return set()
                
            key_matches = self.metadata_indices[key][value]
            
            if matching_ids is None:
                matching_ids = key_matches.copy()
            else:
                matching_ids &= key_matches
                
            if not matching_ids:
                # No matches for intersection
                return set()
                
        return matching_ids
    
    def delete(self, ids: List[str]) -> None:
        """
        Delete vectors from the store.
        
        Args:
            ids: IDs of vectors to delete
        """
        with self.lock:
            # Remove from index
            self.index.delete(ids)
            
            # Remove metadata
            for id in ids:
                if id in self.metadata:
                    # Remove from metadata indices
                    self._remove_from_metadata_indices(id)
                    
                    # Remove metadata
                    del self.metadata[id]
                    
            # Update last modified time
            self.last_modified = time.time()
            
            # Persist if needed
            if self.persist_dir:
                self.save()
    
    def get_vector(self, id: str) -> Optional[np.ndarray]:
        """
        Get a vector by ID.
        
        Args:
            id: ID of the vector
            
        Returns:
            Vector array or None if not found
        """
        # This operation requires access to raw vectors, which may not be possible
        # with all index types. For now, this is a placeholder.
        logger.warning("get_vector() is not implemented for all index types")
        return None
    
    def get_metadata(self, id: str) -> Optional[Dict[str, Any]]:
        """
        Get metadata for a vector.
        
        Args:
            id: ID of the vector
            
        Returns:
            Metadata dictionary or None if not found
        """
        with self.lock:
            return self.metadata.get(id)
    
    def update_metadata(self, id: str, metadata: Dict[str, Any]) -> bool:
        """
        Update metadata for a vector.
        
        Args:
            id: ID of the vector
            metadata: New metadata dictionary
            
        Returns:
            True if successful, False if ID not found
        """
        with self.lock:
            if id not in self.metadata:
                return False
                
            # Remove from metadata indices
            self._remove_from_metadata_indices(id)
            
            # Update metadata
            self.metadata[id] = metadata
            
            # Update metadata indices
            self._update_metadata_indices(id, metadata)
            
            # Update last modified time
            self.last_modified = time.time()
            
            # Persist if needed
            if self.persist_dir:
                self.save()
                
            return True
    
    def contains(self, id: str) -> bool:
        """
        Check if a vector ID exists in the store.
        
        Args:
            id: ID to check
            
        Returns:
            True if ID exists, False otherwise
        """
        with self.lock:
            return id in self.metadata
    
    def get_size(self) -> int:
        """
        Get the number of vectors in the store.
        
        Returns:
            Number of vectors
        """
        with self.lock:
            return self.index.get_size()
    
    def save(self) -> None:
        """Save the vector store to disk."""
        if not self.persist_dir:
            raise ValueError("Cannot save without persist_dir")
            
        with self.lock:
            os.makedirs(self.persist_dir, exist_ok=True)
            
            # Create a temporary directory for atomic saving
            with tempfile.TemporaryDirectory() as tmp_dir:
                # Save index
                index_path = os.path.join(tmp_dir, "index")
                self.index.save(index_path)
                
                # Save metadata
                metadata_path = os.path.join(tmp_dir, "metadata.pkl")
                with open(metadata_path, "wb") as f:
                    pickle.dump(self.metadata, f)
                    
                # Save metadata indices
                if self.metadata_index:
                    indices_path = os.path.join(tmp_dir, "indices.pkl")
                    with open(indices_path, "wb") as f:
                        pickle.dump(self.metadata_indices, f)
                        
                # Save config
                config = {
                    "dim": self.dim,
                    "metric": self.metric.value,
                    "index_type": self.index_type.value,
                    "index_params": self.index_params,
                    "metadata_index": self.metadata_index,
                    "last_modified": self.last_modified
                }
                
                config_path = os.path.join(tmp_dir, "config.json")
                with open(config_path, "w") as f:
                    json.dump(config, f)
                    
                # Move temporary files to persist_dir
                for filename in os.listdir(tmp_dir):
                    src = os.path.join(tmp_dir, filename)
                    dst = os.path.join(self.persist_dir, filename)
                    
                    # Remove existing file/directory
                    if os.path.exists(dst):
                        if os.path.isdir(dst):
                            shutil.rmtree(dst)
                        else:
                            os.remove(dst)
                            
                    # Move new file/directory
                    if os.path.isdir(src):
                        shutil.copytree(src, dst)
                    else:
                        shutil.copy2(src, dst)
    
    def load(self) -> None:
        """Load the vector store from disk."""
        if not self.persist_dir:
            raise ValueError("Cannot load without persist_dir")
            
        with self.lock:
            # Check if persist_dir exists
            if not os.path.exists(self.persist_dir):
                raise ValueError(f"Persist directory '{self.persist_dir}' does not exist")
                
            # Load config
            config_path = os.path.join(self.persist_dir, "config.json")
            if not os.path.exists(config_path):
                raise ValueError(f"Config file '{config_path}' does not exist")
                
            with open(config_path, "r") as f:
                config = json.load(f)
                
            # Check compatibility
            if config["dim"] != self.dim:
                raise ValueError(f"Dimension mismatch: {config['dim']} != {self.dim}")
                
            if config["index_type"] != self.index_type.value:
                logger.warning(f"Index type mismatch: {config['index_type']} != {self.index_type.value}")
                
            if config["metric"] != self.metric.value:
                logger.warning(f"Metric mismatch: {config['metric']} != {self.metric.value}")
                
            # Load metadata
            metadata_path = os.path.join(self.persist_dir, "metadata.pkl")
            if not os.path.exists(metadata_path):
                raise ValueError(f"Metadata file '{metadata_path}' does not exist")
                
            with open(metadata_path, "rb") as f:
                self.metadata = pickle.load(f)
                
            # Load metadata indices
            if self.metadata_index:
                indices_path = os.path.join(self.persist_dir, "indices.pkl")
                if os.path.exists(indices_path):
                    with open(indices_path, "rb") as f:
                        self.metadata_indices = pickle.load(f)
                else:
                    # Rebuild metadata indices
                    self.metadata_indices = {}
                    for id, metadata in self.metadata.items():
                        self._update_metadata_indices(id, metadata)
                        
            # Set last modified time
            self.last_modified = config.get("last_modified", time.time())
            
            # Load index
            index_path = os.path.join(self.persist_dir, "index")
            if not os.path.exists(index_path):
                raise ValueError(f"Index file '{index_path}' does not exist")
                
            # Load appropriate index type
            index_type_str = config["index_type"]
            
            if index_type_str == "numpy":
                self.index = NumpyVectorIndex.load(index_path)
            elif index_type_str == "faiss":
                if FAISS_AVAILABLE:
                    self.index = FaissVectorIndex.load(index_path)
                else:
                    raise ImportError("FAISS is not available. Cannot load FAISS index.")
            elif index_type_str == "hnsw":
                if HNSWLIB_AVAILABLE:
                    self.index = HnswVectorIndex.load(index_path)
                else:
                    raise ImportError("HNSWLib is not available. Cannot load HNSW index.")
            elif index_type_str == "annoy":
                if ANNOY_AVAILABLE:
                    self.index = AnnoyVectorIndex.load(index_path)
                else:
                    raise ImportError("Annoy is not available. Cannot load Annoy index.")
            else:
                raise ValueError(f"Unsupported index type: {index_type_str}")











Retrieval Package Initialization (__init__.py)

"""
Retrieval package for the RAG system.

This package contains components for vector search, BM25 search, and hybrid retrieval
that combines multiple search strategies for optimal results.
"""

from retrieval.vector_store import VectorStore
from retrieval.bm25_index import BM25Index
from retrieval.hybrid_search import HybridSearch
from retrieval.knowledge_graph import KnowledgeGraph

__all__ = ['VectorStore', 'BM25Index', 'HybridSearch', 'KnowledgeGraph']














Vector Database Implementation (vector_store.py)

"""
Vector database implementation with multiple backend options, including in-memory,
FAISS, and support for approximate nearest neighbors search.
"""
import os
import logging
import pickle
import json
import time
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from pathlib import Path

logger = logging.getLogger(__name__)

# Try to import optional dependencies
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logger.warning("FAISS not available. Will use NumPy-based vector store.")

try:
    from annoy import AnnoyIndex
    ANNOY_AVAILABLE = True
except ImportError:
    ANNOY_AVAILABLE = False
    logger.warning("Annoy not available. Will use alternative vector store.")

try:
    import hnswlib
    HNSW_AVAILABLE = True
except ImportError:
    HNSW_AVAILABLE = False
    logger.warning("HNSW not available. Will use alternative vector store.")

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorch not available.")

class VectorStore:
    """
    Vector database with multiple backend options for efficient similarity search.
    Supports FAISS, Annoy, HNSW, and NumPy backends, with automatic fallbacks.
    """
    
    BACKEND_FAISS = "faiss"
    BACKEND_ANNOY = "annoy"
    BACKEND_HNSW = "hnsw"
    BACKEND_NUMPY = "numpy"
    
    def __init__(self, 
                 dimension: int = 768, 
                 backend: str = None, 
                 index_path: Optional[str] = None,
                 metric: str = "cosine",
                 use_gpu: bool = False):
        """
        Initialize the vector store.
        
        Args:
            dimension: Vector dimension
            backend: Vector store backend (faiss, annoy, hnsw, numpy, or None for auto)
            index_path: Path to save/load index
            metric: Distance metric (cosine, l2, dot)
            use_gpu: Whether to use GPU for FAISS (if available)
        """
        self.dimension = dimension
        self.metric = metric
        self.index_path = index_path
        self.use_gpu = use_gpu
        
        # Determine available backends
        available_backends = {
            self.BACKEND_NUMPY: True,  # Always available
            self.BACKEND_FAISS: FAISS_AVAILABLE,
            self.BACKEND_ANNOY: ANNOY_AVAILABLE,
            self.BACKEND_HNSW: HNSW_AVAILABLE
        }
        
        # Auto-select backend if not specified
        if backend is None:
            # Prefer FAISS > HNSW > Annoy > NumPy
            if available_backends[self.BACKEND_FAISS]:
                backend = self.BACKEND_FAISS
            elif available_backends[self.BACKEND_HNSW]:
                backend = self.BACKEND_HNSW
            elif available_backends[self.BACKEND_ANNOY]:
                backend = self.BACKEND_ANNOY
            else:
                backend = self.BACKEND_NUMPY
                
        # Verify selected backend is available
        if not available_backends.get(backend, False):
            logger.warning(f"Selected backend {backend} not available. Falling back to numpy.")
            backend = self.BACKEND_NUMPY
            
        self.backend = backend
        logger.info(f"Using {backend} vector store backend")
        
        # Initialize backend-specific attributes
        self.index = None
        self.vectors = []
        self.metadata = []
        self.id_to_index = {}  # Maps document IDs to vector indices
        self.index_to_id = {}  # Maps vector indices to document IDs
        self.is_trained = False
        
        # Initialize backend
        self._init_backend()
        
        # Load index if path provided and file exists
        if index_path and os.path.exists(index_path):
            self.load(index_path)
    
    def _init_backend(self):
        """Initialize the selected backend index."""
        if self.backend == self.BACKEND_FAISS:
            self._init_faiss()
        elif self.backend == self.BACKEND_ANNOY:
            self._init_annoy()
        elif self.backend == self.BACKEND_HNSW:
            self._init_hnsw()
        else:  # numpy
            self._init_numpy()
    
    def _init_faiss(self):
        """Initialize FAISS index."""
        # Configure FAISS index based on metric
        if self.metric == "cosine":
            # Normalize vectors and use L2 for cosine similarity
            self.index = faiss.IndexFlatL2(self.dimension)
        elif self.metric == "l2":
            self.index = faiss.IndexFlatL2(self.dimension)
        elif self.metric == "dot":
            self.index = faiss.IndexFlatIP(self.dimension)
        else:
            raise ValueError(f"Unsupported metric for FAISS: {self.metric}")
            
        # Use GPU if requested and available
        if self.use_gpu and TORCH_AVAILABLE and torch.cuda.is_available():
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                logger.info("Using GPU for FAISS index")
            except Exception as e:
                logger.warning(f"GPU requested but failed to use: {str(e)}")
    
    def _init_annoy(self):
        """Initialize Annoy index."""
        # Map metrics to Annoy distance types
        metric_map = {
            "cosine": "angular",
            "l2": "euclidean",
            "dot": "dot"
        }
        annoy_metric = metric_map.get(self.metric, "angular")
        
        self.index = AnnoyIndex(self.dimension, annoy_metric)
        # Annoy requires explicit building, so we'll do that after adding vectors
    
    def _init_hnsw(self):
        """Initialize HNSW index."""
        # Map metrics to HNSW space types
        metric_map = {
            "cosine": "cosine",
            "l2": "l2",
            "dot": "ip"  # Inner product for dot product
        }
        hnsw_metric = metric_map.get(self.metric, "cosine")
        
        self.index = hnswlib.Index(space=hnsw_metric, dim=self.dimension)
        # HNSW requires explicit initialization with max elements
        # We'll initialize it when adding vectors
    
    def _init_numpy(self):
        """Initialize NumPy-based index (just a list of vectors)."""
        self.vectors = []  # No special initialization needed
    
    def add(self, 
            vectors: Union[List[List[float]], np.ndarray], 
            ids: List[str],
            metadata: Optional[List[Dict[str, Any]]] = None,
            update: bool = False) -> None:
        """
        Add vectors to the index.
        
        Args:
            vectors: List of vectors to add
            ids: List of document IDs corresponding to vectors
            metadata: Optional metadata for each vector
            update: Whether to update existing vectors with same IDs
        """
        if len(vectors) == 0:
            return
            
        if len(vectors) != len(ids):
            raise ValueError(f"Vectors and IDs must have same length. Got {len(vectors)} vectors and {len(ids)} IDs.")
            
        if metadata is not None and len(metadata) != len(vectors):
            raise ValueError(f"Metadata must have same length as vectors. Got {len(metadata)} metadata and {len(vectors)} vectors.")
            
        # Convert to numpy array
        if not isinstance(vectors, np.ndarray):
            vectors = np.array(vectors, dtype=np.float32)
            
        # Check for NaN values
        if np.isnan(vectors).any():
            logger.warning("NaN values detected in vectors. Replacing with zeros.")
            vectors = np.nan_to_num(vectors)
            
        # Prepare metadata
        if metadata is None:
            metadata = [{} for _ in range(len(vectors))]
            
        # Track new and updated indices
        new_indices = []
        updated_indices = []
        
        # Add vectors based on backend
        if self.backend == self.BACKEND_FAISS:
            self._add_faiss(vectors, ids, metadata, update, new_indices, updated_indices)
        elif self.backend == self.BACKEND_ANNOY:
            self._add_annoy(vectors, ids, metadata, update, new_indices, updated_indices)
        elif self.backend == self.BACKEND_HNSW:
            self._add_hnsw(vectors, ids, metadata, update, new_indices, updated_indices)
        else:  # numpy
            self._add_numpy(vectors, ids, metadata, update, new_indices, updated_indices)
            
        # Save if index path provided
        if self.index_path:
            self.save(self.index_path)
            
        logger.info(f"Added {len(new_indices)} new vectors and updated {len(updated_indices)} existing vectors to {self.backend} index")
    
    def _add_faiss(self, vectors, ids, metadata, update, new_indices, updated_indices):
        """Add vectors to FAISS index."""
        # Normalize vectors for cosine similarity
        if self.metric == "cosine":
            vectors_normalized = vectors.copy()
            faiss.normalize_L2(vectors_normalized)
        else:
            vectors_normalized = vectors
            
        # Process each vector
        for i, (vec, doc_id, meta) in enumerate(zip(vectors_normalized, ids, metadata)):
            # Check if ID already exists
            if doc_id in self.id_to_index:
                if update:
                    # Update existing vector
                    idx = self.id_to_index[doc_id]
                    self.metadata[idx] = meta
                    updated_indices.append(idx)
                    # Note: FAISS doesn't support updating individual vectors
                    # We'll need to rebuild the index later
                else:
                    # Skip if not updating
                    continue
            else:
                # Add new vector
                idx = len(self.metadata)
                self.id_to_index[doc_id] = idx
                self.index_to_id[idx] = doc_id
                self.metadata.append(meta)
                new_indices.append(idx)
                
        # Add new vectors to FAISS index
        if new_indices:
            new_vecs = vectors_normalized[[i - len(self.metadata) + len(new_indices) for i in new_indices]]
            self.index.add(new_vecs)
            
        # If we have updated vectors, we need to rebuild the index
        if updated_indices:
            logger.warning("FAISS doesn't support efficient vector updates. Rebuilding index...")
            # Get all vectors and rebuild
            all_vecs = []
            for doc_id in self.id_to_index:
                idx = self.id_to_index[doc_id]
                if idx in updated_indices:
                    # Use updated vector
                    vec_idx = ids.index(doc_id)
                    all_vecs.append(vectors_normalized[vec_idx])
                else:
                    # TODO: This is a limitation - we'd need to store all vectors
                    # For now, we'll just rebuild with new and updated vectors
                    all_vecs.append(np.zeros(self.dimension, dtype=np.float32))
                    
            # Reset and rebuild index
            self._init_faiss()
            if all_vecs:
                self.index.add(np.array(all_vecs, dtype=np.float32))
    
    def _add_annoy(self, vectors, ids, metadata, update, new_indices, updated_indices):
        """Add vectors to Annoy index."""
        # Process each vector
        for i, (vec, doc_id, meta) in enumerate(zip(vectors, ids, metadata)):
            # Check if ID already exists
            if doc_id in self.id_to_index:
                if update:
                    # Update existing vector
                    idx = self.id_to_index[doc_id]
                    self.metadata[idx] = meta
                    updated_indices.append(idx)
                    # Annoy doesn't support updates, so we'll need to rebuild
                else:
                    # Skip if not updating
                    continue
            else:
                # Add new vector
                idx = len(self.metadata)
                self.id_to_index[doc_id] = idx
                self.index_to_id[idx] = doc_id
                self.metadata.append(meta)
                new_indices.append(idx)
                
        # Annoy requires rebuilding for any changes
        if new_indices or updated_indices:
            # Create new index
            self._init_annoy()
            
            # Add all vectors
            all_vectors = []
            for idx in range(len(self.metadata)):
                doc_id = self.index_to_id[idx]
                
                if doc_id in ids:
                    # Use provided vector
                    vec_idx = ids.index(doc_id)
                    vec = vectors[vec_idx]
                else:
                    # Use dummy vector (we should store all vectors in practice)
                    vec = np.zeros(self.dimension, dtype=np.float32)
                    
                self.index.add_item(idx, vec)
                all_vectors.append(vec)
                
            # Build the index with reasonable defaults
            n_trees = min(50, len(self.metadata))  # More trees = better accuracy but slower build
            self.index.build(n_trees)
    
    def _add_hnsw(self, vectors, ids, metadata, update, new_indices, updated_indices):
        """Add vectors to HNSW index."""
        # Initialize index if this is the first add
        if not hasattr(self.index, 'max_elements') or self.index.max_elements == 0:
            # Initialize with room to grow
            initial_capacity = max(len(vectors) * 2, 1000)
            self.index.init_index(max_elements=initial_capacity, ef_construction=200, M=16)
            
        # Process each vector
        for i, (vec, doc_id, meta) in enumerate(zip(vectors, ids, metadata)):
            # Check if ID already exists
            if doc_id in self.id_to_index:
                if update:
                    # Update existing vector
                    idx = self.id_to_index[doc_id]
                    self.metadata[idx] = meta
                    updated_indices.append(idx)
                    self.index.add_items(np.array([vec]), [idx])
                else:
                    # Skip if not updating
                    continue
            else:
                # Add new vector
                idx = len(self.metadata)
                self.id_to_index[doc_id] = idx
                self.index_to_id[idx] = doc_id
                self.metadata.append(meta)
                new_indices.append(idx)
                self.index.add_items(np.array([vec]), [idx])
                
        # If we've filled more than 80% of capacity, resize
        if len(self.metadata) > self.index.max_elements * 0.8:
            new_capacity = max(self.index.max_elements * 2, len(self.metadata) * 2)
            logger.info(f"Resizing HNSW index to {new_capacity} elements")
            self.index.resize_index(new_capacity)
    
    def _add_numpy(self, vectors, ids, metadata, update, new_indices, updated_indices):
        """Add vectors to NumPy-based index."""
        # Process each vector
        for i, (vec, doc_id, meta) in enumerate(zip(vectors, ids, metadata)):
            # Check if ID already exists
            if doc_id in self.id_to_index:
                if update:
                    # Update existing vector
                    idx = self.id_to_index[doc_id]
                    self.vectors[idx] = vec
                    self.metadata[idx] = meta
                    updated_indices.append(idx)
                else:
                    # Skip if not updating
                    continue
            else:
                # Add new vector
                idx = len(self.vectors)
                self.id_to_index[doc_id] = idx
                self.index_to_id[idx] = doc_id
                self.vectors.append(vec)
                self.metadata.append(meta)
                new_indices.append(idx)
    
    def search(self,
               query_vector: Union[List[float], np.ndarray],
               k: int = 10,
               filter_func: Optional[Callable] = None) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        Search for similar vectors.
        
        Args:
            query_vector: Query vector
            k: Number of results to return
            filter_func: Optional function to filter results
            
        Returns:
            List of tuples (doc_id, score, metadata)
        """
        if not self.is_trained and not self.vectors and not self.metadata:
            return []
            
        # Convert to numpy array
        if not isinstance(query_vector, np.ndarray):
            query_vector = np.array(query_vector, dtype=np.float32)
            
        # Reshape to 2D if needed
        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)
            
        # Check for NaN values
        if np.isnan(query_vector).any():
            logger.warning("NaN values detected in query vector. Replacing with zeros.")
            query_vector = np.nan_to_num(query_vector)
            
        # Normalize for cosine similarity
        if self.metric == "cosine":
            if self.backend == self.BACKEND_FAISS:
                query_vector_copy = query_vector.copy()
                faiss.normalize_L2(query_vector_copy)
                query_vector = query_vector_copy
            else:
                norm = np.linalg.norm(query_vector)
                if norm > 0:
                    query_vector = query_vector / norm
                    
        # Search based on backend
        if self.backend == self.BACKEND_FAISS:
            return self._search_faiss(query_vector, k, filter_func)
        elif self.backend == self.BACKEND_ANNOY:
            return self._search_annoy(query_vector, k, filter_func)
        elif self.backend == self.BACKEND_HNSW:
            return self._search_hnsw(query_vector, k, filter_func)
        else:  # numpy
            return self._search_numpy(query_vector, k, filter_func)
    
    def _search_faiss(self, query_vector, k, filter_func):
        """Search using FAISS index."""
        # Get more results in case we need to filter
        extra_k = k * 5 if filter_func else k
        extra_k = min(extra_k, self.index.ntotal)
        
        # Perform search
        distances, indices = self.index.search(query_vector, extra_k)
        
        # Process results
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            # Skip invalid indices
            if idx == -1 or idx >= len(self.metadata):
                continue
                
            # Get document ID and metadata
            doc_id = self.index_to_id.get(idx)
            if not doc_id:
                continue
                
            metadata = self.metadata[idx]
            
            # Skip if filtered out
            if filter_func and not filter_func(doc_id, metadata):
                continue
                
            # Convert distance to similarity score
            if self.metric in ["cosine", "dot"]:
                # For inner product and cosine, higher is better
                # But FAISS returns negative values for inner product
                score = 1.0 - distance / 2 if self.metric == "cosine" else -distance
            else:
                # For L2, lower is better, convert to similarity
                score = 1.0 / (1.0 + distance)
                
            results.append((doc_id, float(score), metadata))
            
            # Stop if we have enough results
            if len(results) >= k:
                break
                
        return results
    
    def _search_annoy(self, query_vector, k, filter_func):
        """Search using Annoy index."""
        # Get more results in case we need to filter
        extra_k = k * 5 if filter_func else k
        extra_k = min(extra_k, len(self.metadata))
        
        # Perform search
        indices, distances = self.index.get_nns_by_vector(
            query_vector[0],
            extra_k,
            include_distances=True
        )
        
        # Process results
        results = []
        for idx, distance in zip(indices, distances):
            # Get document ID and metadata
            doc_id = self.index_to_id.get(idx)
            if not doc_id:
                continue
                
            metadata = self.metadata[idx]
            
            # Skip if filtered out
            if filter_func and not filter_func(doc_id, metadata):
                continue
                
            # Convert distance to similarity score
            if self.metric == "cosine":
                # For angular distance in Annoy, closer to 0 is more similar
                score = 1.0 - distance
            elif self.metric == "dot":
                # For dot product in Annoy, higher is better
                score = distance
            else:
                # For L2, lower is better, convert to similarity
                score = 1.0 / (1.0 + distance)
                
            results.append((doc_id, float(score), metadata))
            
            # Stop if we have enough results
            if len(results) >= k:
                break
                
        return results
    
    def _search_hnsw(self, query_vector, k, filter_func):
        """Search using HNSW index."""
        # Get more results in case we need to filter
        extra_k = k * 5 if filter_func else k
        extra_k = min(extra_k, len(self.metadata))
        
        # Perform search
        labels, distances = self.index.knn_query(query_vector, k=extra_k)
        
        # Process results
        results = []
        for idx_list, distance_list in zip(labels, distances):
            for idx, distance in zip(idx_list, distance_list):
                # Skip invalid indices
                if idx >= len(self.metadata):
                    continue
                    
                # Get document ID and metadata
                doc_id = self.index_to_id.get(idx)
                if not doc_id:
                    continue
                    
                metadata = self.metadata[idx]
                
                # Skip if filtered out
                if filter_func and not filter_func(doc_id, metadata):
                    continue
                    
                # Convert distance to similarity score
                if self.metric == "cosine":
                    # For cosine distance in HNSW, closer to 0 is more similar
                    score = 1.0 - distance
                elif self.metric == "dot":
                    # For inner product in HNSW, higher is better
                    # But HNSW returns negative values for inner product
                    score = -distance
                else:
                    # For L2, lower is better, convert to similarity
                    score = 1.0 / (1.0 + distance)
                    
                results.append((doc_id, float(score), metadata))
                
                # Stop if we have enough results
                if len(results) >= k:
                    break
                    
        return results[:k]
    
    def _search_numpy(self, query_vector, k, filter_func):
        """Search using NumPy-based index."""
        if not self.vectors:
            return []
            
        # Perform search
        results = []
        
        # Convert list of vectors to array for faster computation
        vectors_array = np.array(self.vectors, dtype=np.float32)
        
        # Calculate similarities based on metric
        if self.metric == "cosine":
            # Normalize query and vectors
            query_norm = np.linalg.norm(query_vector)
            if query_norm > 0:
                query_vector = query_vector / query_norm
                
            # Normalize each vector
            vector_norms = np.linalg.norm(vectors_array, axis=1, keepdims=True)
            vectors_array = np.divide(vectors_array, vector_norms, 
                                     out=np.zeros_like(vectors_array), 
                                     where=vector_norms>0)
            
            # Calculate dot product for normalized vectors (= cosine similarity)
            similarities = np.dot(vectors_array, query_vector.T).flatten()
            
        elif self.metric == "dot":
            # Calculate dot product
            similarities = np.dot(vectors_array, query_vector.T).flatten()
            
        else:  # l2
            # Calculate L2 distances
            distances = np.linalg.norm(vectors_array - query_vector, axis=1)
            # Convert to similarities (higher is better)
            similarities = 1.0 / (1.0 + distances)
            
        # Get indices sorted by similarity (descending)
        sorted_indices = np.argsort(-similarities)
        
        # Process results
        for idx in sorted_indices:
            # Get document ID and metadata
            doc_id = self.index_to_id.get(idx)
            if not doc_id:
                continue
                
            metadata = self.metadata[idx]
            
            # Skip if filtered out
            if filter_func and not filter_func(doc_id, metadata):
                continue
                
            results.append((doc_id, float(similarities[idx]), metadata))
            
            # Stop if we have enough results
            if len(results) >= k:
                break
                
        return results
    
    def delete(self, ids: List[str]) -> None:
        """
        Delete vectors by IDs.
        
        Args:
            ids: List of document IDs to delete
        """
        if not ids:
            return
            
        deleted_indices = []
        
        for doc_id in ids:
            if doc_id in self.id_to_index:
                idx = self.id_to_index[doc_id]
                deleted_indices.append(idx)
                
                # Remove from mappings
                del self.id_to_index[doc_id]
                del self.index_to_id[idx]
                
                # Mark metadata as deleted
                if idx < len(self.metadata):
                    self.metadata[idx] = {"deleted": True}
                    
        # Handle deletion based on backend
        if deleted_indices:
            if self.backend == self.BACKEND_FAISS:
                # FAISS doesn't support efficient deletion, rebuild if needed
                if len(deleted_indices) > len(self.metadata) * 0.1:
                    self._rebuild_index()
            elif self.backend == self.BACKEND_ANNOY:
                # Annoy doesn't support deletion, rebuild if needed
                if len(deleted_indices) > len(self.metadata) * 0.1:
                    self._rebuild_index()
            elif self.backend == self.BACKEND_HNSW:
                # HNSW supports marking items as deleted
                for idx in deleted_indices:
                    try:
                        self.index.mark_deleted(idx)
                    except Exception as e:
                        logger.warning(f"Error marking HNSW item as deleted: {str(e)}")
            else:  # numpy
                # For NumPy, we can replace vectors with zeros
                for idx in deleted_indices:
                    if idx < len(self.vectors):
                        self.vectors[idx] = np.zeros(self.dimension, dtype=np.float32)
            
            # Save if index path provided
            if self.index_path:
                self.save(self.index_path)
                
            logger.info(f"Deleted {len(deleted_indices)} vectors from {self.backend} index")
    
    def _rebuild_index(self):
        """Rebuild the index after batch deletions."""
        logger.info(f"Rebuilding {self.backend} index...")
        
        # Collect non-deleted vectors
        valid_ids = []
        valid_vectors = []
        valid_metadata = []
        
        for doc_id, idx in self.id_to_index.items():
            if idx < len(self.metadata) and not self.metadata[idx].get("deleted", False):
                valid_ids.append(doc_id)
                if self.backend == self.BACKEND_NUMPY:
                    valid_vectors.append(self.vectors[idx])
                valid_metadata.append(self.metadata[idx])
        
        # Reinitialize
        self._init_backend()
        self.id_to_index = {}
        self.index_to_id = {}
        self.metadata = []
        
        if self.backend == self.BACKEND_NUMPY:
            self.vectors = []
            # Add vectors back
            if valid_vectors:
                self.add(np.array(valid_vectors), valid_ids, valid_metadata)
        else:
            # For other backends, we need to extract vectors from original source
            # This is a limitation: we should store all vectors in practice
            logger.warning("Index rebuilt without vectors. Add vectors back to restore functionality.")
            
            # Just restore metadata and mappings
            for i, (doc_id, meta) in enumerate(zip(valid_ids, valid_metadata)):
                self.id_to_index[doc_id] = i
                self.index_to_id[i] = doc_id
                self.metadata.append(meta)
    
    def save(self, path: str) -> None:
        """
        Save index to disk.
        
        Args:
            path: Path to save the index
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Save metadata and mappings
        metadata_path = f"{path}.metadata.pkl"
        with open(metadata_path, "wb") as f:
            pickle.dump({
                "dimension": self.dimension,
                "backend": self.backend,
                "metric": self.metric,
                "metadata": self.metadata,
                "id_to_index": self.id_to_index,
                "index_to_id": self.index_to_id
            }, f)
            
        # Save backend-specific data
        if self.backend == self.BACKEND_FAISS:
            index_path = f"{path}.faiss"
            faiss.write_index(self.index, index_path)
        elif self.backend == self.BACKEND_ANNOY:
            index_path = f"{path}.annoy"
            self.index.save(index_path)
        elif self.backend == self.BACKEND_HNSW:
            index_path = f"{path}.hnsw"
            self.index.save_index(index_path)
        else:  # numpy
            vectors_path = f"{path}.vectors.pkl"
            with open(vectors_path, "wb") as f:
                pickle.dump(self.vectors, f)
                
        logger.info(f"Saved {self.backend} index to {path}")
    
    def load(self, path: str) -> None:
        """
        Load index from disk.
        
        Args:
            path: Path to load the index from
        """
        # Load metadata and mappings
        metadata_path = f"{path}.metadata.pkl"
        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
            
        with open(metadata_path, "rb") as f:
            data = pickle.load(f)
            
        self.dimension = data["dimension"]
        self.backend = data["backend"]
        self.metric = data["metric"]
        self.metadata = data["metadata"]
        self.id_to_index = data["id_to_index"]
        self.index_to_id = data["index_to_id"]
        
        # Re-initialize backend
        self._init_backend()
        
        # Load backend-specific data
        if self.backend == self.BACKEND_FAISS:
            index_path = f"{path}.faiss"
            if os.path.exists(index_path):
                self.index = faiss.read_index(index_path)
                self.is_trained = True
        elif self.backend == self.BACKEND_ANNOY:
            index_path = f"{path}.annoy"
            if os.path.exists(index_path):
                metric_map = {
                    "cosine": "angular",
                    "l2": "euclidean",
                    "dot": "dot"
                }
                annoy_metric = metric_map.get(self.metric, "angular")
                self.index = AnnoyIndex(self.dimension, annoy_metric)
                self.index.load(index_path)
                self.is_trained = True
        elif self.backend == self.BACKEND_HNSW:
            index_path = f"{path}.hnsw"
            if os.path.exists(index_path):
                metric_map = {
                    "cosine": "cosine",
                    "l2": "l2",
                    "dot": "ip"
                }
                hnsw_metric = metric_map.get(self.metric, "cosine")
                self.index = hnswlib.Index(space=hnsw_metric, dim=self.dimension)
                self.index.load_index(index_path, max_elements=len(self.metadata))
                self.is_trained = True
        else:  # numpy
            vectors_path = f"{path}.vectors.pkl"
            if os.path.exists(vectors_path):
                with open(vectors_path, "rb") as f:
                    self.vectors = pickle.load(f)
                self.is_trained = True
                
        logger.info(f"Loaded {self.backend} index from {path} with {len(self.metadata)} vectors")
    
    def get_size(self) -> int:
        """Get the number of vectors in the index."""
        if self.backend == self.BACKEND_FAISS:
            return self.index.ntotal
        elif self.backend == self.BACKEND_ANNOY:
            return len(self.metadata)  # Annoy doesn't provide a direct way to get size
        elif self.backend == self.BACKEND_HNSW:
            return self.index.element_count
        else:  # numpy
            return len(self.vectors)
    
    def get_metadata(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """Get metadata for a document ID."""
        if doc_id not in self.id_to_index:
            return None
            
        idx = self.id_to_index[doc_id]
        if idx >= len(self.metadata):
            return None
            
        return self.metadata[idx]
    
    def update_metadata(self, doc_id: str, metadata: Dict[str, Any]) -> bool:
        """Update metadata for a document ID."""
        if doc_id not in self.id_to_index:
            return False
            
        idx = self.id_to_index[doc_id]
        if idx >= len(self.metadata):
            return False
            
        # Update metadata
        self.metadata[idx].update(metadata)
        
        # Save if index path provided
        if self.index_path:
            self.save(self.index_path)
            
        return True









BM25 Indexing and Search (bm25_index.py)

"""
BM25 index implementation for lexical search with advanced tokenization,
field weighting, and query expansion capabilities.
"""
import os
import json
import pickle
import logging
import re
import math
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, Set
from collections import Counter, defaultdict
import numpy as np

# Import nltk for tokenization
try:
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    NLTK_AVAILABLE = True
    
    # Download required resources
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    
    # Initialize resources
    STOPWORDS = set(stopwords.words('english'))
    STEMMER = PorterStemmer()
    LEMMATIZER = WordNetLemmatizer()
    
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK not available. Using basic tokenization.")
    STOPWORDS = set()
    STEMMER = None
    LEMMATIZER = None

logger = logging.getLogger(__name__)

class BM25Index:
    """
    BM25 index implementation with field weighting, n-grams, and query expansion.
    """
    
    def __init__(self, 
                 k1: float = 1.5, 
                 b: float = 0.75,
                 tokenizer: str = "word",
                 stemming: bool = True,
                 lemmatize: bool = False,
                 remove_stopwords: bool = True,
                 ngram_range: Tuple[int, int] = (1, 2),
                 min_df: int = 1,
                 max_df: float = 0.9,
                 index_path: Optional[str] = None):
        """
        Initialize BM25 index.
        
        Args:
            k1: BM25 parameter controlling term frequency saturation
            b: BM25 parameter controlling document length normalization
            tokenizer: Tokenization method (word, char, whitespace)
            stemming: Whether to use Porter stemming
            lemmatize: Whether to use WordNet lemmatization
            remove_stopwords: Whether to remove stopwords
            ngram_range: Range of n-grams to include
            min_df: Minimum document frequency for terms
            max_df: Maximum document frequency for terms (as fraction)
            index_path: Path to save/load index
        """
        self.k1 = k1
        self.b = b
        self.tokenizer = tokenizer
        self.stemming = stemming and NLTK_AVAILABLE
        self.lemmatize = lemmatize and NLTK_AVAILABLE
        self.remove_stopwords = remove_stopwords and NLTK_AVAILABLE
        self.ngram_range = ngram_range
        self.min_df = min_df
        self.max_df = max_df
        self.index_path = index_path
        
        # Initialize index structures
        self.documents = {}  # Document content by ID
        self.document_metadata = {}  # Metadata by document ID
        self.document_lengths = {}  # Number of tokens per document
        self.avg_doc_length = 0.0  # Average document length
        self.doc_count = 0  # Number of documents
        self.term_doc_freq = defaultdict(dict)  # Term frequency per document
        self.doc_freq = Counter()  # Document frequency per term
        self.idf = {}  # Inverse document frequency per term
        self.vocabulary = set()  # Set of all terms
        self.field_weights = {}  # Weights for different fields
        
        # Additional structures for optimizations
        self.doc_vectors = {}  # Sparse vectors for documents
        self.term_index = {}  # Posting lists for terms
        
        # Load index if path provided and file exists
        if index_path and os.path.exists(index_path):
            self.load(index_path)
    
    def tokenize(self, text: str) -> List[str]:
        """
        Tokenize text based on configuration.
        
        Args:
            text: Text to tokenize
            
        Returns:
            List of tokens
        """
        if not text:
            return []
            
        # Convert to lowercase
        text = text.lower()
        
        # Basic preprocessing
        text = re.sub(r'[^\w\s]', ' ', text)  # Replace punctuation with space
        
        # Tokenize based on method
        if NLTK_AVAILABLE and self.tokenizer == "word":
            tokens = word_tokenize(text)
        elif self.tokenizer == "char":
            tokens = list(text)
        else:  # whitespace or fallback
            tokens = text.split()
            
        # Remove stopwords if configured
        if self.remove_stopwords and STOPWORDS:
            tokens = [t for t in tokens if t not in STOPWORDS]
            
        # Apply stemming or lemmatization
        if self.stemming and STEMMER:
            tokens = [STEMMER.stem(t) for t in tokens]
        elif self.lemmatize and LEMMATIZER:
            tokens = [LEMMATIZER.lemmatize(t) for t in tokens]
            
        # Generate n-grams if configured
        if self.ngram_range != (1, 1):
            min_n, max_n = self.ngram_range
            original_tokens = tokens.copy()
            
            # Add n-grams
            for n in range(min_n, min(max_n + 1, len(original_tokens) + 1)):
                if n > 1:  # Skip unigrams as they're already included
                    ngrams = [' '.join(original_tokens[i:i+n]) for i in range(len(original_tokens) - n + 1)]
                    tokens.extend(ngrams)
            
        return tokens
    
    def add_document(self, 
                     doc_id: str, 
                     content: Union[str, Dict[str, str]],
                     metadata: Optional[Dict[str, Any]] = None,
                     field_weights: Optional[Dict[str, float]] = None) -> None:
        """
        Add a document to the index.
        
        Args:
            doc_id: Document ID
            content: Document content (text or dict of field->text)
            metadata: Optional metadata
            field_weights: Optional field weights
        """
        # Process document content
        if isinstance(content, str):
            # Single field document
            self.documents[doc_id] = {"text": content}
            field_content = {"text": content}
        else:
            # Multi-field document
            self.documents[doc_id] = content
            field_content = content
            
        # Store metadata
        self.document_metadata[doc_id] = metadata or {}
        
        # Store or update field weights
        if field_weights:
            for field, weight in field_weights.items():
                self.field_weights[field] = weight
                
        # Default field weight
        if not self.field_weights:
            self.field_weights = {"text": 1.0}
            
        # Process field tokens
        field_tokens = {}
        total_length = 0
        
        for field, text in field_content.items():
            tokens = self.tokenize(text)
            field_tokens[field] = tokens
            
            # Weight token count by field weight
            field_weight = self.field_weights.get(field, 1.0)
            total_length += len(tokens) * field_weight
            
        # Store document length
        self.document_lengths[doc_id] = total_length
        
        # Update document count and average length
        self.doc_count += 1
        self.avg_doc_length = sum(self.document_lengths.values()) / self.doc_count
        
        # Update term frequencies and document frequencies
        doc_terms = set()
        
        for field, tokens in field_tokens.items():
            field_weight = self.field_weights.get(field, 1.0)
            
            # Count terms in this field
            field_term_freq = Counter(tokens)
            
            # Update term-document frequencies with field weighting
            for term, freq in field_term_freq.items():
                # Weighted frequency
                weighted_freq = freq * field_weight
                
                if term not in self.term_doc_freq[doc_id]:
                    self.term_doc_freq[doc_id][term] = weighted_freq
                else:
                    self.term_doc_freq[doc_id][term] += weighted_freq
                    
                doc_terms.add(term)
                self.vocabulary.add(term)
                
                # Update inverted index (posting list)
                if term not in self.term_index:
                    self.term_index[term] = []
                self.term_index[term].append(doc_id)
        
        # Update document frequencies
        for term in doc_terms:
            self.doc_freq[term] += 1
        
        # Recalculate IDF for all terms
        self._calculate_idf()
        
        # Create document vector
        self.doc_vectors[doc_id] = self._create_doc_vector(doc_id)
        
        # Save index if path provided
        if self.index_path:
            self.save(self.index_path)
    
    def add_documents(self, 
                      documents: Dict[str, Union[str, Dict[str, str]]],
                      metadata: Optional[Dict[str, Dict[str, Any]]] = None,
                      field_weights: Optional[Dict[str, float]] = None) -> None:
        """
        Add multiple documents to the index.
        
        Args:
            documents: Dict of document_id -> content
            metadata: Optional dict of document_id -> metadata
            field_weights: Optional field weights
        """
        # Process each document
        for doc_id, content in documents.items():
            doc_metadata = metadata.get(doc_id, {}) if metadata else {}
            self.add_document(doc_id, content, doc_metadata, field_weights)
    
    def _calculate_idf(self) -> None:
        """Calculate inverse document frequency for all terms."""
        for term, df in self.doc_freq.items():
            # Standard BM25 IDF formula
            idf = math.log((self.doc_count - df + 0.5) / (df + 0.5) + 1.0)
            self.idf[term] = max(0.0, idf)  # Ensure non-negative
    
    def _create_doc_vector(self, doc_id: str) -> Dict[str, float]:
        """Create a sparse BM25 vector for a document."""
        if doc_id not in self.term_doc_freq:
            return {}
            
        doc_length = self.document_lengths[doc_id]
        
        # Calculate BM25 score for each term in the document
        vector = {}
        for term, freq in self.term_doc_freq[doc_id].items():
            # BM25 term weight formula
            numerator = freq * (self.k1 + 1)
            denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)
            vector[term] = numerator / denominator
            
        return vector
    
    def search(self, 
               query: str, 
               k: int = 10,
               fields: Optional[List[str]] = None,
               boost_fields: Optional[Dict[str, float]] = None,
               filter_func: Optional[Callable] = None,
               use_query_expansion: bool = False,
               expansion_terms: int = 5,
               expansion_docs: int = 3) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        Search for documents matching the query.
        
        Args:
            query: Search query
            k: Number of results to return
            fields: Fields to search in (default: all)
            boost_fields: Additional field boosts for this query
            filter_func: Optional function to filter results
            use_query_expansion: Whether to use query expansion
            expansion_terms: Number of terms to add in query expansion
            expansion_docs: Number of top docs to use for expansion
            
        Returns:
            List of tuples (doc_id, score, metadata)
        """
        if not self.documents:
            return []
            
        # Tokenize query
        query_tokens = self.tokenize(query)
        if not query_tokens:
            return []
            
        # Apply query expansion if requested
        if use_query_expansion and self.doc_count > expansion_docs:
            query_tokens = self._expand_query(query_tokens, expansion_terms, expansion_docs)
            
        # Create query vector
        query_vector = Counter(query_tokens)
        
        # Calculate query-document scores
        scores = {}
        
        # Optimize by only considering documents that contain query terms
        candidate_docs = self._get_candidate_docs(query_tokens)
        
        for doc_id in candidate_docs:
            # Apply filter if provided
            if filter_func and not filter_func(doc_id, self.document_metadata.get(doc_id, {})):
                continue
                
            # Calculate BM25 score
            score = self._score_document(doc_id, query_vector, fields, boost_fields)
            if score > 0:
                scores[doc_id] = score
        
        # Sort by score (descending)
        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]
        
        # Format results
        results = []
        for doc_id, score in sorted_results:
            results.append((doc_id, score, self.document_metadata.get(doc_id, {})))
            
        return results
    
    def _get_candidate_docs(self, query_tokens: List[str]) -> Set[str]:
        """Get candidate documents containing at least one query term."""
        candidate_docs = set()
        
        for term in query_tokens:
            if term in self.term_index:
                candidate_docs.update(self.term_index[term])
                
        return candidate_docs
    
    def _score_document(self, 
                       doc_id: str, 
                       query_vector: Counter,
                       fields: Optional[List[str]] = None,
                       boost_fields: Optional[Dict[str, float]] = None) -> float:
        """Calculate BM25 score between a query and document."""
        if doc_id not in self.document_lengths:
            return 0.0
            
        # Apply field restrictions and boosts
        applied_boosts = {}
        
        # Start with default field weights
        for field, weight in self.field_weights.items():
            applied_boosts[field] = weight
            
        # Apply query-specific boosts
        if boost_fields:
            for field, boost in boost_fields.items():
                applied_boosts[field] = applied_boosts.get(field, 1.0) * boost
                
        # Apply field restrictions
        if fields:
            for field in list(applied_boosts.keys()):
                if field not in fields:
                    applied_boosts[field] = 0.0
        
        # Calculate score
        score = 0.0
        doc_vector = self.doc_vectors.get(doc_id, {})
        
        for term, query_freq in query_vector.items():
            if term in doc_vector and term in self.idf:
                term_score = self.idf[term] * doc_vector[term] * query_freq
                score += term_score
                
        return score
    
    def _expand_query(self, 
                     query_tokens: List[str], 
                     expansion_terms: int = 5,
                     expansion_docs: int = 3) -> List[str]:
        """
        Expand query using pseudo-relevance feedback.
        
        Args:
            query_tokens: Original query tokens
            expansion_terms: Number of terms to add
            expansion_docs: Number of top docs to use
            
        Returns:
            Expanded list of query tokens
        """
        # Create initial query vector
        query_vector = Counter(query_tokens)
        
        # Get top documents for initial query
        candidate_docs = self._get_candidate_docs(query_tokens)
        doc_scores = {}
        
        for doc_id in candidate_docs:
            score = self._score_document(doc_id, query_vector)
            if score > 0:
                doc_scores[doc_id] = score
                
        top_docs = [doc_id for doc_id, _ in sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:expansion_docs]]
        if not top_docs:
            return query_tokens
            
        # Count term frequencies in top documents
        term_freq = Counter()
        for doc_id in top_docs:
            for term, freq in self.term_doc_freq.get(doc_id, {}).items():
                # Skip original query terms
                if term not in query_tokens:
                    term_freq[term] += freq
                    
        # Select top expansion terms
        expansion_tokens = [term for term, _ in term_freq.most_common(expansion_terms)]
        
        # Combine with original query
        return query_tokens + expansion_tokens
    
    def get_document(self, doc_id: str) -> Optional[Dict[str, str]]:
        """Get document content by ID."""
        return self.documents.get(doc_id)
    
    def get_document_metadata(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """Get document metadata by ID."""
        return self.document_metadata.get(doc_id)
    
    def delete_document(self, doc_id: str) -> bool:
        """
        Delete a document from the index.
        
        Args:
            doc_id: Document ID
            
        Returns:
            True if document was deleted, False otherwise
        """
        if doc_id not in self.documents:
            return False
            
        # Remove document from structures
        document_terms = set(self.term_doc_freq.get(doc_id, {}).keys())
        
        # Update document count and average length
        self.doc_count -= 1
        if self.doc_count > 0:
            old_length = self.document_lengths.pop(doc_id, 0)
            self.avg_doc_length = (self.avg_doc_length * (self.doc_count + 1) - old_length) / self.doc_count
        else:
            self.document_lengths.pop(doc_id, None)
            self.avg_doc_length = 0
            
        # Update document frequencies
        for term in document_terms:
            self.doc_freq[term] -= 1
            
            # Remove term from vocabulary if no longer used
            if self.doc_freq[term] <= 0:
                self.vocabulary.discard(term)
                self.doc_freq.pop(term, None)
                self.idf.pop(term, None)
                self.term_index.pop(term, None)
            else:
                # Update posting list
                if term in self.term_index:
                    self.term_index[term] = [d for d in self.term_index[term] if d != doc_id]
        
        # Remove document data
        self.documents.pop(doc_id, None)
        self.document_metadata.pop(doc_id, None)
        self.term_doc_freq.pop(doc_id, None)
        self.doc_vectors.pop(doc_id, None)
        
        # Recalculate IDF for remaining terms
        self._calculate_idf()
        
        # Save index if path provided
        if self.index_path:
            self.save(self.index_path)
            
        return True
    
    def save(self, path: str) -> None:
        """
        Save index to disk.
        
        Args:
            path: Path to save the index
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
        
        # Prepare data to save
        data = {
            "parameters": {
                "k1": self.k1,
                "b": self.b,
                "tokenizer": self.tokenizer,
                "stemming": self.stemming,
                "lemmatize": self.lemmatize,
                "remove_stopwords": self.remove_stopwords,
                "ngram_range": self.ngram_range,
                "min_df": self.min_df,
                "max_df": self.max_df
            },
            "statistics": {
                "doc_count": self.doc_count,
                "avg_doc_length": self.avg_doc_length,
                "vocabulary_size": len(self.vocabulary)
            },
            "field_weights": self.field_weights,
            "documents": self.documents,
            "document_metadata": self.document_metadata,
            "document_lengths": self.document_lengths,
            "term_doc_freq": dict(self.term_doc_freq),  # Convert defaultdict to dict
            "doc_freq": dict(self.doc_freq),
            "idf": self.idf,
            "vocabulary": list(self.vocabulary),
            "doc_vectors": self.doc_vectors,
            "term_index": self.term_index
        }
        
        # Save to file
        with open(path, "wb") as f:
            pickle.dump(data, f)
            
        logger.info(f"Saved BM25 index to {path} with {self.doc_count} documents and {len(self.vocabulary)} terms")
    
    def load(self, path: str) -> None:
        """
        Load index from disk.
        
        Args:
            path: Path to load the index from
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Index file not found: {path}")
            
        with open(path, "rb") as f:
            data = pickle.load(f)
            
        # Load parameters
        params = data["parameters"]
        self.k1 = params["k1"]
        self.b = params["b"]
        self.tokenizer = params["tokenizer"]
        self.stemming = params["stemming"]
        self.lemmatize = params["lemmatize"]
        self.remove_stopwords = params["remove_stopwords"]
        self.ngram_range = params["ngram_range"]
        self.min_df = params["min_df"]
        self.max_df = params["max_df"]
        
        # Load index structures
        self.field_weights = data["field_weights"]
        self.documents = data["documents"]
        self.document_metadata = data["document_metadata"]
        self.document_lengths = data["document_lengths"]
        self.term_doc_freq = defaultdict(dict)
        
        # Convert dict to defaultdict for term_doc_freq
        for doc_id, terms in data["term_doc_freq"].items():
            self.term_doc_freq[doc_id] = terms
            
        self.doc_freq = Counter(data["doc_freq"])
        self.idf = data["idf"]
        self.vocabulary = set(data["vocabulary"])
        self.doc_vectors = data["doc_vectors"]
        self.term_index = data["term_index"]
        
        # Load statistics
        self.doc_count = data["statistics"]["doc_count"]
        self.avg_doc_length = data["statistics"]["avg_doc_length"]
        
        logger.info(f"Loaded BM25 index from {path} with {self.doc_count} documents and {len(self.vocabulary)} terms")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get index statistics."""
        return {
            "doc_count": self.doc_count,
            "avg_doc_length": self.avg_doc_length,
            "vocabulary_size": len(self.vocabulary),
            "tokenizer": self.tokenizer,
            "stemming": self.stemming,
            "lemmatize": self.lemmatize,
            "remove_stopwords": self.remove_stopwords,
            "ngram_range": self.ngram_range
        }
    
    def get_similar_documents(self, doc_id: str, k: int = 5) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        Find documents similar to a given document.
        
        Args:
            doc_id: Document ID
            k: Number of similar documents to return
            
        Returns:
            List of tuples (doc_id, similarity, metadata)
        """
        if doc_id not in self.documents:
            return []
            
        # Get document content as query
        doc_content = self.documents[doc_id]
        
        if isinstance(doc_content, dict):
            # Combine all fields
            query = " ".join(doc_content.values())
        else:
            query = doc_content
            
        # Search for similar documents
        results = self.search(query, k=k+1)  # +1 because the document itself will be included
        
        # Remove the document itself from results
        results = [(result_id, score, meta) for result_id, score, meta in results if result_id != doc_id]
        
        return results[:k]










Document Processors Package Initialization (__init__.py)



"""
Document processors package for the RAG system.

This package contains components for processing different types of content,
including text documents, tables, images, code blocks, and math formulas.
"""

from processors.document_processor import DocumentProcessor, Document, DocumentContent, ContentType
from processors.chunking import SemanticChunker
from processors.table_processor import TableProcessor
from processors.image_processor import ImageProcessor
from processors.formula_processor import FormulaProcessor

__all__ = [
    'DocumentProcessor', 
    'Document', 
    'DocumentContent', 
    'ContentType',
    'SemanticChunker',
    'TableProcessor',
    'ImageProcessor',
    'FormulaProcessor'
]












Table Extraction and Processing (table_processor.py)

"""
Advanced table processing module for extracting, cleaning, and analyzing tabular data
from various sources including HTML, PDF, and images.
"""
import os
import logging
import re
import json
from typing import Dict, List, Any, Optional, Tuple, Union
import traceback
import tempfile
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# Try to import optional dependencies
try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False
    logger.warning("tabula-py not available. PDF table extraction will be limited.")

try:
    import camelot
    CAMELOT_AVAILABLE = True
except ImportError:
    CAMELOT_AVAILABLE = False
    logger.warning("camelot-py not available. Advanced PDF table extraction will be limited.")

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("pdfplumber not available. PDF table extraction will be limited.")

try:
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logger.warning("pytesseract not available. OCR-based table extraction will be limited.")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logger.warning("OpenCV (cv2) not available. Image-based table detection will be limited.")

class TableProcessor:
    """
    Advanced table processor for extracting, cleaning, and analyzing tabular data.
    """
    
    def __init__(self, use_ocr: bool = True, ocr_lang: str = 'eng'):
        """
        Initialize table processor.
        
        Args:
            use_ocr: Whether to use OCR for image-based tables
            ocr_lang: Language for OCR
        """
        self.use_ocr = use_ocr and OCR_AVAILABLE
        self.ocr_lang = ocr_lang
        
    def extract_tables_from_html(self, html_content: str) -> List[pd.DataFrame]:
        """
        Extract tables from HTML content.
        
        Args:
            html_content: HTML content
            
        Returns:
            List of pandas DataFrames
        """
        if not html_content:
            return []
            
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            tables = []
            
            # Find all table elements
            for table_elem in soup.find_all('table'):
                # Get table rows
                rows = []
                
                # Try to find header row (th elements)
                header_row = []
                th_elements = table_elem.find_all('th')
                if th_elements:
                    for th in th_elements:
                        header_row.append(self._clean_cell_text(th.get_text()))
                else:
                    # Try to get header from first row
                    first_tr = table_elem.find('tr')
                    if first_tr:
                        for td in first_tr.find_all(['td', 'th']):
                            header_row.append(self._clean_cell_text(td.get_text()))
                
                # Process data rows
                for tr in table_elem.find_all('tr'):
                    # Skip row if it was the header
                    if tr == table_elem.find('tr') and not th_elements:
                        continue
                        
                    row_data = []
                    for td in tr.find_all(['td', 'th']):
                        row_data.append(self._clean_cell_text(td.get_text()))
                        
                    if row_data:
                        rows.append(row_data)
                        
                # Create DataFrame
                if rows:
                    if header_row and len(header_row) == len(rows[0]):
                        df = pd.DataFrame(rows, columns=header_row)
                    else:
                        df = pd.DataFrame(rows)
                        
                    # Clean the DataFrame
                    df = self._clean_dataframe(df)
                    tables.append(df)
                    
            return tables
            
        except Exception as e:
            logger.error(f"Error extracting tables from HTML: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
            
    def extract_tables_from_pdf(self, pdf_path: str, pages: Optional[str] = 'all') -> List[Dict[str, Any]]:
        """
        Extract tables from PDF document.
        
        Args:
            pdf_path: Path to PDF file
            pages: Pages to extract tables from (e.g., '1,3-5')
            
        Returns:
            List of dictionaries with DataFrames and metadata
        """
        if not os.path.exists(pdf_path):
            logger.error(f"PDF file not found: {pdf_path}")
            return []
            
        results = []
        
        # Try different extraction methods in order of quality
        if CAMELOT_AVAILABLE:
            camelot_tables = self._extract_with_camelot(pdf_path, pages)
            if camelot_tables:
                results.extend(camelot_tables)
                
        if TABULA_AVAILABLE and (not results or len(results) < 2):
            # Only try tabula if camelot found nothing or just one table
            tabula_tables = self._extract_with_tabula(pdf_path, pages)
            if tabula_tables:
                # If we already have tables from camelot, compare and merge
                if results:
                    # Simple deduplication by comparing shape and first row
                    for tabula_table in tabula_tables:
                        is_duplicate = False
                        for camelot_table in results:
                            if self._are_similar_tables(
                                tabula_table["dataframe"], 
                                camelot_table["dataframe"]
                            ):
                                is_duplicate = True
                                break
                                
                        if not is_duplicate:
                            results.append(tabula_table)
                else:
                    results.extend(tabula_tables)
                    
        if PDFPLUMBER_AVAILABLE and (not results or len(results) < 2):
            # Try pdfplumber as a last resort
            plumber_tables = self._extract_with_pdfplumber(pdf_path, pages)
            if plumber_tables:
                # Deduplication
                for plumber_table in plumber_tables:
                    is_duplicate = False
                    for existing_table in results:
                        if self._are_similar_tables(
                            plumber_table["dataframe"], 
                            existing_table["dataframe"]
                        ):
                            is_duplicate = True
                            break
                            
                    if not is_duplicate:
                        results.append(plumber_table)
        
        return results
        
    def _extract_with_camelot(self, pdf_path: str, pages: str) -> List[Dict[str, Any]]:
        """Extract tables using Camelot."""
        try:
            # Try lattice mode first (for tables with borders)
            tables = camelot.read_pdf(
                pdf_path, 
                pages=pages, 
                flavor='lattice',
                suppress_stdout=True
            )
            
            results = []
            
            for i, table in enumerate(tables):
                # Check table quality
                if table.parsing_report['accuracy'] < 50:  # Skip low-quality tables
                    continue
                    
                # Convert to pandas DataFrame
                df = table.df
                
                # Clean the DataFrame
                df = self._clean_dataframe(df)
                
                # Get table metadata
                metadata = {
                    "page": table.parsing_report['page'],
                    "accuracy": table.parsing_report['accuracy'],
                    "whitespace": table.parsing_report['whitespace'],
                    "order": i,
                    "source": "camelot_lattice"
                }
                
                results.append({
                    "dataframe": df,
                    "metadata": metadata
                })
                
            # If lattice mode didn't find tables, try stream mode
            if not results:
                stream_tables = camelot.read_pdf(
                    pdf_path, 
                    pages=pages, 
                    flavor='stream',
                    suppress_stdout=True
                )
                
                for i, table in enumerate(stream_tables):
                    # Check table quality
                    if table.parsing_report['accuracy'] < 50:
                        continue
                        
                    # Convert to pandas DataFrame
                    df = table.df
                    
                    # Clean the DataFrame
                    df = self._clean_dataframe(df)
                    
                    # Get table metadata
                    metadata = {
                        "page": table.parsing_report['page'],
                        "accuracy": table.parsing_report['accuracy'],
                        "whitespace": table.parsing_report['whitespace'],
                        "order": i,
                        "source": "camelot_stream"
                    }
                    
                    results.append({
                        "dataframe": df,
                        "metadata": metadata
                    })
                    
            return results
            
        except Exception as e:
            logger.error(f"Error extracting tables with Camelot: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
            
    def _extract_with_tabula(self, pdf_path: str, pages: str) -> List[Dict[str, Any]]:
        """Extract tables using Tabula."""
        try:
            # Multiple extraction modes for different table types
            tables_lattice = tabula.read_pdf(
                pdf_path,
                pages=pages,
                multiple_tables=True,
                lattice=True
            )
            
            tables_stream = tabula.read_pdf(
                pdf_path,
                pages=pages,
                multiple_tables=True,
                stream=True
            )
            
            # Combine and deduplicate results
            all_dfs = []
            for i, df in enumerate(tables_lattice):
                if not df.empty:
                    # Clean the DataFrame
                    df = self._clean_dataframe(df)
                    all_dfs.append({
                        "dataframe": df,
                        "metadata": {
                            "source": "tabula_lattice",
                            "order": i
                        }
                    })
                    
            for i, df in enumerate(tables_stream):
                if not df.empty:
                    # Check if this is a duplicate
                    is_duplicate = False
                    df_clean = self._clean_dataframe(df)
                    
                    for existing in all_dfs:
                        if self._are_similar_tables(df_clean, existing["dataframe"]):
                            is_duplicate = True
                            break
                            
                    if not is_duplicate:
                        all_dfs.append({
                            "dataframe": df_clean,
                            "metadata": {
                                "source": "tabula_stream",
                                "order": i + len(tables_lattice)
                            }
                        })
                        
            return all_dfs
            
        except Exception as e:
            logger.error(f"Error extracting tables with Tabula: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
            
    def _extract_with_pdfplumber(self, pdf_path: str, pages: str) -> List[Dict[str, Any]]:
        """Extract tables using PDFPlumber."""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                results = []
                
                # Parse page specification
                if pages == 'all':
                    page_numbers = range(len(pdf.pages))
                else:
                    page_numbers = []
                    for page_spec in pages.split(','):
                        if '-' in page_spec:
                            start, end = page_spec.split('-')
                            page_numbers.extend(range(int(start) - 1, int(end)))
                        else:
                            page_numbers.append(int(page_spec) - 1)
                            
                for page_idx in page_numbers:
                    if page_idx >= len(pdf.pages):
                        continue
                        
                    page = pdf.pages[page_idx]
                    tables = page.extract_tables()
                    
                    for i, table_data in enumerate(tables):
                        # Skip empty tables
                        if not table_data or not table_data[0]:
                            continue
                            
                        # Convert to pandas DataFrame
                        if len(table_data) > 1:
                            df = pd.DataFrame(table_data[1:], columns=table_data[0])
                        else:
                            df = pd.DataFrame(table_data)
                            
                        # Clean the DataFrame
                        df = self._clean_dataframe(df)
                        
                        results.append({
                            "dataframe": df,
                            "metadata": {
                                "page": page_idx + 1,
                                "order": i,
                                "source": "pdfplumber"
                            }
                        })
                        
                return results
                
        except Exception as e:
            logger.error(f"Error extracting tables with PDFPlumber: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
        
    def extract_tables_from_image(self, image_path: str) -> List[Dict[str, Any]]:
        """
        Extract tables from image file.
        
        Args:
            image_path: Path to image file
            
        Returns:
            List of dictionaries with DataFrames and metadata
        """
        if not os.path.exists(image_path):
            logger.error(f"Image file not found: {image_path}")
            return []
            
        if not CV2_AVAILABLE or not self.use_ocr:
            logger.warning("OpenCV or OCR not available for image table extraction")
            return []
            
        try:
            # Load image
            img = cv2.imread(image_path)
            if img is None:
                logger.error(f"Could not read image: {image_path}")
                return []
                
            # Detect tables
            tables = self._detect_tables_in_image(img)
            
            results = []
            for i, table_img in enumerate(tables):
                # Extract text from table using OCR
                table_data = self._extract_table_data_with_ocr(table_img)
                if table_data:
                    # Convert to pandas DataFrame
                    df = pd.DataFrame(table_data)
                    
                    # Clean the DataFrame
                    df = self._clean_dataframe(df)
                    
                    results.append({
                        "dataframe": df,
                        "metadata": {
                            "order": i,
                            "source": "image_ocr"
                        }
                    })
                    
            return results
            
        except Exception as e:
            logger.error(f"Error extracting tables from image: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
            
    def _detect_tables_in_image(self, img) -> List[np.ndarray]:
        """Detect tables in an image using OpenCV."""
        # Convert to grayscale
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Apply thresholding
        _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)
        
        # Detect horizontal lines
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
        horizontal_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel)
        
        # Detect vertical lines
        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))
        vertical_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel)
        
        # Combine lines
        table_mask = cv2.bitwise_or(horizontal_lines, vertical_lines)
        
        # Find contours
        contours, _ = cv2.findContours(table_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Filter and extract table regions
        tables = []
        for contour in contours:
            # Get bounding rectangle
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter by size (minimum size for a table)
            if w > 100 and h > 100:
                # Extract table region
                table_img = img[y:y+h, x:x+w]
                tables.append(table_img)
                
        return tables
        
    def _extract_table_data_with_ocr(self, table_img) -> List[List[str]]:
        """Extract table data from image using OCR."""
        if not OCR_AVAILABLE:
            return []
            
        try:
            # Convert to PIL Image
            table_img_rgb = cv2.cvtColor(table_img, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(table_img_rgb)
            
            # Extract text
            text = pytesseract.image_to_string(pil_img, lang=self.ocr_lang)
            
            # Split into rows
            rows = text.strip().split('\n')
            
            # Process rows to columns
            table_data = []
            for row in rows:
                # Skip empty rows
                if not row.strip():
                    continue
                    
                # Split by whitespace or tabs
                cells = row.split('\t')
                if len(cells) == 1:
                    # Try splitting by multiple spaces
                    cells = re.split(r'\s{2,}', row)
                    
                # Remove empty cells
                cells = [cell.strip() for cell in cells if cell.strip()]
                
                if cells:
                    table_data.append(cells)
                    
            return table_data
            
        except Exception as e:
            logger.error(f"Error extracting table data with OCR: {str(e)}")
            return []
            
    def _clean_cell_text(self, text: str) -> str:
        """Clean text from table cell."""
        if text is None:
            return ""
            
        text = str(text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Remove non-breaking spaces
        text = text.replace('\xa0', ' ')
        
        return text
        
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and normalize a DataFrame."""
        if df.empty:
            return df
            
        # Copy to avoid modifying the original
        df = df.copy()
        
        # Remove unnamed or numeric-only columns
        unnamed_cols = [col for col in df.columns if 'unnamed' in str(col).lower() or str(col).isdigit()]
        if unnamed_cols and len(unnamed_cols) != len(df.columns):
            df = df.drop(columns=unnamed_cols)
            
        # Clean column names
        df.columns = [self._clean_cell_text(col) for col in df.columns]
        
        # Handle duplicate column names
        if df.columns.duplicated().any():
            cols = pd.Series(df.columns)
            for i, col in enumerate(df.columns):
                if cols.loc[cols == col].size > 1:
                    cols.iloc[i] = f"{col}_{i}"
            df.columns = cols
            
        # Remove empty rows
        df = df.dropna(how='all')
        
        # Remove empty columns
        df = df.dropna(axis=1, how='all')
        
        # Clean cell values
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].apply(lambda x: self._clean_cell_text(x) if x is not None else "")
                
        return df
        
    def _are_similar_tables(self, df1: pd.DataFrame, df2: pd.DataFrame, threshold: float = 0.7) -> bool:
        """Check if two tables are similar (likely duplicates)."""
        # Quick size comparison
        if df1.shape != df2.shape:
            return False
            
        # Check column names (if non-numeric)
        if not all(str(col).isdigit() for col in df1.columns) and not all(str(col).isdigit() for col in df2.columns):
            col_sim = sum(c1 == c2 for c1, c2 in zip(df1.columns, df2.columns)) / len(df1.columns)
            if col_sim < threshold:
                return False
                
        # Check a sample of cells
        cell_matches = 0
        cell_total = 0
        
        # Use min to avoid index errors
        for i in range(min(3, len(df1))):
            for j in range(min(3, len(df1.columns))):
                if i < len(df1) and j < len(df1.columns) and i < len(df2) and j < len(df2.columns):
                    val1 = str(df1.iloc[i, j])
                    val2 = str(df2.iloc[i, j])
                    if val1 == val2:
                        cell_matches += 1
                    cell_total += 1
                    
        if cell_total == 0:
            return False
            
        return (cell_matches / cell_total) >= threshold
        
    def analyze_table(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze table content and structure.
        
        Args:
            df: Pandas DataFrame
            
        Returns:
            Dictionary with table analysis
        """
        if df.empty:
            return {"error": "Empty table"}
            
        analysis = {
            "shape": df.shape,
            "columns": list(df.columns),
            "row_count": len(df),
            "column_count": len(df.columns),
            "dtypes": {},
            "has_header": True,  # Assume pandas DataFrames have headers
            "numeric_columns": [],
            "categorical_columns": [],
            "date_columns": [],
            "statistics": {},
            "missing_values": {},
            "sample_rows": []
        }
        
        # Detect data types
        for col in df.columns:
            col_dtype = str(df[col].dtype)
            analysis["dtypes"][col] = col_dtype
            
            # Categorize columns
            if col_dtype in ['int64', 'float64', 'int32', 'float32']:
                analysis["numeric_columns"].append(col)
            elif col_dtype in ['datetime64[ns]', 'timedelta64[ns]']:
                analysis["date_columns"].append(col)
            else:
                # Check if appears to be categorical
                unique_ratio = df[col].nunique() / len(df)
                if unique_ratio < 0.5:  # Less than 50% unique values
                    analysis["categorical_columns"].append(col)
                    
        # Calculate statistics for numeric columns
        for col in analysis["numeric_columns"]:
            analysis["statistics"][col] = {
                "min": float(df[col].min()) if not pd.isna(df[col].min()) else None,
                "max": float(df[col].max()) if not pd.isna(df[col].max()) else None,
                "mean": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,
                "median": float(df[col].median()) if not pd.isna(df[col].median()) else None
            }
            
        # Calculate missing values
        for col in df.columns:
            missing = df[col].isna().sum()
            if missing > 0:
                analysis["missing_values"][col] = {
                    "count": int(missing),
                    "percentage": float(missing / len(df) * 100)
                }
                
        # Get sample rows
        sample_size = min(5, len(df))
        sample_df = df.head(sample_size)
        analysis["sample_rows"] = sample_df.to_dict(orient='records')
        
        return analysis
        
    def table_to_text(self, df: pd.DataFrame, include_stats: bool = True) -> str:
        """
        Convert table to descriptive text.
        
        Args:
            df: Pandas DataFrame
            include_stats: Whether to include table statistics
            
        Returns:
            Textual description of table
        """
        if df.empty:
            return "Empty table"
            
        # Start with basic description
        lines = [f"Table with {len(df)} rows and {len(df.columns)} columns."]
        
        # Add column names
        lines.append(f"Columns: {', '.join(str(col) for col in df.columns)}")
        
        # Add sample rows
        lines.append("\nSample data:")
        sample_size = min(5, len(df))
        for i in range(sample_size):
            row_values = [str(val)[:50] + ('...' if len(str(val)) > 50 else '') for val in df.iloc[i]]
            lines.append(f"Row {i+1}: {' | '.join(row_values)}")
            
        # Add statistics if requested
        if include_stats:
            analysis = self.analyze_table(df)
            
            # Add numeric stats
            if analysis["numeric_columns"]:
                lines.append("\nNumeric column statistics:")
                for col in analysis["numeric_columns"]:
                    stats = analysis["statistics"][col]
                    if stats["min"] is not None and stats["max"] is not None:
                        lines.append(f"{col}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}")
            
            # Add categorical stats
            if analysis["categorical_columns"]:
                lines.append("\nTop values in categorical columns:")
                for col in analysis["categorical_columns"][:3]:  # Limit to 3 columns
                    value_counts = df[col].value_counts().head(3)
                    values_str = ", ".join(f"{val} ({count})" for val, count in value_counts.items())
                    lines.append(f"{col}: {values_str}")
                    
            # Add missing value info
            if analysis["missing_values"]:
                lines.append("\nMissing values:")
                for col, missing in analysis["missing_values"].items():
                    lines.append(f"{col}: {missing['count']} rows ({missing['percentage']:.1f}%)")
        
        return "\n".join(lines)

















Mathematical Formula Processing (formula_processor.py)



"""
Mathematical formula processing for detecting, extracting, and interpreting formulas
from various document types, with support for LaTeX, MathML, and plain text notation.
"""
import re
import logging
from typing import Dict, List, Any, Optional, Tuple, Union, Set
import math
import json

logger = logging.getLogger(__name__)

try:
    import sympy
    from sympy.parsing.sympy_parser import parse_expr, standard_transformations
    SYMPY_AVAILABLE = True
except ImportError:
    SYMPY_AVAILABLE = False
    logger.warning("SymPy not available. Formula processing will be limited.")

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False
    logger.warning("BeautifulSoup not available. MathML processing will be limited.")

class FormulaProcessor:
    """
    Processor for mathematical formulas in various formats.
    """
    
    # Formula format constants
    FORMAT_LATEX = "latex"
    FORMAT_MATHML = "mathml"
    FORMAT_ASCII = "ascii"
    FORMAT_UNKNOWN = "unknown"
    
    def __init__(self):
        """Initialize formula processor."""
        # Common variables used in formulas
        self.common_variables = {
            'x', 'y', 'z', 'a', 'b', 'c', 'n', 'm', 'i', 'j', 'k',
            'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'theta', 'lambda', 'mu', 'sigma', 'tau', 'phi', 'omega',
            't', 'r', 's', 'p', 'q', 'f', 'g', 'h'
        }
        
        # Common functions
        self.common_functions = {
            'sin', 'cos', 'tan', 'exp', 'log', 'ln', 'sqrt', 'abs', 'max', 'min',
            'lim', 'sum', 'prod', 'int', 'sinh', 'cosh', 'tanh'
        }
        
        # Common operators
        self.common_operators = {
            '+', '-', '*', '/', '^', '=', '', '<', '>', '', '', '', '',
            '', '', '', '', '', '', '', '', '', '', '', '', '',
            '', '', '', ''
        }
        
        # Compiled regexes
        self._latex_pattern = re.compile(r'(\$\$|\$|\\begin\{(equation|align|math|display)\})(.*?)(\$\$|\$|\\end\{(equation|align|math|display)\})', re.DOTALL)
        self._ascii_equation_pattern = re.compile(r'([a-zA-Z0-9_]+)\s*=\s*([^=]+)$')
        
        # Initialize sympy parser if available
        if SYMPY_AVAILABLE:
            self.sympy_transformations = standard_transformations
    
    def detect_formula(self, text: str) -> bool:
        """
        Detect if text contains a mathematical formula.
        
        Args:
            text: Text to check
            
        Returns:
            True if text contains a formula, False otherwise
        """
        if not text:
            return False
            
        # Check for LaTeX formulas
        if self._latex_pattern.search(text):
            return True
            
        # Check for MathML
        if BEAUTIFULSOUP_AVAILABLE and ('<math' in text or '<mrow' in text):
            try:
                soup = BeautifulSoup(text, 'html.parser')
                if soup.find('math') or soup.find('mrow'):
                    return True
            except:
                pass
                
        # Check for ASCII math notation
        if self._is_ascii_formula(text):
            return True
            
        return False
    
    def _is_ascii_formula(self, text: str) -> bool:
        """Check if text contains an ASCII formula."""
        text = text.strip()
        
        # Check for equations with equals sign
        if self._ascii_equation_pattern.search(text):
            return True
            
        # Check for formulas with common operators and variables
        # Count math symbols
        symbol_count = 0
        
        # Check for math operators
        for op in self.common_operators:
            if op in text:
                symbol_count += text.count(op)
                
        # Check for function names
        for func in self.common_functions:
            if func in text:
                symbol_count += 1
                
        # Check for variables
        words = re.findall(r'\b[a-zA-Z]+\b', text)
        vars_found = [word for word in words if word in self.common_variables]
        symbol_count += len(vars_found)
        
        # Check for numeric values
        numbers = re.findall(r'\b\d+(?:\.\d+)?(?:[eE][+-]?\d+)?\b', text)
        symbol_count += len(numbers)
        
        # Heuristic: if we have multiple math symbols and at least one variable or function
        return symbol_count >= 3 and (len(vars_found) > 0 or any(func in text for func in self.common_functions))
    
    def extract_formulas(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract formulas from text.
        
        Args:
            text: Text to extract formulas from
            
        Returns:
            List of extracted formulas with metadata
        """
        if not text:
            return []
            
        formulas = []
        
        # Extract LaTeX formulas
        latex_formulas = self._extract_latex_formulas(text)
        formulas.extend(latex_formulas)
        
        # Extract MathML formulas
        if BEAUTIFULSOUP_AVAILABLE:
            mathml_formulas = self._extract_mathml_formulas(text)
            formulas.extend(mathml_formulas)
            
        # If no LaTeX or MathML formulas found, try to extract ASCII formulas
        if not formulas:
            ascii_formulas = self._extract_ascii_formulas(text)
            formulas.extend(ascii_formulas)
            
        return formulas
    
    def _extract_latex_formulas(self, text: str) -> List[Dict[str, Any]]:
        """Extract LaTeX formulas from text."""
        formulas = []
        
        # Find all LaTeX formula patterns
        matches = self._latex_pattern.finditer(text)
        
        for match in matches:
            start_delim = match.group(1)
            end_delim = match.group(4)
            formula_text = match.group(3)
            
            # Determine formula type (inline or display)
            if start_delim in ['$', '\\begin{math}']:
                formula_type = 'inline'
            else:
                formula_type = 'display'
                
            # Clean up formula text
            formula_text = formula_text.strip()
            
            # Extract variables and functions
            variables, functions = self._extract_latex_elements(formula_text)
            
            # Parse formula if possible
            parsed = self._parse_formula(formula_text, self.FORMAT_LATEX)
            
            formulas.append({
                'text': formula_text,
                'format': self.FORMAT_LATEX,
                'type': formula_type,
                'original': match.group(0),
                'variables': list(variables),
                'functions': list(functions),
                'parsed': parsed,
                'position': {'start': match.start(), 'end': match.end()}
            })
            
        return formulas
    
    def _extract_latex_elements(self, latex: str) -> Tuple[Set[str], Set[str]]:
        """Extract variables and functions from LaTeX formula."""
        variables = set()
        functions = set()
        
        # Extract variable-like tokens
        var_matches = re.finditer(r'\\([a-zA-Z]+)|([a-zA-Z])', latex)
        for match in var_matches:
            token = match.group(1) if match.group(1) else match.group(2)
            if token in self.common_variables:
                variables.add(token)
            elif token in self.common_functions:
                functions.add(token)
                
        return variables, functions
    
    def _extract_mathml_formulas(self, text: str) -> List[Dict[str, Any]]:
        """Extract MathML formulas from text."""
        if not BEAUTIFULSOUP_AVAILABLE:
            return []
            
        formulas = []
        
        try:
            soup = BeautifulSoup(text, 'html.parser')
            math_elements = soup.find_all('math')
            
            for i, math_elem in enumerate(math_elements):
                # Extract the formula text
                mathml = str(math_elem)
                
                # Determine if inline or display
                formula_type = 'inline'
                if math_elem.get('display') == 'block':
                    formula_type = 'display'
                    
                # Extract variables and identifiers
                variables = set()
                functions = set()
                
                # Find mi elements (identifiers)
                for mi in math_elem.find_all('mi'):
                    var = mi.get_text().strip()
                    if var in self.common_variables:
                        variables.add(var)
                    elif var in self.common_functions:
                        functions.add(var)
                        
                # Try to parse as LaTeX (simplified)
                latex_equiv = self._mathml_to_latex(math_elem)
                parsed = self._parse_formula(latex_equiv, self.FORMAT_MATHML)
                
                formulas.append({
                    'text': mathml,
                    'format': self.FORMAT_MATHML,
                    'type': formula_type,
                    'original': mathml,
                    'variables': list(variables),
                    'functions': list(functions),
                    'latex_equivalent': latex_equiv,
                    'parsed': parsed,
                    'position': None  # Position info not available with BeautifulSoup
                })
                
        except Exception as e:
            logger.error(f"Error extracting MathML formulas: {str(e)}")
            
        return formulas
    
    def _mathml_to_latex(self, math_elem) -> str:
        """Convert MathML to LaTeX (simplified)."""
        if not math_elem:
            return ""
            
        try:
            # Very simplified conversion
            latex = ""
            
            # Process operators
            for mo in math_elem.find_all('mo'):
                op = mo.get_text().strip()
                latex += op
                
            # Process identifiers
            for mi in math_elem.find_all('mi'):
                var = mi.get_text().strip()
                latex += var
                
            # Process numbers
            for mn in math_elem.find_all('mn'):
                num = mn.get_text().strip()
                latex += num
                
            return latex
        except:
            return ""
    
    def _extract_ascii_formulas(self, text: str) -> List[Dict[str, Any]]:
        """Extract ASCII formulas from text."""
        formulas = []
        
        # Split text into lines
        lines = text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            
            # Skip short lines
            if len(line) < 5:
                continue
                
            # Check if line contains an ASCII formula
            if self._is_ascii_formula(line):
                # Try to extract variables and functions
                variables, functions = self._extract_ascii_elements(line)
                
                # Parse if possible
                parsed = self._parse_formula(line, self.FORMAT_ASCII)
                
                formulas.append({
                    'text': line,
                    'format': self.FORMAT_ASCII,
                    'type': 'inline',
                    'original': line,
                    'variables': list(variables),
                    'functions': list(functions),
                    'parsed': parsed,
                    'position': None  # Position info not tracked for plain text
                })
                
        return formulas
    
    def _extract_ascii_elements(self, text: str) -> Tuple[Set[str], Set[str]]:
        """Extract variables and functions from ASCII formula."""
        variables = set()
        functions = set()
        
        # Extract word-like tokens
        words = re.findall(r'\b[a-zA-Z]+\b', text)
        
        for word in words:
            if word in self.common_variables:
                variables.add(word)
            elif word in self.common_functions:
                functions.add(word)
                
        return variables, functions
    
    def _parse_formula(self, formula: str, format_type: str) -> Optional[Dict[str, Any]]:
        """
        Parse formula using SymPy if available.
        
        Args:
            formula: Formula text
            format_type: Format of formula (latex, mathml, ascii)
            
        Returns:
            Dictionary with parsed formula information or None if parsing fails
        """
        if not SYMPY_AVAILABLE:
            return None
            
        try:
            # For LaTeX, we need to clean up the formula
            if format_type == self.FORMAT_LATEX:
                # Remove LaTeX-specific commands
                clean_formula = formula
                
                # Remove \text{} blocks
                clean_formula = re.sub(r'\\text\{([^}]*)\}', r'\1', clean_formula)
                
                # Replace common LaTeX math commands
                replacements = {
                    r'\\frac{([^}]*)}{([^}]*)}': r'(\1)/(\2)',
                    r'\\cdot': '*',
                    r'\\times': '*',
                    r'\\div': '/',
                    r'\\alpha': 'alpha',
                    r'\\beta': 'beta',
                    r'\\gamma': 'gamma',
                    r'\\sum_{([^}]*)}^{([^}]*)}': r'sum',
                    r'\\int_{([^}]*)}^{([^}]*)}': r'integral',
                    r'\\sqrt{([^}]*)}': r'sqrt(\1)',
                    r'\\left': '',
                    r'\\right': '',
                    r'\\mathbf{([^}]*)}': r'\1',
                    r'\\mathrm{([^}]*)}': r'\1',
                    r'\\begin{array}{[^}]*}': '',
                    r'\\end{array}': '',
                    r'\\\\': ' ',
                    r'&': ' ',
                    r'\\quad': ' ',
                    r'\\qquad': ' '
                }
                
                for pattern, replacement in replacements.items():
                    clean_formula = re.sub(pattern, replacement, clean_formula)
                    
                # Handle subscripts and superscripts
                clean_formula = re.sub(r'([a-zA-Z])_([a-zA-Z0-9])', r'\1\2', clean_formula)
                clean_formula = re.sub(r'([a-zA-Z])\^([a-zA-Z0-9])', r'\1**\2', clean_formula)
                
                # Try to parse with SymPy
                if '=' in clean_formula:
                    # Split equation at equals sign
                    parts = clean_formula.split('=')
                    if len(parts) == 2:
                        lhs = parts[0].strip()
                        rhs = parts[1].strip()
                        
                        # Try parsing each side
                        try:
                            lhs_expr = parse_expr(lhs, transformations=self.sympy_transformations)
                            rhs_expr = parse_expr(rhs, transformations=self.sympy_transformations)
                            
                            return {
                                'type': 'equation',
                                'lhs': str(lhs_expr),
                                'rhs': str(rhs_expr),
                                'symbols': [str(s) for s in lhs_expr.free_symbols.union(rhs_expr.free_symbols)]
                            }
                        except:
                            pass
                else:
                    # Try parsing as expression
                    try:
                        expr = parse_expr(clean_formula, transformations=self.sympy_transformations)
                        return {
                            'type': 'expression',
                            'expression': str(expr),
                            'symbols': [str(s) for s in expr.free_symbols]
                        }
                    except:
                        pass
                            
            elif format_type == self.FORMAT_ASCII:
                # For ASCII, try direct parsing
                if '=' in formula:
                    # Split equation at equals sign
                    parts = formula.split('=')
                    if len(parts) == 2:
                        lhs = parts[0].strip()
                        rhs = parts[1].strip()
                        
                        # Try parsing each side
                        try:
                            lhs_expr = parse_expr(lhs, transformations=self.sympy_transformations)
                            rhs_expr = parse_expr(rhs, transformations=self.sympy_transformations)
                            
                            return {
                                'type': 'equation',
                                'lhs': str(lhs_expr),
                                'rhs': str(rhs_expr),
                                'symbols': [str(s) for s in lhs_expr.free_symbols.union(rhs_expr.free_symbols)]
                            }
                        except:
                            pass
                else:
                    # Try parsing as expression
                    try:
                        expr = parse_expr(formula, transformations=self.sympy_transformations)
                        return {
                            'type': 'expression',
                            'expression': str(expr),
                            'symbols': [str(s) for s in expr.free_symbols]
                        }
                    except:
                        pass
        except Exception as e:
            logger.debug(f"Error parsing formula: {str(e)}")
            
        return None
    
    def evaluate_formula(self, formula: Dict[str, Any], variable_values: Dict[str, float]) -> Optional[Dict[str, Any]]:
        """
        Evaluate a formula with given variable values.
        
        Args:
            formula: Formula dictionary from extract_formulas
            variable_values: Dictionary of variable names to values
            
        Returns:
            Dictionary with evaluation results or None if evaluation fails
        """
        if not SYMPY_AVAILABLE:
            return None
            
        try:
            # Check if formula was successfully parsed
            if not formula.get('parsed'):
                return None
                
            parsed = formula['parsed']
            
            # Handle different formula types
            if parsed['type'] == 'equation':
                # For equations, evaluate both sides
                lhs_expr = sympy.sympify(parsed['lhs'])
                rhs_expr = sympy.sympify(parsed['rhs'])
                
                # Substitute values
                lhs_value = float(lhs_expr.subs(variable_values))
                rhs_value = float(rhs_expr.subs(variable_values))
                
                return {
                    'lhs_value': lhs_value,
                    'rhs_value': rhs_value,
                    'is_equal': math.isclose(lhs_value, rhs_value, rel_tol=1e-9),
                    'difference': lhs_value - rhs_value
                }
                
            elif parsed['type'] == 'expression':
                # For expressions, just evaluate
                expr = sympy.sympify(parsed['expression'])
                
                # Substitute values
                value = float(expr.subs(variable_values))
                
                return {
                    'value': value
                }
                
            return None
            
        except Exception as e:
            logger.error(f"Error evaluating formula: {str(e)}")
            return None
    
    def solve_equation(self, formula: Dict[str, Any], solve_for: str) -> Optional[Dict[str, Any]]:
        """
        Solve an equation for a specific variable.
        
        Args:
            formula: Formula dictionary from extract_formulas
            solve_for: Variable to solve for
            
        Returns:
            Dictionary with solution or None if solving fails
        """
        if not SYMPY_AVAILABLE:
            return None
            
        try:
            # Check if formula was successfully parsed and is an equation
            if not formula.get('parsed') or formula['parsed']['type'] != 'equation':
                return None
                
            parsed = formula['parsed']
            
            # Create equation
            lhs_expr = sympy.sympify(parsed['lhs'])
            rhs_expr = sympy.sympify(parsed['rhs'])
            
            # Create the equation
            equation = sympy.Eq(lhs_expr, rhs_expr)
            
            # Solve for the variable
            solve_var = sympy.Symbol(solve_for)
            solution = sympy.solve(equation, solve_var)
            
            if solution:
                return {
                    'variable': solve_for,
                    'solutions': [str(sol) for sol in solution],
                    'solution_count': len(solution)
                }
                
            return {
                'variable': solve_for,
                'solutions': [],
                'solution_count': 0
            }
            
        except Exception as e:
            logger.error(f"Error solving equation: {str(e)}")
            return None
    
    def formula_to_text(self, formula: Dict[str, Any]) -> str:
        """
        Convert formula to descriptive text.
        
        Args:
            formula: Formula dictionary from extract_formulas
            
        Returns:
            Descriptive text representation of formula
        """
        if not formula:
            return "Invalid formula"
            
        # Start with formula format information
        format_type = formula.get('format', self.FORMAT_UNKNOWN)
        formula_type = formula.get('type', 'inline')
        
        if format_type == self.FORMAT_LATEX:
            text = f"LaTeX formula{' (display mode)' if formula_type == 'display' else ''}: "
        elif format_type == self.FORMAT_MATHML:
            text = f"MathML formula{' (display mode)' if formula_type == 'display' else ''}: "
        elif format_type == self.FORMAT_ASCII:
            text = "Mathematical formula: "
        else:
            text = "Formula: "
            
        # Add the original formula text
        original = formula.get('original', formula.get('text', ''))
        if original:
            text += original + "\n"
            
        # Add parsed information if available
        parsed = formula.get('parsed')
        if parsed:
            if parsed['type'] == 'equation':
                text += f"Equation: {parsed['lhs']} = {parsed['rhs']}\n"
            elif parsed['type'] == 'expression':
                text += f"Expression: {parsed['expression']}\n"
                
            # Add variables/symbols
            symbols = parsed.get('symbols', [])
            if symbols:
                text += f"Variables: {', '.join(symbols)}\n"
        else:
            # If not parsed, add extracted variables and functions
            variables = formula.get('variables', [])
            if variables:
                text += f"Variables: {', '.join(variables)}\n"
                
            functions = formula.get('functions', [])
            if functions:
                text += f"Functions: {', '.join(functions)}\n"
                
        return text.strip()










Mathematical Formula Processor (formula_processor.py)




"""
Advanced processor for handling mathematical formulas in various formats,
including LaTeX, MathML, and ASCII math.
"""
import logging
import re
import os
import tempfile
import io
import base64
from typing import Dict, Any, List, Optional, Tuple, Union, Set
import json
import numpy as np
from pathlib import Path

# Try to import specialized libraries
try:
    import sympy
    SYMPY_AVAILABLE = True
except ImportError:
    SYMPY_AVAILABLE = False
    logging.warning("SymPy not installed. Formula symbolic processing will be limited.")

try:
    from latex2sympy2 import latex2sympy, latex2latex
    LATEX2SYMPY_AVAILABLE = True
except ImportError:
    LATEX2SYMPY_AVAILABLE = False
    logging.warning("latex2sympy2 not installed. LaTeX parsing will be limited.")

try:
    from PIL import Image, ImageDraw, ImageFont
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from matplotlib import mathtext
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    logging.warning("Matplotlib not installed. Formula rendering will be limited.")

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logging.warning("OpenCV not installed. Formula detection in images will be limited.")

logger = logging.getLogger(__name__)

class FormulaFormat:
    """Enumeration of supported formula formats."""
    LATEX = "latex"
    MATHML = "mathml"
    ASCII = "ascii"
    SYMPY = "sympy"
    UNKNOWN = "unknown"

class FormulaType:
    """Enumeration of formula types."""
    EQUATION = "equation"
    EXPRESSION = "expression"
    INLINE = "inline"
    BLOCK = "block"
    MULTI_LINE = "multi_line"

class Formula:
    """
    Class representing a mathematical formula with various representations
    and metadata.
    """
    
    def __init__(self, 
                 content: str,
                 format: str = FormulaFormat.UNKNOWN,
                 formula_type: str = FormulaType.EXPRESSION,
                 metadata: Optional[Dict[str, Any]] = None,
                 source_document: Optional[str] = None,
                 source_page: Optional[int] = None,
                 source_position: Optional[Dict[str, Any]] = None):
        """
        Initialize a formula.
        
        Args:
            content: The formula content in the specified format
            format: Format of the formula content (latex, mathml, ascii, etc.)
            formula_type: Type of formula (equation, expression, etc.)
            metadata: Additional metadata about the formula
            source_document: Source document identifier
            source_page: Page number in source document
            source_position: Position information in source
        """
        self.content = content
        self.format = format
        self.formula_type = formula_type
        self.metadata = metadata or {}
        self.source_document = source_document
        self.source_page = source_page
        self.source_position = source_position or {}
        
        # Representations in different formats (to be populated as needed)
        self.representations = {format: content}
        
        # Symbolic representation (if available)
        self.symbolic = None
        
        # Variables used in the formula
        self.variables = set()
        
        # Constants used in the formula
        self.constants = set()
        
        # Operators used in the formula
        self.operators = set()
        
        # Functions used in the formula
        self.functions = set()
        
        # Text representation for display/search
        self.text_representation = self._generate_text_representation()
        
        # Parse the formula to extract components
        self._parse_formula()
    
    def _generate_text_representation(self) -> str:
        """Generate a plain text representation for display and search."""
        if self.format == FormulaFormat.LATEX:
            # Replace common LaTeX commands with text equivalents
            text = self.content
            
            # Remove LaTeX-specific commands
            text = re.sub(r'\\text\{([^}]*)\}', r'\1', text)  # Extract text
            
            # Replace common math operators
            replacements = {
                r'\\alpha': 'alpha',
                r'\\beta': 'beta',
                r'\\gamma': 'gamma',
                r'\\delta': 'delta',
                r'\\epsilon': 'epsilon',
                r'\\zeta': 'zeta',
                r'\\eta': 'eta',
                r'\\theta': 'theta',
                r'\\iota': 'iota',
                r'\\kappa': 'kappa',
                r'\\lambda': 'lambda',
                r'\\mu': 'mu',
                r'\\nu': 'nu',
                r'\\xi': 'xi',
                r'\\pi': 'pi',
                r'\\rho': 'rho',
                r'\\sigma': 'sigma',
                r'\\tau': 'tau',
                r'\\upsilon': 'upsilon',
                r'\\phi': 'phi',
                r'\\chi': 'chi',
                r'\\psi': 'psi',
                r'\\omega': 'omega',
                r'\\Gamma': 'Gamma',
                r'\\Delta': 'Delta',
                r'\\Theta': 'Theta',
                r'\\Lambda': 'Lambda',
                r'\\Xi': 'Xi',
                r'\\Pi': 'Pi',
                r'\\Sigma': 'Sigma',
                r'\\Phi': 'Phi',
                r'\\Psi': 'Psi',
                r'\\Omega': 'Omega',
                r'\\sum': 'sum',
                r'\\prod': 'product',
                r'\\int': 'integral',
                r'\\iint': 'double integral',
                r'\\iiint': 'triple integral',
                r'\\oint': 'contour integral',
                r'\\partial': 'partial',
                r'\\infty': 'infinity',
                r'\\nabla': 'nabla',
                r'\\forall': 'for all',
                r'\\exists': 'exists',
                r'\\in': 'in',
                r'\\subset': 'subset of',
                r'\\supset': 'superset of',
                r'\\cup': 'union',
                r'\\cap': 'intersection',
                r'\\emptyset': 'empty set',
                r'\\mathbb\{R\}': 'real numbers',
                r'\\mathbb\{Z\}': 'integers',
                r'\\mathbb\{N\}': 'natural numbers',
                r'\\mathbb\{Q\}': 'rational numbers',
                r'\\mathbb\{C\}': 'complex numbers',
                r'\\rightarrow': 'right arrow',
                r'\\Rightarrow': 'implies',
                r'\\Leftrightarrow': 'if and only if',
                r'\\approx': 'approximately equal to',
                r'\\neq': 'not equal to',
                r'\\equiv': 'equivalent to',
                r'\\leq': 'less than or equal to',
                r'\\geq': 'greater than or equal to',
                r'\\times': 'times',
                r'\\div': 'divided by',
                r'\\cdot': 'dot',
                r'\\frac\{([^}]*)\}\{([^}]*)\}': r'\1 divided by \2',
                r'\\sqrt\{([^}]*)\}': r'square root of \1',
                r'\\sqrt\[([^]]*)\]\{([^}]*)\}': r'\1 root of \2',
                r'\^([0-9])': r' to the power of \1',
                r'_([0-9])': r' subscript \1',
                r'\\log': 'logarithm',
                r'\\ln': 'natural logarithm',
                r'\\exp': 'exponential',
                r'\\sin': 'sine',
                r'\\cos': 'cosine',
                r'\\tan': 'tangent',
                r'\\cot': 'cotangent',
                r'\\sec': 'secant',
                r'\\csc': 'cosecant',
                r'\\arcsin': 'arcsine',
                r'\\arccos': 'arccosine',
                r'\\arctan': 'arctangent',
                r'\\sinh': 'hyperbolic sine',
                r'\\cosh': 'hyperbolic cosine',
                r'\\tanh': 'hyperbolic tangent',
                r'\\left': '',
                r'\\right': '',
                r'\\lim': 'limit',
                r'\\to': 'approaches',
                r'\\begin\{([^}]*)\}': r'begin \1',
                r'\\end\{([^}]*)\}': r'end \1',
                r'\\label\{([^}]*)\}': r'',
                r'\\nonumber': '',
                r'\\displaystyle': ''
            }
            
            for pattern, replacement in replacements.items():
                text = re.sub(pattern, replacement, text)
                
            # Remove remaining LaTeX commands and special characters
            text = re.sub(r'\\[a-zA-Z]+', ' ', text)
            text = re.sub(r'[{}\\]', ' ', text)
            
            # Clean up whitespace
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
            
        elif self.format == FormulaFormat.MATHML:
            # For MathML, try to extract the text content
            text = re.sub(r'<[^>]*>', ' ', self.content)
            text = re.sub(r'\s+', ' ', text).strip()
            return text
            
        elif self.format == FormulaFormat.ASCII:
            # ASCII math should be somewhat readable already
            return self.content
            
        elif self.format == FormulaFormat.SYMPY:
            # For SymPy expressions, convert to string
            return str(self.content)
            
        else:
            return self.content
    
    def _parse_formula(self) -> None:
        """Parse the formula to extract variables, constants, operators, and functions."""
        if self.format == FormulaFormat.LATEX:
            self._parse_latex()
        elif self.format == FormulaFormat.MATHML:
            self._parse_mathml()
        elif self.format == FormulaFormat.ASCII:
            self._parse_ascii()
        elif self.format == FormulaFormat.SYMPY and SYMPY_AVAILABLE:
            self._parse_sympy()
    
    def _parse_latex(self) -> None:
        """Parse LaTeX formula to extract components."""
        # Check if we can convert to symbolic form
        if LATEX2SYMPY_AVAILABLE:
            try:
                self.symbolic = latex2sympy(self.content)
                self.representations[FormulaFormat.SYMPY] = str(self.symbolic)
                
                # Extract variables, constants, and functions from symbolic form
                self._parse_sympy_expression(self.symbolic)
                return
            except Exception as e:
                logger.debug(f"Error converting LaTeX to symbolic form: {str(e)}")
                # Fall back to regex-based parsing
        
        # Regex-based parsing of LaTeX
        # Extract variables (single letters that are not part of commands)
        var_pattern = r'(?<![\\a-zA-Z])([a-zA-Z])(?![a-zA-Z])'
        self.variables = set(re.findall(var_pattern, self.content))
        
        # Extract numeric constants
        const_pattern = r'(?<![a-zA-Z])([-+]?\d*\.\d+|\d+)'
        self.constants = set(re.findall(const_pattern, self.content))
        
        # Extract operators
        op_pattern = r'([\+\-\*/=<>]|\\times|\\div|\\cdot|\\leq|\\geq|\\neq|\\approx|\\equiv)'
        self.operators = set(re.findall(op_pattern, self.content))
        
        # Extract functions
        func_pattern = r'\\([a-zA-Z]+)(?:\{|\[|\\)'
        self.functions = set(re.findall(func_pattern, self.content))
        
        # Determine formula type
        if "=" in self.content or "\\eq" in self.content:
            self.formula_type = FormulaType.EQUATION
        
        if "\\begin{align" in self.content or "\\begin{equation" in self.content:
            self.formula_type = FormulaType.BLOCK
        elif "\\begin{multline" in self.content or "\\\\" in self.content:
            self.formula_type = FormulaType.MULTI_LINE
    
    def _parse_mathml(self) -> None:
        """Parse MathML formula to extract components."""
        # Extract variables (mi elements)
        var_pattern = r'<mi>([a-zA-Z])</mi>'
        self.variables = set(re.findall(var_pattern, self.content))
        
        # Extract numeric constants (mn elements)
        const_pattern = r'<mn>([-+]?\d*\.\d+|\d+)</mn>'
        self.constants = set(re.findall(const_pattern, self.content))
        
        # Extract operators (mo elements)
        op_pattern = r'<mo>([\+\-\*/=<>])</mo>'
        self.operators = set(re.findall(op_pattern, self.content))
        
        # Extract functions (function-specific elements)
        func_pattern = r'<([a-z]+:)?([a-zA-Z]+)>'
        matches = re.findall(func_pattern, self.content)
        self.functions = set([m[1] for m in matches if m[1] not in ['mi', 'mn', 'mo', 'mrow', 'mfrac', 'msup', 'msub', 'math']])
        
        # Determine formula type
        if "<mo>=</mo>" in self.content:
            self.formula_type = FormulaType.EQUATION
    
    def _parse_ascii(self) -> None:
        """Parse ASCII formula to extract components."""
        # Extract variables (single letters)
        var_pattern = r'(?<![a-zA-Z])([a-zA-Z])(?![a-zA-Z])'
        self.variables = set(re.findall(var_pattern, self.content))
        
        # Extract numeric constants
        const_pattern = r'(?<![a-zA-Z])([-+]?\d*\.\d+|\d+)'
        self.constants = set(re.findall(const_pattern, self.content))
        
        # Extract operators
        op_pattern = r'([\+\-\*/=<>])'
        self.operators = set(re.findall(op_pattern, self.content))
        
        # Extract functions (common function names)
        func_pattern = r'\b(sin|cos|tan|log|ln|exp|sqrt)\b'
        self.functions = set(re.findall(func_pattern, self.content))
        
        # Determine formula type
        if "=" in self.content:
            self.formula_type = FormulaType.EQUATION
    
    def _parse_sympy(self) -> None:
        """Parse SymPy expression to extract components."""
        if not SYMPY_AVAILABLE:
            return
            
        try:
            # Convert string to SymPy expression if needed
            if isinstance(self.content, str):
                self.symbolic = sympy.sympify(self.content)
            else:
                self.symbolic = self.content
                
            self._parse_sympy_expression(self.symbolic)
            
        except Exception as e:
            logger.debug(f"Error parsing SymPy expression: {str(e)}")
    
    def _parse_sympy_expression(self, expr) -> None:
        """Parse a SymPy expression to extract components."""
        if not SYMPY_AVAILABLE:
            return
            
        try:
            # Extract variables (free symbols)
            self.variables = set([str(symbol) for symbol in expr.free_symbols])
            
            # Extract numeric constants
            constants = []
            for atom in expr.atoms():
                if isinstance(atom, sympy.Number) and not isinstance(atom, sympy.Symbol):
                    constants.append(str(atom))
            self.constants = set(constants)
            
            # Extract functions
            functions = []
            for func in expr.atoms(sympy.Function):
                functions.append(str(func.func))
            self.functions = set(functions)
            
            # Extract operators based on expression type
            operators = set()
            if isinstance(expr, sympy.Add):
                operators.add('+')
            if isinstance(expr, sympy.Mul):
                operators.add('*')
            if isinstance(expr, sympy.Pow):
                operators.add('^')
            if isinstance(expr, sympy.Equality):
                operators.add('=')
                self.formula_type = FormulaType.EQUATION
            self.operators = operators
            
        except Exception as e:
            logger.debug(f"Error analyzing SymPy expression: {str(e)}")
    
    def to_latex(self) -> str:
        """Convert formula to LaTeX format."""
        if self.format == FormulaFormat.LATEX:
            return self.content
            
        if FormulaFormat.LATEX in self.representations:
            return self.representations[FormulaFormat.LATEX]
            
        if self.format == FormulaFormat.SYMPY and SYMPY_AVAILABLE:
            try:
                latex = sympy.latex(self.symbolic)
                self.representations[FormulaFormat.LATEX] = latex
                return latex
            except Exception as e:
                logger.debug(f"Error converting SymPy to LaTeX: {str(e)}")
                
        if self.format == FormulaFormat.MATHML:
            # Simple conversion of basic MathML to LaTeX
            # This is a simplified approach - a full converter would be more complex
            text = self.content
            
            # Replace variables
            text = re.sub(r'<mi>([^<]*)</mi>', r'\1', text)
            
            # Replace numbers
            text = re.sub(r'<mn>([^<]*)</mn>', r'\1', text)
            
            # Replace operators
            text = re.sub(r'<mo>([^<]*)</mo>', r'\1', text)
            
            # Replace fractions
            text = re.sub(r'<mfrac><mrow>([^<]*)</mrow><mrow>([^<]*)</mrow></mfrac>', r'\\frac{\1}{\2}', text)
            
            # Replace superscripts
            text = re.sub(r'<msup><mrow>([^<]*)</mrow><mrow>([^<]*)</mrow></msup>', r'\1^{\2}', text)
            
            # Replace subscripts
            text = re.sub(r'<msub><mrow>([^<]*)</mrow><mrow>([^<]*)</mrow></msub>', r'\1_{\2}', text)
            
            # Remove remaining tags
            text = re.sub(r'<[^>]*>', '', text)
            
            # Save and return
            self.representations[FormulaFormat.LATEX] = text
            return text
            
        # For ASCII math, attempt simple conversion (limited)
        if self.format == FormulaFormat.ASCII:
            # Convert common patterns
            text = self.content
            
            # Replace sqrt() with \sqrt{}
            text = re.sub(r'sqrt\(([^)]*)\)', r'\\sqrt{\1}', text)
            
            # Replace ^ with proper superscript
            text = re.sub(r'(\w)\^(\w|\d)', r'\1^{\2}', text)
            
            # Replace / with fraction
            text = re.sub(r'([\w\d]+)/([\w\d]+)', r'\\frac{\1}{\2}', text)
            
            # Save and return
            self.representations[FormulaFormat.LATEX] = text
            return text
            
        # Default: return as-is with $ delimiters
        return f"${self.content}$"
    
    def to_sympy(self):
        """Convert formula to SymPy expression."""
        if not SYMPY_AVAILABLE:
            return None
            
        if self.symbolic is not None:
            return self.symbolic
            
        if self.format == FormulaFormat.SYMPY:
            try:
                self.symbolic = sympy.sympify(self.content)
                return self.symbolic
            except:
                return None
                
        if self.format == FormulaFormat.LATEX and LATEX2SYMPY_AVAILABLE:
            try:
                self.symbolic = latex2sympy(self.content)
                return self.symbolic
            except Exception as e:
                logger.debug(f"Error converting LaTeX to SymPy: {str(e)}")
                return None
                
        # Other formats not supported
        return None
    
    def evaluate(self, variable_values: Dict[str, Any] = None) -> Optional[float]:
        """
        Evaluate the formula with the given variable values.
        
        Args:
            variable_values: Dictionary mapping variable names to values
            
        Returns:
            Evaluation result or None if evaluation fails
        """
        if not SYMPY_AVAILABLE:
            return None
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return None
            
        if variable_values is None:
            variable_values = {}
            
        try:
            # Substitute variables with values
            expr = sympy_expr.subs(variable_values)
            
            # Try to evaluate to a numerical value
            result = float(expr.evalf())
            return result
            
        except Exception as e:
            logger.debug(f"Error evaluating formula: {str(e)}")
            return None
    
    def solve_for(self, variable: str) -> Optional[str]:
        """
        Solve the equation for the specified variable.
        
        Args:
            variable: Variable to solve for
            
        Returns:
            Solution as string, or None if solving fails
        """
        if not SYMPY_AVAILABLE:
            return None
            
        if self.formula_type != FormulaType.EQUATION:
            return None
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return None
            
        try:
            # Ensure we have an equation
            if not isinstance(sympy_expr, sympy.Equality):
                return None
                
            # Extract left and right sides
            lhs = sympy_expr.lhs
            rhs = sympy_expr.rhs
            
            # Solve for the variable
            var_sym = sympy.Symbol(variable)
            solution = sympy.solve(lhs - rhs, var_sym)
            
            if solution:
                # Convert solution to LaTeX
                solution_latex = ", ".join([sympy.latex(sol) for sol in solution])
                return f"{variable} = {solution_latex}"
                
            return None
            
        except Exception as e:
            logger.debug(f"Error solving formula: {str(e)}")
            return None
    
    def simplify(self) -> str:
        """
        Simplify the formula.
        
        Returns:
            Simplified formula in LaTeX format
        """
        if not SYMPY_AVAILABLE:
            return self.to_latex()
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return self.to_latex()
            
        try:
            # Simplify the expression
            simplified = sympy.simplify(sympy_expr)
            
            # Convert back to LaTeX
            simplified_latex = sympy.latex(simplified)
            return simplified_latex
            
        except Exception as e:
            logger.debug(f"Error simplifying formula: {str(e)}")
            return self.to_latex()
    
    def expand(self) -> str:
        """
        Expand the formula.
        
        Returns:
            Expanded formula in LaTeX format
        """
        if not SYMPY_AVAILABLE:
            return self.to_latex()
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return self.to_latex()
            
        try:
            # Expand the expression
            expanded = sympy.expand(sympy_expr)
            
            # Convert back to LaTeX
            expanded_latex = sympy.latex(expanded)
            return expanded_latex
            
        except Exception as e:
            logger.debug(f"Error expanding formula: {str(e)}")
            return self.to_latex()
    
    def factor(self) -> str:
        """
        Factor the formula.
        
        Returns:
            Factored formula in LaTeX format
        """
        if not SYMPY_AVAILABLE:
            return self.to_latex()
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return self.to_latex()
            
        try:
            # Factor the expression
            factored = sympy.factor(sympy_expr)
            
            # Convert back to LaTeX
            factored_latex = sympy.latex(factored)
            return factored_latex
            
        except Exception as e:
            logger.debug(f"Error factoring formula: {str(e)}")
            return self.to_latex()
    
    def get_derivative(self, variable: str) -> str:
        """
        Calculate the derivative of the formula with respect to the given variable.
        
        Args:
            variable: Variable to differentiate with respect to
            
        Returns:
            Derivative formula in LaTeX format
        """
        if not SYMPY_AVAILABLE:
            return None
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return None
            
        try:
            # Calculate the derivative
            var_sym = sympy.Symbol(variable)
            derivative = sympy.diff(sympy_expr, var_sym)
            
            # Convert to LaTeX
            derivative_latex = sympy.latex(derivative)
            return derivative_latex
            
        except Exception as e:
            logger.debug(f"Error calculating derivative: {str(e)}")
            return None
    
    def get_integral(self, variable: str) -> str:
        """
        Calculate the indefinite integral of the formula with respect to the given variable.
        
        Args:
            variable: Variable to integrate with respect to
            
        Returns:
            Integral formula in LaTeX format
        """
        if not SYMPY_AVAILABLE:
            return None
            
        sympy_expr = self.to_sympy()
        if sympy_expr is None:
            return None
            
        try:
            # Calculate the integral
            var_sym = sympy.Symbol(variable)
            integral = sympy.integrate(sympy_expr, var_sym)
            
            # Convert to LaTeX
            integral_latex = sympy.latex(integral)
            return integral_latex
            
        except Exception as e:
            logger.debug(f"Error calculating integral: {str(e)}")
            return None
    
    def get_explanation(self) -> str:
        """
        Generate a text explanation of the formula.
        
        Returns:
            Text explanation of the formula
        """
        explanation = []
        
        # Start with formula type
        if self.formula_type == FormulaType.EQUATION:
            explanation.append("This is an equation")
        elif self.formula_type == FormulaType.EXPRESSION:
            explanation.append("This is a mathematical expression")
        elif self.formula_type == FormulaType.MULTI_LINE:
            explanation.append("This is a multi-line mathematical expression")
            
        # Add variables
        if self.variables:
            explanation.append(f"It contains the following variables: {', '.join(sorted(self.variables))}")
            
        # Add constants
        if self.constants:
            explanation.append(f"It contains the following constants: {', '.join(sorted(self.constants))}")
            
        # Add functions
        if self.functions:
            explanation.append(f"It uses the following functions: {', '.join(sorted(self.functions))}")
            
        # Add plain text representation
        explanation.append(f"In plain language: {self.text_representation}")
        
        return ". ".join(explanation)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "content": self.content,
            "format": self.format,
            "formula_type": self.formula_type,
            "metadata": self.metadata,
            "source_document": self.source_document,
            "source_page": self.source_page,
            "source_position": self.source_position,
            "text_representation": self.text_representation,
            "variables": list(self.variables),
            "constants": list(self.constants),
            "operators": list(self.operators),
            "functions": list(self.functions),
            "representations": self.representations
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Formula':
        """Create from dictionary."""
        formula = cls(
            content=data["content"],
            format=data["format"],
            formula_type=data["formula_type"],
            metadata=data["metadata"],
            source_document=data["source_document"],
            source_page=data["source_page"],
            source_position=data["source_position"]
        )
        
        formula.text_representation = data["text_representation"]
        formula.variables = set(data["variables"])
        formula.constants = set(data["constants"])
        formula.operators = set(data["operators"])
        formula.functions = set(data["functions"])
        formula.representations = data["representations"]
        
        return formula
    
    def render_as_image(self, output_path: Optional[str] = None) -> Optional[str]:
        """
        Render the formula as an image.
        
        Args:
            output_path: Path to save the rendered image (if None, a temporary file is used)
            
        Returns:
            Path to the rendered image or None if rendering fails
        """
        if not MATPLOTLIB_AVAILABLE:
            logger.warning("Matplotlib is required for rendering formulas")
            return None
            
        # Convert to LaTeX format
        latex = self.to_latex()
        
        # Remove outer $ if present
        if latex.startswith('$') and latex.endswith('$'):
            latex = latex[1:-1]
            
        try:
            # Create figure
            fig = plt.figure(figsize=(6, 1.5))
            fig.text(0.5, 0.5, f"${latex}$", fontsize=14, ha='center', va='center')
            
            # Create temporary file if no output path
            if output_path is None:
                with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as temp:
                    output_path = temp.name
                    
            # Save figure
            fig.savefig(output_path, bbox_inches='tight', pad_inches=0.1, dpi=150)
            plt.close(fig)
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error rendering formula as image: {str(e)}")
            return None


class FormulaProcessor:
    """Processor for detecting and handling mathematical formulas."""
    
    def __init__(self):
        """Initialize formula processor."""
        # Confidence threshold for formula detection
        self.confidence_threshold = 0.7
    
    def detect_formula(self, text: str) -> bool:
        """
        Determine if a string likely contains a mathematical formula.
        
        Args:
            text: Text to check
            
        Returns:
            True if text likely contains a formula, False otherwise
        """
        if not text:
            return False
            
        # Check for LaTeX delimiters
        if re.search(r'\$\$.*?\$\$|\$.*?\$', text):
            return True
            
        # Check for LaTeX environments
        if re.search(r'\\begin\{equation\}|\\begin\{align\}|\\begin\{math\}', text):
            return True
            
        # Check for MathML tags
        if re.search(r'<math.*?>.*?</math>', text, re.DOTALL):
            return True
            
        # Count mathematical symbols
        math_symbols = 0
        
        # Check for equality and inequality symbols
        if re.search(r'[=<>]', text):
            math_symbols += 1
            
        # Check for basic operators
        if re.search(r'[+\-*/%]', text):
            math_symbols += 1
            
        # Check for calculus symbols
        if re.search(r'[]', text):
            math_symbols += 2
            
        # Check for set operators
        if re.search(r'[]', text):
            math_symbols += 2
            
        # Check for roots
        if re.search(r'[]', text):
            math_symbols += 1
            
        # Check for summation, product
        if re.search(r'[]', text):
            math_symbols += 2
            
        # Check for Greek letters
        if re.search(r'[]', text):
            math_symbols += 1
            
        # Check for exponents and subscripts
        if re.search(r'\^[0-9a-z]+|_[0-9a-z]+', text):
            math_symbols += 1
            
        # Check for function names
        if re.search(r'\b(sin|cos|tan|log|ln|exp|sqrt)\b', text):
            math_symbols += 1
            
        # Return true if enough math symbols are present
        return math_symbols >= 3
    
    def extract_formulas(self, text: str) -> List[Formula]:
        """
        Extract mathematical formulas from text.
        
        Args:
            text: Text to extract formulas from
            
        Returns:
            List of Formula objects
        """
        formulas = []
        
        if not text:
            return formulas
            
        # Extract LaTeX display math
        display_matches = re.finditer(r'\$\$(.*?)\$\$', text, re.DOTALL)
        for match in display_matches:
            latex = match.group(1)
            formulas.append(Formula(
                content=latex,
                format=FormulaFormat.LATEX,
                formula_type=FormulaType.BLOCK,
                source_position={"start": match.start(), "end": match.end()}
            ))
            
        # Extract LaTeX inline math
        inline_matches = re.finditer(r'\$([^$]+)\$', text)
        for match in inline_matches:
            latex = match.group(1)
            formulas.append(Formula(
                content=latex,
                format=FormulaFormat.LATEX,
                formula_type=FormulaType.INLINE,
                source_position={"start": match.start(), "end": match.end()}
            ))
            
        # Extract LaTeX equation environments
        equation_matches = re.finditer(r'\\begin\{equation\}(.*?)\\end\{equation\}', text, re.DOTALL)
        for match in equation_matches:
            latex = match.group(1)
            formulas.append(Formula(
                content=latex,
                format=FormulaFormat.LATEX,
                formula_type=FormulaType.EQUATION,
                source_position={"start": match.start(), "end": match.end()}
            ))
            
        # Extract LaTeX align environments
        align_matches = re.finditer(r'\\begin\{align\}(.*?)\\end\{align\}', text, re.DOTALL)
        for match in align_matches:
            latex = match.group(1)
            formulas.append(Formula(
                content=latex,
                format=FormulaFormat.LATEX,
                formula_type=FormulaType.MULTI_LINE,
                source_position={"start": match.start(), "end": match.end()}
            ))
            
        # Extract MathML
        mathml_matches = re.finditer(r'<math.*?>(.*?)</math>', text, re.DOTALL)
        for match in mathml_matches:
            mathml = match.group()
            formulas.append(Formula(
                content=mathml,
                format=FormulaFormat.MATHML,
                formula_type=FormulaType.EXPRESSION,
                source_position={"start": match.start(), "end": match.end()}
            ))
            
        # Extract ASCII math expressions (if not already captured)
        if not formulas:
            lines = text.split('\n')
            for i, line in enumerate(lines):
                line = line.strip()
                
                # Skip too long lines
                if len(line) > 100:
                    continue
                    
                # Check if line might contain a formula
                if '=' in line and self.detect_formula(line):
                    # Likely an equation
                    formulas.append(Formula(
                        content=line,
                        format=FormulaFormat.ASCII,
                        formula_type=FormulaType.EQUATION,
                        source_position={"line": i}
                    ))
                elif self.detect_formula(line):
                    # Likely a mathematical expression
                    formulas.append(Formula(
                        content=line,
                        format=FormulaFormat.ASCII,
                        formula_type=FormulaType.EXPRESSION,
                        source_position={"line": i}
                    ))
            
        return formulas
    
    def detect_formulas_in_image(self, image_path: str) -> List[Dict[str, Any]]:
        """
        Detect regions containing formulas in an image.
        
        Args:
            image_path: Path to image file
            
        Returns:
            List of dictionaries with positions of detected formulas
        """
        if not CV2_AVAILABLE:
            logger.warning("OpenCV is required for formula detection in images")
            return []
            
        try:
            # Load image
            img = cv2.imread(image_path)
            if img is None:
                logger.error(f"Failed to load image: {image_path}")
                return []
                
            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Apply adaptive thresholding
            thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                          cv2.THRESH_BINARY_INV, 11, 2)
            
            # Dilate to connect components
            kernel = np.ones((3, 3), np.uint8)
            dilated = cv2.dilate(thresh, kernel, iterations=1)
            
            # Find contours
            contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Initialize results
            formula_regions = []
            
            # Process contours
            for contour in contours:
                # Get bounding box
                x, y, w, h = cv2.boundingRect(contour)
                
                # Filter out very small or large regions
                if w < 50 or h < 20 or w > img.shape[1] * 0.95 or h > img.shape[0] * 0.95:
                    continue
                    
                # Calculate aspect ratio
                aspect_ratio = w / h
                
                # Formulas typically have specific aspect ratios
                if aspect_ratio > 0.5 and aspect_ratio < 10:
                    # Extract region
                    roi = gray[y:y+h, x:x+w]
                    
                    # Calculate features (density of black pixels, etc.)
                    black_pixels = np.sum(roi < 128)
                    density = black_pixels / (w * h)
                    
                    # Check for horizontal lines (common in fractions)
                    horizontal_kernel = np.ones((1, w//10), np.uint8)
                    horizontal = cv2.morphologyEx(roi, cv2.MORPH_OPEN, horizontal_kernel)
                    has_horizontal = np.sum(horizontal > 0) > 0
                    
                    # Score based on features
                    score = 0
                    if 0.05 < density < 0.5:
                        score += 0.4
                    if has_horizontal:
                        score += 0.3
                    if 1.5 < aspect_ratio < 5:
                        score += 0.3
                        
                    # Add to results if score is high enough
                    if score > self.confidence_threshold:
                        formula_regions.append({
                            "x": x,
                            "y": y,
                            "width": w,
                            "height": h,
                            "confidence": score
                        })
            
            return formula_regions
            
        except Exception as e:
            logger.error(f"Error detecting formulas in image: {str(e)}")
            return []
    
    def extract_formula_from_image(self, image_path: str, region: Optional[Dict[str, Any]] = None) -> Optional[str]:
        """
        Extract formula text from an image using OCR.
        
        Args:
            image_path: Path to image file
            region: Optional region to focus on (dict with x, y, width, height)
            
        Returns:
            Extracted formula text or None if extraction fails
        """
        try:
            # Import here to handle the case where it's not installed
            import pytesseract
            from PIL import Image
            
            # Open image
            img = Image.open(image_path)
            
            # Crop to region if provided
            if region:
                img = img.crop((
                    region["x"], 
                    region["y"], 
                    region["x"] + region["width"], 
                    region["y"] + region["height"]
                ))
            
            # Apply OCR with special config for formulas
            # Pass configuration to improve formula recognition
            custom_config = r'--psm 6 -c tessedit_char_whitelist="0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ+-=*/()[]{}^_<>,.\\|"'
            text = pytesseract.image_to_string(img, config=custom_config)
            
            return text
            
        except ImportError:
            logger.warning("pytesseract is required for formula extraction from images")
            return None
        except Exception as e:
            logger.error(f"Error extracting formula from image: {str(e)}")
            return None
    
    def get_formula_explanation(self, formula: Formula) -> str:
        """
        Generate a human-readable explanation of a formula.
        
        Args:
            formula: Formula to explain
            
        Returns:
            Text explanation of the formula
        """
        # Use the Formula class's get_explanation method
        return formula.get_explanation()
    
    def formula_to_text(self, formula: Formula) -> str:
        """
        Convert a formula to plain text for indexing and search.
        
        Args:
            formula: Formula to convert
            
        Returns:
            Plain text representation of the formula
        """
        return formula.text_representation
    
    def process_text(self, text: str, source_document: Optional[str] = None, 
                    source_page: Optional[int] = None) -> List[Formula]:
        """
        Process text to extract and analyze formulas.
        
        Args:
            text: Text to process
            source_document: Source document identifier
            source_page: Page number in source document
            
        Returns:
            List of processed Formula objects
        """
        # Extract formulas from text
        formulas = self.extract_formulas(text)
        
        # Set source information
        for formula in formulas:
            formula.source_document = source_document
            formula.source_page = source_page
            
        return formulas
    
    def process_image(self, image_path: str, source_document: Optional[str] = None,
                     source_page: Optional[int] = None) -> List[Formula]:
        """
        Process an image to extract formulas.
        
        Args:
            image_path: Path to image file
            source_document: Source document identifier
            source_page: Page number in source document
            
        Returns:
            List of processed Formula objects
        """
        formulas = []
        
        # Detect formula regions
        regions = self.detect_formulas_in_image(image_path)
        
        # Extract text from each region
        for region in regions:
            formula_text = self.extract_formula_from_image(image_path, region)
            
            if formula_text and self.detect_formula(formula_text):
                # Determine format and type
                format_type = FormulaFormat.ASCII
                formula_type = FormulaType.EXPRESSION
                
                if "=" in formula_text:
                    formula_type = FormulaType.EQUATION
                    
                # Create formula object
                formula = Formula(
                    content=formula_text,
                    format=format_type,
                    formula_type=formula_type,
                    metadata={"confidence": region["confidence"]},
                    source_document=source_document,
                    source_page=source_page,
                    source_position=region
                )
                
                formulas.append(formula)
        
        return formulas
    
    def render_formula(self, formula: Formula, output_path: Optional[str] = None) -> Optional[str]:
        """
        Render a formula as an image.
        
        Args:
            formula: Formula to render
            output_path: Path to save the rendered image
            
        Returns:
            Path to the rendered image or None if rendering fails
        """
        return formula.render_as_image(output_path)
    
    def search_formulas(self, formulas: List[Formula], query: str) -> List[Tuple[Formula, float]]:
        """
        Search for formulas matching a query.
        
        Args:
            formulas: List of formulas to search
            query: Search query
            
        Returns:
            List of (formula, score) tuples sorted by relevance
        """
        # Extract search terms from query
        search_terms = set(term.lower() for term in query.split())
        
        results = []
        
        for formula in formulas:
            score = 0
            
            # Check text representation
            text_rep = formula.text_representation.lower()
            
            # Count matching terms
            matching_terms = 0
            for term in search_terms:
                if term in text_rep:
                    matching_terms += 1
                    
            # Calculate score based on matching terms
            if matching_terms > 0:
                score += 0.5 * (matching_terms / len(search_terms))
                
            # Check variables
            for term in search_terms:
                if term in formula.variables:
                    score += 0.3
                    
            # Check functions
            for term in search_terms:
                if term in formula.functions:
                    score += 0.2
                    
            # Add to results if score is positive
            if score > 0:
                results.append((formula, score))
                
        # Sort by score (highest first)
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results



















Web Package Initialization (__init__.py)


"""
Web package for the RAG system.
Contains the web server, API routes, and frontend assets.
"""
from flask import Flask, render_template, request, jsonify, Blueprint
import logging
import json

# Package version
__version__ = '1.0.0'

def create_app(config=None):
    """
    Create and configure the Flask application.
    
    Args:
        config: Optional configuration to override defaults
        
    Returns:
        Configured Flask application
    """
    app = Flask(__name__)
    
    # Load default configuration
    app.config.from_object('config.settings')
    
    # Override with passed config
    if config:
        app.config.update(config)
    
    # Register blueprints
    from web.routes import api_bp, main_bp
    app.register_blueprint(api_bp)
    app.register_blueprint(main_bp)
    
    # Configure logging
    if not app.debug:
        handler = logging.StreamHandler()
        handler.setLevel(logging.INFO)
        app.logger.addHandler(handler)
        app.logger.setLevel(logging.INFO)
    
    # Register error handlers
    @app.errorhandler(404)
    def not_found(error):
        return render_template('error.html', error=error), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        return render_template('error.html', error=error), 500
    
    return app














CSS Stylesheets (styles.css)




/* 
 * Main stylesheet for the RAG System
 * Modern, responsive design with a soothing color scheme
 */

:root {
  /* Color variables */
  --primary-color: #4361ee;
  --primary-light: #4895ef;
  --primary-dark: #3f37c9;
  --secondary-color: #4cc9f0;
  --accent-color: #7209b7;
  --text-color: #333333;
  --text-light: #666666;
  --text-dark: #111111;
  --background-color: #f8f9fa;
  --surface-color: #ffffff;
  --border-color: #e0e0e0;
  --error-color: #e63946;
  --success-color: #2a9d8f;
  --warning-color: #e9c46a;
  --info-color: #457b9d;
  
  /* Typography */
  --font-family-sans: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', sans-serif;
  --font-family-mono: 'JetBrains Mono', 'Fira Code', 'Courier New', monospace;
  
  /* Spacing */
  --spacing-xs: 0.25rem;
  --spacing-sm: 0.5rem;
  --spacing-md: 1rem;
  --spacing-lg: 1.5rem;
  --spacing-xl: 2rem;
  --spacing-xxl: 3rem;
  
  /* Borders */
  --border-radius-sm: 0.25rem;
  --border-radius-md: 0.5rem;
  --border-radius-lg: 0.75rem;
  --border-radius-xl: 1rem;
  
  /* Shadows */
  --shadow-sm: 0 1px 2px rgba(0, 0, 0, 0.05);
  --shadow-md: 0 4px 6px rgba(0, 0, 0, 0.1);
  --shadow-lg: 0 10px 15px rgba(0, 0, 0, 0.1);
  --shadow-xl: 0 20px 25px rgba(0, 0, 0, 0.15);
  
  /* Transitions */
  --transition-fast: 150ms ease;
  --transition-normal: 250ms ease;
  --transition-slow: 350ms ease;
  
  /* Z-index layers */
  --z-background: -1;
  --z-default: 1;
  --z-above: 10;
  --z-header: 100;
  --z-modal: 1000;
  --z-tooltip: 1500;
}

/* Base styles */
* {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

html {
  font-size: 16px;
  height: 100%;
  scroll-behavior: smooth;
}

body {
  font-family: var(--font-family-sans);
  color: var(--text-color);
  background-color: var(--background-color);
  line-height: 1.6;
  min-height: 100%;
  display: flex;
  flex-direction: column;
}

a {
  color: var(--primary-color);
  text-decoration: none;
  transition: color var(--transition-fast);
}

a:hover {
  color: var(--primary-dark);
  text-decoration: underline;
}

/* Typography */
h1, h2, h3, h4, h5, h6 {
  font-weight: 600;
  line-height: 1.3;
  margin-bottom: var(--spacing-md);
  color: var(--text-dark);
}

h1 {
  font-size: 2.5rem;
  margin-top: var(--spacing-xl);
}

h2 {
  font-size: 2rem;
  margin-top: var(--spacing-lg);
}

h3 {
  font-size: 1.75rem;
  margin-top: var(--spacing-lg);
}

h4 {
  font-size: 1.5rem;
  margin-top: var(--spacing-md);
}

h5 {
  font-size: 1.25rem;
  margin-top: var(--spacing-md);
}

h6 {
  font-size: 1rem;
  margin-top: var(--spacing-md);
}

p, ul, ol {
  margin-bottom: var(--spacing-md);
}

ul, ol {
  padding-left: var(--spacing-lg);
}

code, pre {
  font-family: var(--font-family-mono);
  background-color: #f1f1f1;
  border-radius: var(--border-radius-sm);
}

code {
  padding: 0.15em 0.3em;
  font-size: 0.85em;
}

pre {
  padding: var(--spacing-md);
  margin-bottom: var(--spacing-md);
  overflow-x: auto;
  white-space: pre-wrap;
  word-wrap: break-word;
}

pre code {
  padding: 0;
  background-color: transparent;
}

blockquote {
  border-left: 4px solid var(--primary-light);
  padding-left: var(--spacing-md);
  margin-left: 0;
  margin-bottom: var(--spacing-md);
  color: var(--text-light);
}

/* Layout */
.container {
  width: 100%;
  max-width: 1200px;
  margin: 0 auto;
  padding: 0 var(--spacing-md);
}

.row {
  display: flex;
  flex-wrap: wrap;
  margin: 0 calc(-1 * var(--spacing-md));
}

.col {
  flex: 1;
  padding: 0 var(--spacing-md);
  min-width: 0;
}

@media (max-width: 768px) {
  .row {
    flex-direction: column;
  }
  
  .col {
    margin-bottom: var(--spacing-md);
  }
}

/* Header */
.header {
  background-color: var(--surface-color);
  box-shadow: var(--shadow-sm);
  padding: var(--spacing-md) 0;
  position: sticky;
  top: 0;
  z-index: var(--z-header);
}

.header-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.logo {
  font-size: 1.5rem;
  font-weight: 700;
  color: var(--primary-color);
  display: flex;
  align-items: center;
}

.logo img {
  height: 2rem;
  margin-right: var(--spacing-sm);
}

.nav-menu {
  display: flex;
  list-style-type: none;
  padding: 0;
  margin: 0;
}

.nav-item {
  margin-left: var(--spacing-lg);
}

.nav-link {
  font-weight: 500;
  color: var(--text-color);
  transition: color var(--transition-fast);
}

.nav-link:hover, 
.nav-link.active {
  color: var(--primary-color);
  text-decoration: none;
}

.menu-toggle {
  display: none;
  cursor: pointer;
  font-size: 1.5rem;
}

@media (max-width: 768px) {
  .menu-toggle {
    display: block;
  }
  
  .nav-menu {
    display: none;
    flex-direction: column;
    position: absolute;
    top: 100%;
    left: 0;
    right: 0;
    background-color: var(--surface-color);
    box-shadow: var(--shadow-md);
    padding: var(--spacing-md);
  }
  
  .nav-menu.active {
    display: flex;
  }
  
  .nav-item {
    margin: var(--spacing-sm) 0;
    margin-left: 0;
  }
}

/* Main content */
.main {
  flex: 1;
  padding: var(--spacing-xl) 0;
}

/* Chat interface */
.chat-container {
  display: flex;
  flex-direction: column;
  height: 80vh;
  background-color: var(--surface-color);
  border-radius: var(--border-radius-lg);
  box-shadow: var(--shadow-md);
  overflow: hidden;
}

.chat-header {
  background-color: var(--primary-color);
  color: white;
  padding: var(--spacing-md) var(--spacing-lg);
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.chat-title {
  font-size: 1.2rem;
  font-weight: 600;
  margin: 0;
}

.chat-controls {
  display: flex;
  gap: var(--spacing-sm);
}

.source-selector {
  display: flex;
  background-color: rgba(255, 255, 255, 0.2);
  border-radius: var(--border-radius-md);
  overflow: hidden;
}

.source-btn {
  background: none;
  border: none;
  color: white;
  padding: var(--spacing-xs) var(--spacing-md);
  cursor: pointer;
  font-size: 0.9rem;
  opacity: 0.8;
  transition: opacity var(--transition-fast), background-color var(--transition-fast);
}

.source-btn.active {
  background-color: rgba(255, 255, 255, 0.3);
  opacity: 1;
}

.source-btn:hover {
  opacity: 1;
}

.chat-messages {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-lg);
  display: flex;
  flex-direction: column;
  gap: var(--spacing-md);
}

.message {
  display: flex;
  max-width: 80%;
}

.message.user {
  align-self: flex-end;
}

.message.system {
  align-self: flex-start;
}

.message-content {
  padding: var(--spacing-md);
  border-radius: var(--border-radius-md);
  box-shadow: var(--shadow-sm);
}

.user .message-content {
  background-color: var(--primary-light);
  color: white;
  border-radius: var(--border-radius-md) var(--border-radius-md) 0 var(--border-radius-md);
}

.system .message-content {
  background-color: #f1f1f1;
  border-radius: 0 var(--border-radius-md) var(--border-radius-md) var(--border-radius-md);
}

.message-time {
  font-size: 0.8rem;
  color: var(--text-light);
  margin-top: var(--spacing-xs);
  align-self: flex-end;
}

.chat-input {
  display: flex;
  padding: var(--spacing-md);
  background-color: #f8f9fa;
  border-top: 1px solid var(--border-color);
}

.chat-input-field {
  flex: 1;
  padding: var(--spacing-md);
  border: 1px solid var(--border-color);
  border-radius: var(--border-radius-md);
  resize: none;
  font-family: var(--font-family-sans);
  font-size: 1rem;
  transition: border-color var(--transition-fast);
}

.chat-input-field:focus {
  outline: none;
  border-color: var(--primary-color);
}

.chat-submit {
  background-color: var(--primary-color);
  color: white;
  border: none;
  padding: var(--spacing-md) var(--spacing-lg);
  margin-left: var(--spacing-md);
  border-radius: var(--border-radius-md);
  cursor: pointer;
  font-weight: 500;
  transition: background-color var(--transition-fast);
}

.chat-submit:hover {
  background-color: var(--primary-dark);
}

.chat-submit:disabled {
  background-color: var(--border-color);
  cursor: not-allowed;
}

/* Response section */
.response-section {
  margin-top: var(--spacing-xl);
}

.response-tabs {
  display: flex;
  border-bottom: 1px solid var(--border-color);
  margin-bottom: var(--spacing-lg);
}

.response-tab {
  padding: var(--spacing-sm) var(--spacing-lg);
  cursor: pointer;
  border-bottom: 2px solid transparent;
  transition: border-color var(--transition-fast), color var(--transition-fast);
}

.response-tab.active {
  border-bottom-color: var(--primary-color);
  color: var(--primary-color);
  font-weight: 500;
}

.response-content {
  display: none;
}

.response-content.active {
  display: block;
}

.response-card {
  background-color: var(--surface-color);
  border-radius: var(--border-radius-lg);
  box-shadow: var(--shadow-md);
  padding: var(--spacing-lg);
  margin-bottom: var(--spacing-lg);
}

.response-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: var(--spacing-md);
}

.response-title {
  font-size: 1.2rem;
  font-weight: 600;
  margin: 0;
}

.response-source {
  font-size: 0.9rem;
  color: var(--text-light);
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--border-radius-sm);
  background-color: #f1f1f1;
}

.response-text {
  line-height: 1.6;
}

.response-footer {
  margin-top: var(--spacing-md);
  display: flex;
  justify-content: space-between;
  align-items: center;
  font-size: 0.9rem;
  color: var(--text-light);
}

.response-metadata {
  display: flex;
  gap: var(--spacing-md);
}

.response-confidence {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.confidence-meter {
  width: 50px;
  height: 6px;
  background-color: #e0e0e0;
  border-radius: 3px;
  overflow: hidden;
}

.confidence-fill {
  height: 100%;
  background-color: var(--success-color);
}

/* Source attribution */
.source-list {
  margin-top: var(--spacing-lg);
}

.source-item {
  margin-bottom: var(--spacing-md);
  padding-bottom: var(--spacing-md);
  border-bottom: 1px solid var(--border-color);
}

.source-title {
  font-weight: 500;
  margin-bottom: var(--spacing-xs);
}

.source-meta {
  font-size: 0.9rem;
  color: var(--text-light);
}

/* Tables */
table {
  width: 100%;
  border-collapse: collapse;
  margin-bottom: var(--spacing-lg);
}

th, td {
  padding: var(--spacing-sm) var(--spacing-md);
  text-align: left;
  border-bottom: 1px solid var(--border-color);
}

th {
  background-color: #f8f9fa;
  font-weight: 600;
}

tr:nth-child(even) {
  background-color: #f8f9fa;
}

/* Code highlighting */
.code-block {
  background-color: #2b2b2b;
  color: #f8f8f2;
  border-radius: var(--border-radius-md);
  margin-bottom: var(--spacing-md);
  overflow: hidden;
}

.code-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: var(--spacing-sm) var(--spacing-md);
  background-color: #202020;
  color: #e0e0e0;
  font-family: var(--font-family-mono);
  font-size: 0.9rem;
}

.code-lang {
  display: inline-block;
  padding: 0.15em 0.5em;
  background-color: rgba(255, 255, 255, 0.1);
  border-radius: var(--border-radius-sm);
}

.code-copy {
  background: none;
  border: none;
  color: #e0e0e0;
  cursor: pointer;
  font-size: 0.9rem;
  transition: color var(--transition-fast);
}

.code-copy:hover {
  color: white;
}

.code-content {
  padding: var(--spacing-md);
  overflow-x: auto;
}

/* Math formulas */
.formula {
  padding: var(--spacing-md);
  background-color: #f8f9fa;
  border-radius: var(--border-radius-md);
  overflow-x: auto;
  margin-bottom: var(--spacing-md);
  text-align: center;
}

/* Loading states */
.loading {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 2px solid rgba(0, 0, 0, 0.1);
  border-radius: 50%;
  border-top-color: var(--primary-color);
  animation: spin 1s ease-in-out infinite;
}

@keyframes spin {
  to { transform: rotate(360deg); }
}

.loading-dots {
  display: flex;
  justify-content: center;
  gap: 0.3em;
  margin: var(--spacing-md) 0;
}

.dot {
  width: 0.5em;
  height: 0.5em;
  border-radius: 50%;
  background-color: var(--primary-color);
  animation: pulse 1.5s ease-in-out infinite;
}

.dot:nth-child(2) {
  animation-delay: 0.2s;
}

.dot:nth-child(3) {
  animation-delay: 0.4s;
}

@keyframes pulse {
  0%, 100% { opacity: 0.5; transform: scale(0.8); }
  50% { opacity: 1; transform: scale(1.2); }
}

/* Alerts */
.alert {
  padding: var(--spacing-md) var(--spacing-lg);
  border-radius: var(--border-radius-md);
  margin-bottom: var(--spacing-md);
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
}

.alert-info {
  background-color: rgba(69, 123, 157, 0.1);
  color: var(--info-color);
  border-left: 4px solid var(--info-color);
}

.alert-success {
  background-color: rgba(42, 157, 143, 0.1);
  color: var(--success-color);
  border-left: 4px solid var(--success-color);
}

.alert-warning {
  background-color: rgba(233, 196, 106, 0.1);
  color: #e76f51;
  border-left: 4px solid #e76f51;
}

.alert-error {
  background-color: rgba(230, 57, 70, 0.1);
  color: var(--error-color);
  border-left: 4px solid var(--error-color);
}

.alert-icon {
  font-size: 1.2rem;
}

.alert-content {
  flex: 1;
}

.alert-title {
  font-weight: 600;
  margin-bottom: var(--spacing-xs);
}

/* Footer */
.footer {
  background-color: var(--surface-color);
  padding: var(--spacing-lg) 0;
  border-top: 1px solid var(--border-color);
  margin-top: auto;
}

.footer-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.footer-links {
  display: flex;
  gap: var(--spacing-lg);
}

.footer-copyright {
  font-size: 0.9rem;
  color: var(--text-light);
}

@media (max-width: 768px) {
  .footer-content {
    flex-direction: column;
    gap: var(--spacing-md);
  }
  
  .footer-links {
    flex-direction: column;
    gap: var(--spacing-sm);
    text-align: center;
  }
}

/* Error page */
.error-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  min-height: 60vh;
  text-align: center;
  padding: var(--spacing-xl);
}

.error-code {
  font-size: 6rem;
  font-weight: 700;
  color: var(--primary-color);
  line-height: 1;
  margin-bottom: var(--spacing-md);
}

.error-message {
  font-size: 1.5rem;
  margin-bottom: var(--spacing-lg);
  color: var(--text-light);
}

.error-details {
  margin-bottom: var(--spacing-lg);
  max-width: 600px;
}

.error-action {
  margin-top: var(--spacing-lg);
}

.btn {
  display: inline-block;
  padding: var(--spacing-sm) var(--spacing-lg);
  background-color: var(--primary-color);
  color: white;
  border: none;
  border-radius: var(--border-radius-md);
  font-weight: 500;
  cursor: pointer;
  text-align: center;
  transition: background-color var(--transition-fast);
}

.btn:hover {
  background-color: var(--primary-dark);
  text-decoration: none;
}

.btn-secondary {
  background-color: transparent;
  color: var(--primary-color);
  border: 1px solid var(--primary-color);
}

.btn-secondary:hover {
  background-color: rgba(67, 97, 238, 0.1);
}

/* Utilities */
.text-center {
  text-align: center;
}

.mt-1 { margin-top: var(--spacing-sm); }
.mt-2 { margin-top: var(--spacing-md); }
.mt-3 { margin-top: var(--spacing-lg); }
.mt-4 { margin-top: var(--spacing-xl); }
.mt-5 { margin-top: var(--spacing-xxl); }

.mb-1 { margin-bottom: var(--spacing-sm); }
.mb-2 { margin-bottom: var(--spacing-md); }
.mb-3 { margin-bottom: var(--spacing-lg); }
.mb-4 { margin-bottom: var(--spacing-xl); }
.mb-5 { margin-bottom: var(--spacing-xxl); }

.p-1 { padding: var(--spacing-sm); }
.p-2 { padding: var(--spacing-md); }
.p-3 { padding: var(--spacing-lg); }
.p-4 { padding: var(--spacing-xl); }
.p-5 { padding: var(--spacing-xxl); }

.text-primary { color: var(--primary-color); }
.text-secondary { color: var(--secondary-color); }
.text-accent { color: var(--accent-color); }
.text-success { color: var(--success-color); }
.text-error { color: var(--error-color); }
.text-warning { color: var(--warning-color); }
.text-info { color: var(--info-color); }
.text-light { color: var(--text-light); }
.text-dark { color: var(--text-dark); }

.bg-primary { background-color: var(--primary-color); }
.bg-secondary { background-color: var(--secondary-color); }
.bg-accent { background-color: var(--accent-color); }
.bg-success { background-color: var(--success-color); }
.bg-error { background-color: var(--error-color); }
.bg-warning { background-color: var(--warning-color); }
.bg-info { background-color: var(--info-color); }
.bg-light { background-color: var(--background-color); }
.bg-dark { background-color: var(--text-dark); }

.hidden {
  display: none !important;
}

.visually-hidden {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border-width: 0;
}

.d-flex { display: flex; }
.d-inline-flex { display: inline-flex; }
.flex-column { flex-direction: column; }
.justify-center { justify-content: center; }
.justify-between { justify-content: space-between; }
.items-center { align-items: center; }
.gap-1 { gap: var(--spacing-sm); }
.gap-2 { gap: var(--spacing-md); }
.gap-3 { gap: var(--spacing-lg); }

.w-full { width: 100%; }
.h-full { height: 100%; }

/* Light mode / Dark mode toggle */
.theme-toggle {
  background: none;
  border: none;
  cursor: pointer;
  color: var(--text-color);
  padding: var(--spacing-xs);
  border-radius: var(--border-radius-sm);
  transition: background-color var(--transition-fast);
}

.theme-toggle:hover {
  background-color: rgba(0, 0, 0, 0.05);
}

.theme-toggle i {
  font-size: 1.2rem;
}

/* Dark mode styles */
body.dark-mode {
  --text-color: #e0e0e0;
  --text-light: #a0a0a0;
  --text-dark: #ffffff;
  --background-color: #121212;
  --surface-color: #1e1e1e;
  --border-color: #333333;
  
  color-scheme: dark;
}

body.dark-mode .system .message-content {
  background-color: #2a2a2a;
}

body.dark-mode code, 
body.dark-mode pre {
  background-color: #2a2a2a;
}

body.dark-mode .chat-input {
  background-color: #1a1a1a;
}

body.dark-mode .response-source {
  background-color: #2a2a2a;
}

body.dark-mode tr:nth-child(even) {
  background-color: #222222;
}

body.dark-mode th {
  background-color: #2a2a2a;
}

body.dark-mode .formula {
  background-color: #2a2a2a;
}

body.dark-mode .theme-toggle:hover {
  background-color: rgba(255, 255, 255, 0.1);
}













Error Template (error.html)

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ error.code if error.code else 'Error' }} - RAG System</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link rel="icon" href="{{ url_for('static', filename='images/favicon.ico') }}">
</head>
<body>
    <header class="header">
        <div class="container header-content">
            <a href="{{ url_for('main.index') }}" class="logo">
                <img src="{{ url_for('static', filename='images/logo.svg') }}" alt="RAG System Logo">
                <span>RAG System</span>
            </a>
            <button class="menu-toggle" aria-label="Toggle menu">
                <i class="fas fa-bars"></i>
            </button>
            <nav>
                <ul class="nav-menu">
                    <li class="nav-item">
                        <a href="{{ url_for('main.index') }}" class="nav-link">Home</a>
                    </li>
                    <li class="nav-item">
                        <a href="{{ url_for('main.about') }}" class="nav-link">About</a>
                    </li>
                    <li class="nav-item">
                        <a href="{{ url_for('main.documentation') }}" class="nav-link">Documentation</a>
                    </li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="main">
        <div class="container">
            <div class="error-container">
                <div class="error-code">{{ error.code if error.code else '500' }}</div>
                
                <h1 class="error-message">
                    {% if error.code == 404 %}
                        Page Not Found
                    {% elif error.code == 403 %}
                        Access Forbidden
                    {% elif error.code == 401 %}
                        Authentication Required
                    {% else %}
                        Something Went Wrong
                    {% endif %}
                </h1>
                
                <div class="error-details">
                    {% if error.code == 404 %}
                        <p>The page you are looking for might have been removed, had its name changed, or is temporarily unavailable.</p>
                    {% elif error.code == 403 %}
                        <p>You don't have permission to access this resource.</p>
                    {% elif error.code == 401 %}
                        <p>You need to be authenticated to access this resource.</p>
                    {% else %}
                        <p>We're experiencing some technical difficulties. Our team has been notified and is working on the issue.</p>
                    {% endif %}
                    
                    {% if error.description %}
                        <div class="alert alert-error mt-3">
                            <div class="alert-icon"><i class="fas fa-exclamation-circle"></i></div>
                            <div class="alert-content">
                                <div class="alert-title">Error Details</div>
                                <p>{{ error.description }}</p>
                            </div>
                        </div>
                    {% endif %}
                </div>
                
                <div class="error-action">
                    <a href="{{ url_for('main.index') }}" class="btn">
                        <i class="fas fa-home"></i> Go to Homepage
                    </a>
                    
                    <button onclick="window.history.back()" class="btn btn-secondary ml-2">
                        <i class="fas fa-arrow-left"></i> Go Back
                    </button>
                </div>
                
                <div class="error-support mt-4">
                    <p class="text-light">Need help? <a href="{{ url_for('main.contact') }}">Contact support</a></p>
                </div>
            </div>
        </div>
    </main>

    <footer class="footer">
        <div class="container footer-content">
            <div class="footer-links">
                <a href="{{ url_for('main.about') }}">About</a>
                <a href="{{ url_for('main.documentation') }}">Documentation</a>
                <a href="{{ url_for('main.contact') }}">Contact</a>
                <a href="{{ url_for('main.privacy') }}">Privacy Policy</a>
            </div>
            <div class="footer-copyright">
                &copy; {{ now.year }} RAG System. All rights reserved.
            </div>
        </div>
    </footer>

    <script>
        // Mobile menu toggle
        document.querySelector('.menu-toggle').addEventListener('click', function() {
            document.querySelector('.nav-menu').classList.toggle('active');
        });
        
        // Add current year to footer if needed
        if (document.querySelector('.footer-copyright').textContent.includes('{{ now.year }}')) {
            document.querySelector('.footer-copyright').textContent = 
                document.querySelector('.footer-copyright').textContent.replace('{{ now.year }}', new Date().getFullYear());
        }
    </script>
</body>
</html>








Python Dependencies (requirements.txt)

# Core dependencies
flask==2.3.3
requests==2.31.0
python-dotenv==1.0.0
nltk==3.8.1
numpy==1.24.4
pandas==2.0.3
scikit-learn==1.3.0
spacy==3.6.1
networkx==3.1
lxml==4.9.3
beautifulsoup4==4.12.2
diskcache==5.6.1
pydantic==2.3.0
gunicorn==21.2.0

# Processing Dependencies
PyPDF2==3.0.1
python-docx==0.8.11
faiss-cpu==1.7.4
rank-bm25==0.2.2
Pillow==10.0.0
pycryptodome==3.18.0

# NLP & ML Dependencies
sentence-transformers==2.2.2
transformers==4.33.1
torch==2.0.1
sympy==1.12
latex2sympy2==1.0.0
sumy==0.11.0

# API/Web Connectors
atlassian-python-api==3.41.3
Flask-Cors==4.0.0
Flask-RESTful==0.3.10
Werkzeug==2.3.7
Jinja2==3.1.2

# Optional Dependencies (Uncomment as needed)
# pytesseract==0.3.10      # OCR support
# pdf2image==1.16.3        # PDF to image conversion
# pymupdf==1.22.5          # Advanced PDF processing
# tabula-py==2.7.0         # PDF table extraction
# camelot-py==0.11.0       # Advanced PDF table extraction
# pdfplumber==0.10.1       # PDF data extraction
# easyocr==1.7.0           # Advanced OCR
# opencv-python==4.8.0.76  # Image processing

# Development & Testing
pytest==7.4.0
black==23.7.0
flake8==6.1.0
mypy==1.5.1
isort==5.12.0
pre-commit==3.3.3












Environment Variables Template (.env)



# Application Settings
DEBUG=true
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO
SECRET_KEY=your-secret-key-here
CACHE_DIR=cache
VECTOR_DB_PATH=vector_db
KNOWLEDGE_GRAPH_PATH=knowledge_graph

# Document Processing
CHUNK_SIZE=300
CHUNK_OVERLAP=50
TOP_K_RETRIEVAL=10
USE_OCR=true
USE_KNOWLEDGE_GRAPH=true

# Confluence API Settings
CONFLUENCE_URL=https://your-instance.atlassian.net
CONFLUENCE_USERNAME=your-username
CONFLUENCE_PASSWORD=your-password
CONFLUENCE_API_TOKEN=your-api-token
CONFLUENCE_SPACE_KEY=TEAM

# Remedy API Settings
REMEDY_URL=https://remedy-server.example.com
REMEDY_USERNAME=your-username
REMEDY_PASSWORD=your-password
REMEDY_SERVER_NAME=your-server-name

# Advanced Settings
ENABLE_CROSS_LANGUAGE=false
DEFAULT_LANGUAGE=en
USE_ADVANCED_OCR=false
ENABLE_RATE_LIMITING=true
MAX_CALLS_PER_MINUTE=60
MAX_CALLS_PER_HOUR=1000










Main Application Entry Point (app.py)



#!/usr/bin/env python
"""
Main application entry point for the Comprehensive RAG System.
Initializes and runs the application with the configured components.
"""
import os
import logging
import argparse
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import core components
from config.config import config
from utils.logger import setup_logging
from web.app import create_app
from connectors.confluence import ConfluenceConnector
from connectors.remedy import RemedyConnector
from processors.document_processor import DocumentProcessor
from retrieval.hybrid_search import HybridSearcher
from retrieval.knowledge_graph import KnowledgeGraph
from nlp.answer_generation import AnswerGenerator

# Setup logging
logger = setup_logging()

def init_components():
    """Initialize all system components."""
    logger.info("Initializing RAG system components...")
    
    # Initialize document processor
    cache_dir = config.get('app', 'cache_dir', 'cache')
    doc_processor = DocumentProcessor(cache_dir=cache_dir)
    
    # Initialize knowledge graph if enabled
    kg_path = config.get('app', 'knowledge_graph_path', 'knowledge_graph')
    use_kg = config.get('app', 'use_knowledge_graph', 'True').lower() == 'true'
    knowledge_graph = KnowledgeGraph(kg_path) if use_kg else None
    
    # Initialize connectors
    connectors = {}
    
    # Confluence connector
    if all([config.get('confluence', key) for key in ['url', 'api_token']]):
        confluence_connector = ConfluenceConnector(
            base_url=config.get('confluence', 'url'),
            username=config.get('confluence', 'username'),
            api_token=config.get('confluence', 'api_token'),
            space_key=config.get('confluence', 'space_key'),
            cache_dir=os.path.join(cache_dir, 'confluence')
        )
        if confluence_connector.check_connection():
            connectors['confluence'] = confluence_connector
            logger.info("Confluence connector initialized successfully.")
        else:
            logger.warning("Confluence connector initialization failed. Continuing without Confluence.")
    else:
        logger.info("Confluence configuration not found or incomplete. Skipping Confluence connector.")
    
    # Remedy connector
    if all([config.get('remedy', key) for key in ['url', 'username', 'password']]):
        remedy_connector = RemedyConnector(
            base_url=config.get('remedy', 'url'),
            username=config.get('remedy', 'username'),
            password=config.get('remedy', 'password'),
            server_name=config.get('remedy', 'server_name'),
            cache_dir=os.path.join(cache_dir, 'remedy')
        )
        if remedy_connector.check_connection():
            connectors['remedy'] = remedy_connector
            logger.info("Remedy connector initialized successfully.")
        else:
            logger.warning("Remedy connector initialization failed. Continuing without Remedy.")
    else:
        logger.info("Remedy configuration not found or incomplete. Skipping Remedy connector.")
    
    # Initialize hybrid searcher
    vector_db_path = config.get('app', 'vector_db_path', 'vector_db')
    top_k = int(config.get('app', 'top_k_retrieval', 10))
    hybrid_searcher = HybridSearcher(
        vector_db_path=vector_db_path,
        knowledge_graph=knowledge_graph,
        top_k=top_k
    )
    
    # Initialize answer generator
    answer_generator = AnswerGenerator()
    
    # Return initialized components
    return {
        'document_processor': doc_processor,
        'knowledge_graph': knowledge_graph,
        'connectors': connectors,
        'hybrid_searcher': hybrid_searcher,
        'answer_generator': answer_generator
    }

def main():
    """Main application entry point."""
    parser = argparse.ArgumentParser(description='Comprehensive RAG System')
    parser.add_argument('--mode', choices=['web', 'cli'], default='web',
                      help='Run mode: web interface or command line')
    parser.add_argument('--host', type=str, default=config.get('app', 'host', '0.0.0.0'),
                      help='Host to bind')
    parser.add_argument('--port', type=int, default=config.get('app', 'port', 8000),
                      help='Port to listen on')
    parser.add_argument('--debug', action='store_true',
                      help='Enable debug mode')
    args = parser.parse_args()
    
    logger.info(f"Starting RAG system in {args.mode} mode")
    
    # Initialize components
    components = init_components()
    
    # Check if at least one connector is initialized
    if not components['connectors']:
        logger.warning("No connectors initialized. The system may have limited functionality.")
    
    # Run in appropriate mode
    if args.mode == 'web':
        # Run web interface
        app = create_app(components)
        debug = args.debug or config.get('app', 'debug', 'False').lower() == 'true'
        app.run(host=args.host, port=args.port, debug=debug)
    else:
        # Run CLI mode
        run_cli_mode(components)

def run_cli_mode(components):
    """Run in command-line interface mode."""
    print("RAG System CLI Mode")
    print("===================")
    print("Type your query, or 'exit' to quit:")
    
    while True:
        query = input("\nQuery> ")
        if query.lower() == 'exit':
            break
            
        source = input("Source (confluence, remedy, both) [both]> ") or "both"
        
        # Process query
        process_query(query, source, components)

def process_query(query, source, components):
    """Process a query and print the response."""
    # Get appropriate connectors
    connectors_to_use = {}
    if source == 'both':
        connectors_to_use = components['connectors']
    elif source in components['connectors']:
        connectors_to_use = {source: components['connectors'][source]}
    else:
        print(f"Error: Source '{source}' not available.")
        return
    
    # Fetch content based on query
    retrieved_content = {}
    for source_name, connector in connectors_to_use.items():
        try:
            if source_name == 'confluence':
                # Search Confluence
                results = connector.search_content(query, limit=5)
                processed_results = []
                for result in results:
                    content = connector.parse_content(result)
                    processed_results.append(content)
                retrieved_content[source_name] = processed_results
            elif source_name == 'remedy':
                # Search Remedy
                results = connector.search_tickets_by_keyword(
                    form_type='incident',
                    keywords=query.split(),
                    limit=5
                )
                processed_results = []
                for ticket in results:
                    processed_ticket = connector.get_ticket_with_details(
                        form_type='incident',
                        ticket_id=ticket.get('id'),
                        include_history=True
                    )
                    processed_results.append(processed_ticket)
                retrieved_content[source_name] = processed_results
        except Exception as e:
            print(f"Error retrieving content from {source_name}: {str(e)}")
    
    # Generate response
    if not retrieved_content:
        print("No relevant information found.")
        return
    
    answer = components['answer_generator'].generate_answer(query, retrieved_content)
    print("\nResponse:")
    print("=========")
    print(answer['response'])
    
    # Print sources if available
    if answer.get('sources'):
        print("\nSources:")
        for source in answer['sources']:
            print(f"- {source}")

if __name__ == "__main__":
    main()








Web Server Application (web/app.py)


"""
Web server for the RAG system.
Creates and configures the Flask application with routes and error handlers.
"""
import os
import logging
from datetime import datetime
from flask import Flask, render_template, request, jsonify, g
from flask_cors import CORS

from config.config import config
from web.routes import api_bp, main_bp

logger = logging.getLogger(__name__)

def create_app(components=None):
    """
    Create and configure the Flask application.
    
    Args:
        components: Dictionary of initialized system components
        
    Returns:
        Configured Flask application
    """
    app = Flask(__name__, 
               template_folder=os.path.join(os.path.dirname(__file__), 'templates'),
               static_folder=os.path.join(os.path.dirname(__file__), 'static'))
    
    # Load default configuration
    app.config.from_object('config.settings')
    
    # Override with environment variable configuration
    app.config['SECRET_KEY'] = config.get('app', 'secret_key', 'your-secret-key-here')
    app.config['DEBUG'] = config.get('app', 'debug', 'False').lower() == 'true'
    
    # Enable CORS
    CORS(app)
    
    # Register components for use in routes
    if components:
        @app.before_request
        def before_request():
            g.components = components
    
    # Register blueprints
    app.register_blueprint(api_bp, url_prefix='/api')
    app.register_blueprint(main_bp)
    
    # Configure logging
    if not app.debug:
        handler = logging.StreamHandler()
        handler.setLevel(logging.INFO)
        app.logger.addHandler(handler)
        app.logger.setLevel(logging.INFO)
    
    # Register error handlers
    @app.errorhandler(404)
    def not_found(error):
        if request.path.startswith('/api'):
            return jsonify({'error': 'Not found', 'code': 404}), 404
        return render_template('error.html', error={'code': 404, 'description': 'The requested resource was not found.'}, now=datetime.now()), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        app.logger.error(f"Internal error: {str(error)}")
        if request.path.startswith('/api'):
            return jsonify({'error': 'Internal server error', 'code': 500}), 500
        return render_template('error.html', error={'code': 500, 'description': 'An internal server error occurred.'}, now=datetime.now()), 500
    
    # Add template context processors
    @app.context_processor
    def inject_now():
        return {'now': datetime.now()}
        
    @app.context_processor
    def inject_sources():
        available_sources = []
        if components and 'connectors' in components:
            available_sources = list(components['connectors'].keys())
        return {'available_sources': available_sources}
    
    return app

if __name__ == '__main__':
    # This allows testing the web app in isolation
    from dotenv import load_dotenv
    load_dotenv()
    
    app = create_app()
    app.run(host=config.get('app', 'host', '0.0.0.0'), 
           port=int(config.get('app', 'port', 8000)), 
           debug=True)














Connectors Package Initialization (connectors/__init__.py)


"""
Connectors package for the RAG system.
Contains connectors for various external systems like Confluence and Remedy.
"""
from connectors.connector_base import BaseConnector
from connectors.confluence import ConfluenceConnector
from connectors.remedy import RemedyConnector

# Package version
__version__ = '1.0.0'

# Available connector classes
CONNECTOR_CLASSES = {
    'confluence': ConfluenceConnector,
    'remedy': RemedyConnector
}

def get_connector(connector_type, **kwargs):
    """
    Factory function to get a connector instance.
    
    Args:
        connector_type: Type of connector ('confluence', 'remedy')
        **kwargs: Connector constructor arguments
        
    Returns:
        Connector instance or None if type not supported
    """
    if connector_type not in CONNECTOR_CLASSES:
        return None
        
    connector_class = CONNECTOR_CLASSES[connector_type]
    return connector_class(**kwargs)














Processors Package Initialization (processors/__init__.py)




"""
Processors package for the RAG system.
Contains document processing and content extraction components.
"""
from processors.document_processor import DocumentProcessor
from processors.chunking import ChunkingEngine
from processors.image_processor import ImageProcessor
from processors.table_processor import TableProcessor
from processors.formula_processor import FormulaProcessor

# Package version
__version__ = '1.0.0'

# Document content types
class ContentType:
    """Content type constants."""
    TEXT = "text"
    TABLE = "table"
    IMAGE = "image"
    CODE = "code"
    FORMULA = "formula"
    LIST = "list"
    HEADING = "heading"
    ATTACHMENT = "attachment"
    UNKNOWN = "unknown"

# Document chunk types
class ChunkType:
    """Chunk type constants."""
    SEMANTIC = "semantic"
    FIXED_SIZE = "fixed_size"
    HEADING_BASED = "heading_based"
    SENTENCE = "sentence"
    PARAGRAPH = "paragraph"
    TABLE = "table"
    IMAGE = "image"
    CODE_BLOCK = "code_block"
    FORMULA = "formula"
